<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CentOS安装Jupyter-Notebook]]></title>
    <url>%2F2019%2F10%2F06%2Fcentos-install-jupyter.html</url>
    <content type="text"><![CDATA[安装依赖1sudo yum install openssl* bzip2 expat zlib* sqlite* libffi* libssl* wget gcc make cmake automake -y 下载、编译、安装Python12345678910111213141516171819# download python sourcewget https://www.python.org/ftp/python/3.7.4/Python-3.7.4.tgz# untartar xf Python-3.7.4.tgzcd Python-3.7.4 # make.configure --prefix=/opt/python3 --with-sslvi setup.py## addsqlite_inc_path=[&apos;/usr/bin/sqlite3&apos;,]## makemake -j20# installmake install# configure profilevi /etc/profile## addexport PATH=$&#123;PATH&#125;:/opt/python3/binsource /etc/profile 安装、配置Jupyter Notebook1234567891011121314151617181920212223242526# install jupyter by pippip3 install jupyter# configure jupyter## get passwd sha1python3 &gt;&gt;&gt; from notebook.auth import passwd&gt;&gt;&gt; passwd()## copy passwd sha1## get jupyter configure location jupyter notebook --generate-config## create certfile and keyfilemkdir ~/key &amp;&amp; cd ~/keyopenssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout j.key -out j.pem## configure jupyter vi ~/.jupyter/jupyter_notebook_conf.py### change passwd use pastec.NotebookApp.password=&apos;passwd&apos; ### change certfile and keyfile.NotebookApp.certfile = &apos;/home/user/key/j.pem&apos;c.NotebookApp.keyfile = &apos;/home/user/key/j.key&apos;### change listen ip c.NotebookApp.ip = &apos;192.168.0.230&apos;### disable open_browserc.NotebookApp.open_browser=False### change llisten portc.NotebookApp.port = 9088 添加开机启动123456789---jupyter.sh---#!/bin/bash# function: onboot start jupytercd /home/user/code &amp;&amp; jupyter notebook &gt; /dev/null 2&gt;&amp;1 &amp;---rc.local---# addsu - user -c &quot;sh /home/user/jupyter.sh&quot;chmod +x rc.local 添加防火墙端口123sufirewall-cmd --zone=public --add-port=9088/tcp --permanentfirewall-cmd --reload 测试打开浏览器，输入服务器地址https://jupyter:9088]]></content>
      <categories>
        <category>Python</category>
        <category>jupyter</category>
      </categories>
      <tags>
        <tag>jupyter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop基础学习]]></title>
    <url>%2F2019%2F07%2F29%2Fhadoop_learning.html</url>
    <content type="text"><![CDATA[HDFSHDFS的设计本质是为了大量的数据横跨成百上千台机器，用户看到的是一个文件系统，而不是很多的文件系统。例如我们引用一个路径中的数据/home/user/hdfs/file,我们引用的是一个路径，但是实际的数据存放在很多不同的机器上。HDFS就用来管理存储在不同机器上的数据。 计算引擎 Mareduce是第一代计算引擎，采用了很简化的计算模型，只有Map和Reduce两个计算过程(中间用Shuffle串联)。例如我们要统计在HDFS中存放的一个很大的文本文件中各个词出现的频率，我们首先会启动一个MapReduce程序，Map阶段会有很多机器读取文件的各个部分，分别把各自读到部分统计出词频;Reduce阶段同样会有很多机器从Mapper机器收到按照Hash分类的词的统计结果，Reducer会汇总相同词的词频，最终会得到整个文件的词频结果。MapReduce的模型比较简单，但是比较笨重。 第二代计算引擎Tez/Spark除了有内存、cache之类的新特性，还让Map和Reduce之间的界限模糊，数据交换更加灵活，更少的磁盘读写，更高的吞吐量。 Pig用接近脚本的方式描述MapReduce，Hive用SQL描述MapReduce，他们用脚本的SQL语言翻印MapReduce程序，然后让计算引擎去计算。 Hive是Hadoop的数据仓，严格来说不算是数据库，主要用于解决数据处理和计算问题，使用SQL来计算和处理HDFS上的结构化数据，适用于离线的批量数据计算。 Hbase是面向列的NoSQL数据库，用于快速读/写大量的数据，主要解决实时数据查询问题，应用场景多是海量数据的随机实时查询。 Storm是最流行的流计算平台，它的计算思路是：在数据流进来的是后就开始统计，好处是无延迟，但是短处是不灵活，要预先知道要统计的东西，毕竟数据流流过后就没有了。 调度系统 基础知识 yarn是目前较为流行的调度系统，负责资源调度、作业管理。在Yarn中有2种节点，一种是Resource Manager(master),另一种是Node Manager(slave). Resource Manager是Master上一个独立运行的进程，负责集群统一的资源管理、调度、分配等 Scheduler根据各个应用程序的资源需求进行资源分配，资源分配用一个抽象概念Container表示，它是一个动态资源分配单位，将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。 Applications Manager负责整个系统中所有的应用程序 Resource Tracker负责响应Node Manager的调度，例如节点的增加和删除 Node Manager是Slave上一个独立运行的进程，负责上报节点的状态 Node Manager是TaskTracker的一种更加普通和高效的版本，它拥有许多动态创建的资源容器。容器的大小取决于它包含的资源量。节点中容器的数量由配置参数与专用于从属后台进程和操作系统资源意外的资源总量共同决定。 Application Master运行在Slave上的组件 在用户提交一个应用程序时，Application Master进程实例会启动来协调应用程序内的所有任务的执行，包括监视、重启失败的任务、推测性运行缓慢的任务，以及计算应用程序计数器值的总和。它和它的应用程序均在受Node Manager控制的资源容器中运行。 Client向Resource Manager提交的每一个应用程序都必须有一个Application Master，它经过Resource Manager分配资源后，运行于某个Slave节点的Container中，具体做事情的Task也同样运行在某个Slave节点的Container中。RM、AM、NM、Container之间的通信，都是使用RPC机制。 Yarn资源管理和配置参数内存参数 Yarn允许用户配置每个节点上Yarn可用的物理内存，使用参数yarn.nodemanager.resource.memory-mb,默认大小是8192M yarn.nodemanager.vmem-pmem-ratio任务使用1M物理内存最多可使用虚拟内存，默认是2.1 yarn.nodemanager.peme-check-enabled是否启用一个线程检查每个任务正使用的物理内存量，如果超出任务分配值，直接kill，默认为true yarn.nodemanager.vmem-check-enabled是否启用一个线程检查每个任务使用的虚拟内存量，默认true yarn.scheduler.minimum-allocation-mb单个任务可以使用的最小物理内存量，默认1024M yarn.scheduler.maximum-allocation-mb单个任务可以使用的最大物理内存量，默认8192M CPU参数 yarn.nodemanager.resource.cpu-vcoresyarn在该节点上可使用的虚拟cpu个数，默认是8,推荐该值与物理cpu核数相同，不足8个，需要调小该值 yarn.schedulaer.minimum-allocation-vcores/yarn.scheduler.maximumallocation-vcores单个任务可申请的最小/最多cpu,最小为1,不足1的，默认使用1,最大默认是32]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>基础知识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop部署集群]]></title>
    <url>%2F2019%2F07%2F28%2Fhadoop_cluster_setup.html</url>
    <content type="text"><![CDATA[架构 h-master role:NameNode/JobTracker ip:192.168.0.210 app:hadoop/jdk jobs:主节点，总管分布式数据和分解任务的执行;主节点负责调度构成一个作业的所有任务 h-slave role:DataNode/Tasktracker ip:192.168.0.211 app:hadoop/jdk jobs:从节点，负责分布式数据存储以及任务的执行;从节点负责由主节点指派的任务 mapreduce框架 主节点JobTracker 每个从节点TaskTracker 安装步骤配置h-master无密码ssh登录h-slave123ssh-keygen -t rsa -b 2048ssh-copy-id hadoop@h-slavessh hadoop@h-slave 安装Java环境 安装JDK(All Servers)12345678910111213141516&lt;!--install_java.sh--&gt;#!/bin/bash# envjava_env=java.env# install jdk8rpm -ivh jdk-8u221-linux-x64.rpm# configure java envcp /etc/profile /etc/profile.oldcat $&#123;java_env&#125; &gt;&gt; /etc/profile.old# source profilesource /etc/profile# show java versionjava -version&lt;!--java.env--&gt;export JAVA_HOME=/usr/java/jdk1.8.0_221-amd64export PATH=$&#123;PATH&#125;:$&#123;JAVA_HOME&#125; 安装Hadoop(All Servers)h-master服务器 安装和配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110&lt;!--install_hadoop.sh--&gt;#!/bin/bash# envhadoop_dir=/opt/hadoop-3.1.2pwd_dir=$(echo `pwd`)host_name=$(echo &apos;hostname&apos;)# install hadooptar -xzvf hadoop-3.1.2.tar.gz -C /opt# create group and usergroupadd hadoopuseradd -g hadoop hadooppasswd hadoop# change owner of hadoop_dirchown -R hadoop:hadoop $&#123;hadoop_dir&#125;# source profilecp /etc/profile /etc/profile.addcat hadoop.env &gt;&gt; /etc/profilesource /etc/profile# set firewall rulefirewall-cmd --zone=public --add-rich-rule=&apos;rule family=&quot;ipv4&quot; source address=&quot;192.168.0.210&quot; port port=&quot;9000&quot; protocol=&quot;tcp&quot; accept&apos; --permanentfirewall-cmd --zone=public --add-rich-rule=&apos;rule family=&quot;ipv4&quot; source address=&quot;192.168.0.210&quot; port port=&quot;9870&quot; protocol=&quot;tcp&quot; accept&apos; --permanentfirewall-cmd --zone=public --add-rich-rule=&apos;rule family=&quot;ipv4&quot; source address=&quot;192.168.0.0/24&quot; port port=&quot;8088&quot; protocol=&quot;tcp&quot; accept&apos; --permanentfirewall-cmd --zone=public --add-rich-rule=&apos;rule family=&quot;ipv4&quot; source address=&quot;192.168.0.0/24&quot; port port=&quot;10020&quot; protocol=&quot;tcp&quot; accept&apos; --permanentfirewall-cmd --zone=public --add-rich-rule=&apos;rule family=&quot;ipv4&quot; source address=&quot;192.168.0.0/24&quot; port port=&quot;19888&quot; protocol=&quot;tcp&quot; accept&apos; --permanentfirewall-cmd --reload# configure hadoopmkdir /opt/hadoop-3.1.2/hdfs/namenode## configure xml filemv $&#123;hadoop_dir&#125;/etc/hadoop/core-site.xml $&#123;hadoop_dir&#125;/etc/hadoop/core-site.xml.oldmv $&#123;hadoop_dir&#125;/etc/hadoop/hdfs-site.xml $&#123;hadoop_dir&#125;/etc/hadoop/hdfs-site.xml.oldmv $&#123;hadoop_dir&#125;/etc/hadoop/mapred-site.xml $&#123;hadoop_dir&#125;/etc/hadoop/mapred-site.xml.oldmv $&#123;hadoop_dir&#125;/etc/hadoop/yarn-site.xml $&#123;hadoop_dir&#125;/etc/hadoop/yarn-site.xml.oldcp core-site-master.xml $&#123;hadoop_dir&#125;/etc/hadoop/core-site.xmlcp hdfs-site-master.xml $&#123;hadoop_dir&#125;/etc/hadoop/hdfs-site.xmlcp mapred-site-master.xml $&#123;hadoop_dir&#125;/etc/hadoop/mapred-site.xmlcp yarn-site-master.xml $&#123;hadoop_dir&#125;/etc/hadoop/yarn-site.xml## configure workers cd $&#123;hadoop_dir&#125;/etc/hadoop &amp;&amp; cp workers workser.oldsed -i &apos;1d&apos; workersecho &quot;h-slave&quot; &gt; workerschown -R hadoop:hadoop $&#123;hadoop_dir&#125;# configure hadoop_env.shecho &quot;export JAVA_HOME=/usr/java/jdk1.8.0_221-amd64&quot; &gt;&gt; $&#123;hadoop_dir&#125;/etc/hadoop/hadoop_env.sh&lt;!--hadoop.env--&gt;export HADOOP_HOME=/opt/hadoop-3.1.2export PATH=$&#123;PATH&#125;:$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin&lt;!--core-site-master.xml--&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://h-mster:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/home/hadoop/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;&lt;!--hdfs-site-master.xml--&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop-3.1.2/hdfs/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop-3.1.2/hdfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;&lt;!--mapred-site-master.xml--&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;http://h-master:9001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;&lt;!--yarn-site-master.xml--&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;http://h-master:9001&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;h-master:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;h-master:19888&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;/name&gt; &lt;value&gt;/home/hadoop/history/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.done-dir&lt;/name&gt; &lt;value&gt;/home/hadoop/history/done&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动 123456789# format hdfs on first startsu hadoophdfs namenode -format# startcd /opt/hadoop-3.1.2/sbin &amp;&amp; ./start-all.sh# start jobhistorymapred --daemon start jobhistory# show runing infojps 注意如果ssh端口不是默认的22，需要在文件hadoop_env.sh中修改，具体的位置是HADOOP_SSH_OPTS=&quot;-p 9022&quot; h-slave服务器 在主服务器中复制hadoop安装目录到slave 删除workers文件中的内容 配置环境变量 在hadoop_env.sh文件中添加java环境变量 Web UI Yarn WebUI http://h-master:8088 NameNode WebUI http://h-master:9870 JobHistory WebUI http://h-master:19888 参考 Hadoop多节点集群的构建 hadoop分布式集群搭建 How to Install and Set Up a 3-Node Hadoop Cluster Hadoop Cluster Setup]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>cluster</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive部署单节点过程]]></title>
    <url>%2F2019%2F07%2F27%2Fhadoop_hive_mysql.html</url>
    <content type="text"><![CDATA[Hive部署单节点过程 基础环境安装JDK8 安装jdk8sudo rpm -ivh jdk-8u221-linux-x64.rpm 配置环境变量123456suvi /etc/profile# addJAVA_HOME=/usr/java/jdk1.8.0_221-amd64. /etc/profilejava -version 安装Hadoop 下载hadoopwget http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-3.1.2/hadoop-3.1.2.tar.gz 解压tar -xzvf hadoop-3.1.2.tar.gz 移动文件sudo mv hadoop-3.1.2 /opt 创建用户和组sudo groupadd hadoop &amp;&amp; sudo useradd -g hadoop hadoop &amp;&amp; sudo passwd hadoop 修改权限 123cd /optsuchown -R hadoop:hadoop hadoop-3.1.2 配置用户环境变量 123456789101112131415161718su hadoopcd ~vi .bash_profile# add## JAVA env variablesexport JAVA_HOME=/usr/java/jdk1.8.0_221-amd64export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar## HADOOP env variablesexport HADOOP_HOME=/opt/hadoop-3.1.2export HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_YARN_HOME=$HADOOP_HOMEexport HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib/native&quot;export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin 测试hadoop是否安装成功hadoop dfs -ls 安装Hive 下载hivewget http://mirror.bit.edu.cn/apache/hive/hive-2.3.5/apache-hive-2.3.5-bin.tar.gz 解压sudo tar -xzvf apache-hive-2.3.5-bin.tar.gz /opt 修改用户和组cd /opt &amp;&amp; sudo mv apache-hive-2.3.5-bin/ hive-2.3.5 &amp;&amp; sudo chown -R hadoop:hadoopp hive-2.3.5 添加环境变量 1234567su hadoopcd ~vi .bash_profile# add## Hiveexport HIVE_HOME=/opt/hive-2.3.5export PATH=$HIVE_HOME/bin:$PATH 测试hive是否安装成功hive 配置单节点模式 安装mysql-connector-javasudo yum install mysql-connector-java -y 复制文件sudo cp /usr/share/java/mysql-connector-java.jar /opt/hive-2.3.5/lib &amp;&amp; cd /opt/hive-2.3.5/lib &amp;&amp; sudo chown hadoop:hadoop mysql-connector-java.jar 编辑hive-site.xml,如果没有，需要创建文件 12345678910111213141516171819202122232425262728293031323334353637383940414243su hadoopcd /opt/hive-2.3.5/conf &amp;&amp; touch hive-site.xml# add&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hive/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://mysql-master/hive_db&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;dba&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;passwd&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/home/hadoop/hive/log&lt;/value&gt; &lt;/property&gt;&lt;!--enable webui--&gt; &lt;property&gt; &lt;name&gt;hive.server2.webui.host&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.webui.port&lt;/name&gt; &lt;value&gt;10002&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.scratch.dir.permission&lt;/name&gt; &lt;value&gt;755&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;&lt;/configuration&gt; 配置mysql数据库 123create database hive_db character set utf8;grant all on hive_db.* to &apos;dba&apos;@&apos;hive&apos; identified by &apos;passwd&apos;;flush privileges; 初始化hive数据库schematool --dbType mysql --initSchema 在hive中可以使用hadoop命令dfs -ls / ; 在hive中可以执行简单的bash shell命令! pwd ; hive的历史命令存放在~/.hivehistory 启动hiveserver2服务hive --service hiveserver2 &amp; &gt; /dev/null 打开webuihttp://hive:10002/hiveserver2.jsp 参考 Hadoop: Setting up a Single Node Cluster.]]></content>
      <categories>
        <category>hive</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FineReport使用Linux服务器和Tomcat安装过程]]></title>
    <url>%2F2019%2F07%2F27%2Ffinereport_install.html</url>
    <content type="text"><![CDATA[FineReport使用Linux服务器和Tomcat安装过程 准备基础环境下载finereport 下载FineReport Linux版本下载地址:https://fine-build.oss-cn-shanghai.aliyuncs.com/finereport/10.0/tomcat/tomcat-linux.tar.gz在服务器中执行wget https://fine-build.oss-cn-shanghai.aliyuncs.com/finereport/10.0/tomcat/tomcat-linux.tar.gz 解压文件tomcat-linux.tar.gztar -xzvf tomcat-linux.tar.gz 移动文件sudo mv tomcat-linux /opt &amp;&amp; cd /opt &amp;&amp; sudo mv tomcat-linux finereport 如果使用finereport自带的tomcat和jre，可以直接启动tomcat，设定防火墙端口后直接使用，以下操作步骤是使用自行搭建的tomcat和使用mysql数据库。 下载Java 下载Java jdk版本下载地址:https://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html 安装Java jdksudo rpm -ivh jdk-8u221-linux-x64.rpm 配置java环境变量123456789101112sucd /etccp profile profile.oldvi profile # add java_home=/usr/java/jdk1.8.0_221-amd64 CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar PATH=$JAVA_HOME/bin:$PATH export JAVA_HOME CLASS_PATH PATH. profile# show java versionjava -version 下载tomcat 下载tomcatwget http://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-9/v9.0.22/bin/apache-tomcat-9.0.22.tar.gz 解压文件apache-tomcat-9.0.22.tar.gztar -xzvf apache-tomcat-9.0.22.tar.gz 将解压的文件夹移动到/opt下sudo mv apache-tomcat /opt &amp;&amp; cd /opt &amp;&amp; mv apache-tomcat tomcat 启动tomcat12cd /opt/tomcat/bin./startup.sh 添加防火墙规则 添加8080端口1234sufirewall-cmd --zone=public --add-port=8080/tcp --permanentfirewall-cmd --zone=public --add-source=192.168.0.0/24 --permanentfirewal-cmd --reload 部署finereport部署web文件 将webroot文件复制到tomcat目录下的webapps中 1sudo cp -r /opt/finereport/webapps/webroot /opt/tomcat/webapps/ 额外引入JDK的tools.jar 12cp /usr/java/jdk1.8.0_221-amd64/lib/tools.jar /opt/tomcat/libsudo cp /usr/java/jdk1.8.0_221-amd64/lib/tools.jar /opt/tomcat/webapps/webroot/WEB-INF/lib 注意：如果不执行复制tools.jar的操作，重启完tomcat后打开实例会出现错误HTTP Status 500 – Internal Server Error 重启tomcatcd /opt/tomcat/bin &amp;&amp; ./shutdown.sh &amp;&amp; ./startup.sh 在本地启动浏览器打开http://server_name:8080/webroot/decision 数据库部署 在数据库服务器中创建数据库create database finedb_t character set utf8; 授权数据库grant all on finedb_t.* to &#39;dba&#39;@&#39;finereport&#39; identified by &#39;passwd&#39;; 在web配置中，配置外接数据库 等待数据库配置完成,使用帐号密码登录 参考 部署应用至Linux上的tomcat]]></content>
      <categories>
        <category>finereport</category>
      </categories>
      <tags>
        <tag>finereport</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KVM虚拟化技术的内置快照和外置快照]]></title>
    <url>%2F2019%2F07%2F17%2Fkvm-snapshot.html</url>
    <content type="text"><![CDATA[磁盘快照 内置磁盘快照 内部磁盘快照使用单个qcow2文件来保存快照和快照之后的改动。这种快照是libvirt默认支持的方式，其缺点是只支持qcow2格式的磁盘镜像，而且过程较慢。 内置系统还原点 使用virsh save/restore命令，可以在虚拟机开机状态下保存内存状态、设备状态、磁盘装套到指定文件中，还原的是后虚拟机关机，使用virsh restore还原。(类似于休眠) 外置磁盘快照 外置磁盘快照创建的快照是一个只读文件,成为1个backing-file，快照后改动的内容存放到另一个qcow2文件,成为1个overlay，外置快照可以支持各种格式的磁盘镜像文件，外置快照的结果是形成一个qcow2文件链。快照状态为disk-snapshot的为外置快照。 外置系统还原点 虚拟机的磁盘磁盘状态被保存到一个文件中，内存和设备状态被保存到另一个文件中。 快照操作内置快照创建快照12345678910111213# 创建虚拟机内置快照virsh snapshot-create-as --domain test --name fresh# 查看快照virsh snapshot-list test# 还原快照virsh destroy testvirsh snapshot-revert --domain test fresh# 删除快照virsh snapshot-delete --domain test fresh# 创建内置系统还原点virsh save test /path/test.xml# 还原virsh restore /path/test.xml 外置快照创建快照12345678910# 查看虚拟机当前使用的磁盘文件virsh domblklist test# 创建外置快照virsh snapshot-create-as --domain test --name fresh --disk-only --diskspec vda,snapshot=external,file=/path/test_fresh.qcow2 --atomic# 查看当前虚拟机使用的磁盘文件按virsh domblklist test# 查看虚拟机的backing-fileqemu-img info test_fresh.qcow2# 查看快照链qemu-img info --backing-chain test_fresh.qcow2 注意：如果虚拟机存在多硬盘，在创建外置快照时，为保证原子性，需要添加参数atomic 合并快照虚拟机的快照链：12centos.qcow2(base-image)--test2.qcow2(磁盘镜像)--&gt;test2(虚拟机实例)--&gt;test2_test(快照1)--&gt;test2_nmap(快照2)--&gt;test2_net-tools(快照3)--&gt;test2_lsof(快照4) 外置快照可以用合并的方式缩短快照链，而不能通过删除的方式，因为每个快照中都保存相应的数据。合并快照的方式有2种：blockcommit向下合并和blockpull向上合并。 blockcommit blockcommit将top镜像合并至低层的base镜像，一旦合并完成，处在最上面的overlay将自动被指向低层的overlay或base，即合并overlay到backing-file。12# 合并快照2-3到快照1virsh blockcommit --domain test2 --base /path/test2_test.qcow2 --top /path/test2_net-tools.qcow2 --wait --verbose blockpullblockpull将backing-file向上合并至active。虚拟机快照链：centos.qcow2(base-image)–test2.qcow2(磁盘镜像)–&gt;test2(虚拟机实例)–&gt;test2_test(快照1)–&gt;test2_lsof(快照2)–&gt;test2_vim(快照3)–&gt;test2_htop(快照4) 123456# 合并快照1-3到当前使用的快照4中virsh blockpull --domain test2 --path /path/test2_htop.qcow2 --base /path/test2.qcow2 --wait --verbose# 迁移虚拟机，合并base-image到active,合并需要一段时间virsh blockpull --domain test2 --path /path/test2_htop.qcow2 --wait --verbose# 清理其他快照virsh snapshot-delete --domain test2 vim --metadata Tips在创建外置快照时出现Operation not supported: live disk snapshot not supported with this QEMU binary的错误提示，需要执行以下操作：123yum remove qemu-kvm -yyum install centos-release-qemu-ev -yyum install qemu-kvm-ev -y KVM的快照之间存在链式关系，快照链中在未执行合并前，不能删除快照链中的任意一个快照。]]></content>
      <categories>
        <category>kvm</category>
      </categories>
      <tags>
        <tag>虚拟化</tag>
        <tag>kvm</tag>
        <tag>内外快照</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络基础知识学习笔记]]></title>
    <url>%2F2019%2F07%2F10%2Fknownleage_network.html</url>
    <content type="text"><![CDATA[p地址，在网络中对用户是透明的，用户感受不到存在。路由器工作在网络层，根据ip地址寻址，还可以处理TCP/IP协议，具有分割广播的功能交换机工作在数据链路层，根据MAC地址识别，完成封装、转化数据包的功能，不具备分割广播的功能。 TCP/IP和OSI模型TCP/IP从下到上：链路层–网络层–传输层–应用层链路层封装、解封以太网首部;网络层封装、解封IP首部;传输层封装、解封TCP首部;应用层解封、封装应用协议。 OSI从下到上：物理层–数据链路层–网络层–运输层–会话层–表示层–应用层在现在流行的网络体系中已经不再使用会话和表示层 TCP/UDPTCPTCP工作在运输层，为应用层提供通信功能，是面向连接的传输控制协议。在传送数据之前必须要先建立连接，数据传送结束后要释放连接，不提供广播和多播功能，连接的建立需要通过“三次握手”，断开连接需要通过“4次握手”。通俗解释三次握手：1234主机A--我要给你发数据，同意吗？syn/seq--&gt;主机B主机A&lt;--同意，你发吧ack/syn--主机B主机A--我收到答复，我要传数据了ack--&gt;主机BSYN Flood攻击发生在第二次握手，让主机B一直处于等待状态，直到超时 通俗解释四次握手：1234主机A--FIN=1,数据传送完成,关闭--&gt;主机B主机A&lt;--ACK=1,收到--主机B主机A&lt;--我的数据接收完成，FIN=1，关闭--主机B主机A--收到，关闭，ACK=1--&gt;主机B UDPUDP工作在运输层，是一种无连接的用户数据报协议，UDP在传送数据之前不需要先建立连接，使用尽最大努力交付，不保证可靠交付，比TCP使用更少的资源 路由选择协议路由信息协议RIP路由信息协议RIP是内部网关协议中最先得到广泛应用的，是一种分布式基于距离向量的路由选择协议，最大特点是简单。RIP协议要求网络中的每一个路由器都要维护从它自己到其他每个目的网络的距离记录。从一路由器到直接连接的网络的距离定义为1,到非直接项链的网络的距离定义为所经过的路由器数加1,距离也称为跳数。RIP允许一条路径最多包含15个路由器，距离达到16就相当于不可达，RIP协议适应于小型网络。路由器R1到N1、N2网络的距离是1,到N3的距离是2,到N4的距离是3N1—R1—N2—R2—N3—R3—N4RIP协议中，路由器仅和相邻的路由器交换信息，交换的信息是当前本路由器中所知道的全部信息(自己的路由表)，信息的交换有固定的时间间隔。路由表中最重要的信息：到某个网络的最短距离，以及应经过的下一跳地址。路由表更新的原则是找出到每个目的网络的最短距离，这种算法被称为距离向量算法。RIP协议存在一个问题：网络出现故障时，要经过比较长的时间才能将此信息传递到所有路由器。好消息传得快，坏消息传的慢。 开放最短路径优先协议OSPF与RIP协议相比，OSPF协议具有以下特点： 向本自治系统中的所有路由器发送信息，采用的是洪泛法(RIP是相邻的路由器) 发送的信息是与本路由器相邻的所有路由器的链路状态(RIP是到所有网络的) 只有当链路状态发生变化时，路由器才用洪泛法发送消息(RIP是固定间隔)OSPF协议中，每个路由器都由一个链路状态数据库，这个数据库就是全网的拓扑结构，这个拓扑结构在全网范围内是一致的(链路状态数据库的同步)。OSPF将一个自治系统再划分为若干个更小的范围，叫区域，每个区域由一个32bit的区域标识符，主干区域标识符用0.0.0.0表示。OSPF不使用UDP传输，直接用IP数据报传送，工作在网络层。 FW/IDS/WAF/IPS防火墙是一种隔离技术，对流经的网络通信进行扫描，工作在网络层，用于过滤ip和协议类型，拦截低层攻击行为，但对应用层攻击无能为力。一般部署在外联出口或区域性出口位置，对内外流量进行安全隔离。IDS入侵检测系统，基本以旁路为主，特点是不阻断任何网络访问，主要以提供报告和是后监督为主。IPS入侵检测系统，解决了无法阻断的问题，基本一在线模式为主，系统提供多个端口，以头哦名模式工作，在防火墙中也会由提供该功能的产品，其特点是可以分析到数据包的内容，解决传统防火墙只能工作在4层以下的问题，IPS定义多种攻击模式，并通过模式匹配去阻断非法访问，缺点是不能主动学习攻击方式，对于不能识别的攻击模式，默认会允许。IPS会被串联在主干路上，对内外网异常流量进行监控处理。WAF是web应用防护系统，工作在应用层，一般放置在对外提供网站服务的DMZ区域或数据中心服务区域，位置应尽量靠近web服务器。工作模式有：透明代理(网桥模式)、反向代理、路由代理、端口镜像，前三种称为在线模式，需要将WAF串行部署在web服务器前端，用于检测并阻断异常流量;端口镜像称为离线模式，将WAF旁路在web服务器上游的交换机中，只用于检测异常流量。 透明模式(网桥模式)web客户端访问服务器时，连接请求被WAF拦截和监控，将会话分成2段，并基于桥模式进行转发，客户端感知不到WAF的存在，还可以开启基于硬件的Bypass功能，在WAF设备故障或掉电时不影响网络流量。在该模式下，所有流量都需要经过WAF，对WAF性能由要求，同时无法实现服务器负载均衡功能。 反向代理模式反向代理是将真实服务器地址映射到反向代理服务器上，客户端访问的就是WAF，在收到请求后WAF再将请求转发给后端的真实服务器，可以实现负载均衡。使用该模式需要在WAF中配置IP映射关系，WAF设备的地址和路由。 路由代理模式与网桥模式的区别是，工作在路由转发模块而非网桥模式，其他都一样，也不支持负载均衡。该模式需要为WAF的转发接口配置IP地址及路由，在该模式下可以直接作为web服务器的网站，但是存在单点故障问题，同时负责转发所有的流量。 端口镜像模式使用交换机的端口镜像功能，WAF只进不出，只对http流量进行监控和报警，对网络不会由任何影响。 IPsecIPSec是互联网安全协议，由2类协议组成：AH协议和ESP协议。 AH协议可以同时提供数据完整性确认、数据来源确认、防重方等安全特性，常用摘要算法：MD5、SHA1 ESP协议AH协议使用较少，因为其无法提供数据加密，所有数据传输时以明文传输，而ESP提供数据加密。VPN是IPSec的一种应用方式，提供site-to-site、end-to-site、end-to-end应用场景IPSec提供传输模式和隧道模式2种封装模式： 传输模式在AH、ESP处理前后IP头部保持不变，主要用于end-to-end场景; 隧道模式在AH、ESP处理后再封装了一个外网ip头，主要用于site-to-site场景; 隧道模式可以用于任何场景，传输模式只能用于end-to-end在使用IPSec加密数据包前，需要限建立一个IPSec的SA，可以手工建立也可以使用IKE(密钥交换协议)建立。IKE密钥交换协议属于混合协议，由ISAKMP协议和OAKLEY/SKEME组成，IKE与ISAKMP不同之处：IKE真正定义了一个密钥交换过程，ISAKMP只是定义了一个通用的可以被任何密钥交换协议使用的框架。IPSec VPN构架过程： 开始会话，建立连接 ISAKMP/IKE阶段一：协商阶段二的SA，建立IKE本身的安全信道 ISAKMP/IKE阶段二：协商IPSec的SA，通过阶段一建立的安全信道交换IPSec通信中使用的SA 数据连接建立 ————阶段一————–有2种工作模式：主模式和积极模式，使用UDP:500端口 主模式：3步双向交互过程，任何实体信息都可以免受攻击 SA交换：协商双方安全策略的过程 密钥交换：位第一对消息中所协商的算法生成密钥，通过DH密钥交换算法生成公钥，并交换公钥及随机数(防止重发攻击) ID交换及验证：进行身份验证和对整个SA交换进行验证 积极模式：3次交换协商密钥和进行验证，速度快，但信息实体是明文————阶段二————– 快速模式协商IPSec SA使用的安全参数，创建IPSec SA使用AH或ESP来加密数据流]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>网络基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[美团云DBProxy学习笔记]]></title>
    <url>%2F2019%2F06%2F30%2Fdbproxy.html</url>
    <content type="text"><![CDATA[基本信息DBProxy是美团点评开发和维护的基于Mysql协议的数据中间件，在Atlas基础上做了修改。提供的主要功能有：1 读写分离 从库负载均衡 IP过滤 分表 DBA平滑上下线DB 自动摘除宕机DB 监控信息完备 SQL过滤 从库流量配置 部署架构 安装 安装依赖yum install -y Percona-Server-devel-55.x86_64 Percona-Server-client-55.x86_64 Percona-Server-shared-55 jemalloc jemalloc-devel libevent libevent-devel openssl openssl-devel lua lua-devel bison flex libtool.x86_64 libffi-devel 安装glib2.4.2.0 123456wget https://src.fedoraproject.org/repo/pkgs/mingw-glib2/glib-2.42.0.tar.xz/71af99768063ac24033ac738e2832740/glib-2.42.0.tar.xztar -xJvf glib-2.42.0.tar.xzcd glib-2.42.0autoreconf -ivf./configure --refix=/usr/ --libdir=/usr/lib64/make -j20 &amp;&amp; make install 安装DBProxy 123456git clone https://github.com/Meituan-Dianping/DBProxy.gitcd DBProxysh autogen.shsh bootstrap.shmake -j20 &amp;&amp; make install# DBProxy默认的安装路径/usr/local/mysql-proxy,如果需要修改，修改文件bootstrap.sh中的--pefix路径 配置文件，添加开机启动 1234#!/bin/bash# chkconfig 23456 90 10# description: autostart dbproxy service onbootcd /usr/local/mysql-proxy/bin &amp;&amp; ./mysql-proxyd dbproxy start 登录管理后台 123456789101112131415161718192021mysql -u dba -p -P 3309 -h 192.168.0.130# 查看支持的命令select * from help;# 查看主从数据库select * from backends;# 添加主库，注意：在dbproxy中只能有1个主库add master 192.168.0.101:3306# 添加从数据库,可以设置权重add slave 192.168.0.104:3306@2# 从库添加|移除标签add|remove slave tag $tag_name $backend_ndx# 动态修改从库权重alter slave weight $backend_ndx $weight# 上线|下线从库idset online|offline $backend_ndx;# 删除从库remove backend $backend_ndx# 查看dbproxy中的用户select * from pwds;# 对dbproxy操作后需要进行保存配置save config; 参考 DBProxy Github项目地址 DBProxy配置手册]]></content>
      <categories>
        <category>dbproxy</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>dbproxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS部署OpenStack-Stein全过程]]></title>
    <url>%2F2019%2F06%2F17%2Fopenstack-install-Stein.html</url>
    <content type="text"><![CDATA[基本信息 安装 安装openstack-stein软件源yum install centos-release-openstack-stein -y 安装NTP服务yum install chrony -y 安装openstack客户端yum install python-openstackclient openstack-selinux -y 安装数据库1 安装mariadb-server pyhont2-pymysql 1yum install mariadb mariadb-server python2-pymysql -y 配置数据库 123456789vi /etc/my.cnf.d/mariadb-server.cnf# modify[mysqld]bind-address=192.168.122.11default-storage-engine=innodbinnodb_file_per_table=onmax_connections=4096collation-server=utf8_general_cicharacter-set-server=utf8 启动服务并执行安全检查 123systemctl start mariadb systemctl enable mariadbmysql_secure_installation 添加开放端口firewall-cmd --zone=internal --add-port=3306/tcp --permanent 安装消息服务rabbit2 安装yum install rabbitmq-server -y 启动服务 12systemctl start rabbitmq-serversystemctl enable rabbitmq-server 创建消息服务用户rabbitmqctl add_user rbtmq user_passwd 授权用户读写权限rabbitmqctl set_permissions rbtmq &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; 添加开放端口firewall-cmd --zone=internal --add-port=5672/tcp --permanent 安装认证缓存memcached3 安装yum install memcached python-memcached -y 配置 123vi /etc/sysconfig/memecached ## modifyOPTION=&quot;-l 127.0.0.1,::1,ops-ctr&quot; 启动服务 1234systemctl start memecachedsystemctl enable memecached# add firewall rulefirewall-cmd --zone=internal --add-port=11211 --permanent 安装etcd服务4 安装yum install etcd -y 配置 12345678910111213vi /etc/etcd/etcd.conf# modify#[Member]ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;http://192.168.122.11:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;http://192.168.122.11:2379&quot;ETCD_NAME=&quot;controller&quot;#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;http://192.168.122.11:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;http://192.168.122.11:2379&quot;ETCD_INITIAL_CLUSTER=&quot;controller=http://192.168.122.11:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster-01&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot; 启动服务 12systemctl start etcdsystemctl enabl etcd 添加开放端口 12firewall-cmd --zone=internal --add-port=2379/tcp --permanentfirewall-cmd --zone=internal --add-port=2380/tcp --permanent 安装placement服务10 数据库 123create database placement;grant all on placement.* to &apos;plcm_db&apos;@&apos;localhost&apos; identified by &apos;passwd&apos;;grant all on placement.* to &apos;plcm_db&apos;@&apos;%&apos; identified by &apos;passwd&apos;; 创建用户 123openstack user create --domain default \--password-prompt placementopenstackk role add --project service --user placement admin 创建服务实体 12openstack service --name placement \--description &quot;OpenStack Placement&quot; placement 创建endpoint 123openstack endpoint create --region RegionOne \placement public http://ops-ctr:8778# create internal admin endpoint like public 安装组件yum install openstack-placement-api -y 配置 12345678910111213141516171819202122232425vi /etc/placement/placement.conf[placement_database]connection=mysql+pymysql://plcm_db:passwd@ops-ctr/placement[api]auth_strategy=keystone[keystone_authtoken]auth_url=http://ops-ctr:5000/v3memcached_servers=ops-ctr:11211auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=placementpassword=passwdvi /etc/httpd/conf.d/00-placement-api.conf# add&lt;Directory /usr/bin&gt; &lt;IfVersion &gt;= 2.4&gt; Require all granted &lt;/IfVersion&gt; &lt;IfVersion &lt; 2.4&gt; Order allow,deny Allow from all &lt;/IfVersion&gt;&lt;/Directory&gt; 同步数据库/bin/sh -c &quot;placement-manage db sync&quot; placement 重启httpd服务systemctl restart httpd 安装openstack服务安装认证服务keystone5 数据库服务 12345# create databasecreate database keystone;# set permissiongrant all on keystone.* to &apos;kst_db&apos;@&apos;localhost&apos; identified by &apos;&apos;;grant all on keystone.* to &apos;kst_db&apos;@&apos;%&apos; identified by &apos;&apos;; 安装keystone组件yum install openstack-keystone httpd mod_wsgi -y 配置 1234567891011vi /etc/keystone/keystone.conf[database]connection=mysql+pymysql://kst_db:passwd@ops-cont/keystone[token]provider=fernet[signing]enable=truecertfile=/etc/pki/tls/private/pub.pemkeyfile=/etc/pki/tls/private/key.pemca_certs=/etc/pki/tls/certs/cert.pemcert_required=true 同步数据库/bin/sh -c &quot;keystone-manage db_sync&quot; keystone 初始化fernet 12keystone-manage fernet_setup --keystone-user keystone --keystone-group keystonekeystone-manage credential_setup --keystone-user keystone --keystone-group=keystone 创建bootstrap服务 12345keystone-manage bootstrap --bootstrap-password passwd \--bootstrap-admin-url http://ops-ctr:5000/v3 \--bootstrap-internal-url http://ops-ctr:5000/v3 \--bootstrap-public-url http://ops-ctr:5000/v3 \--bootstrap-region-id RegionOne 配置httpd服务 123456789101112131415161718192021222324vi /etc/httpd/conf/httpd.conf# addServerName ops-ctr# configure /etc/httpd/conf.d/wsgi-keystone.confListen 5000&lt;VirtualHost *:5000&gt; # SSLEngine on # SSLCertificateKeyFile /etc/pki/tls/private/key.pem # SSLCertificateFile /etc/pki/tls/private/cert.pem WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%&#123;GROUP&#125; WSGIProcessGroup keystone-public WSGIScriptAlias / /usr/bin/keystone-wsgi-public WSGIApplicationGroup %&#123;GLOBAL&#125; WSGIPassAuthorization On ErrorLogFormat &quot;%&#123;cu&#125;t %M&quot; ErrorLog /var/log/apache2/keystone.log CustomLog /var/log/apache2/keystone_access.log combined &lt;Directory /usr/bin&gt; Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt;# create linkln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/ 启用ssl加密链接，参考Apache enable ssl on centos 设置环境变量 1234567export OS_USERNAME=adminexport OS_PASSWORD=passwdexport OS_PROJECT_NAME=adminexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_DOMAIN_NAME=defaultexport OS_AUTH_URL=https://ops-ctr:5000/v3export OS_IDENTITY_API_VERSION=3 创建服务用户、角色、项目和域 1234567891011121314# create domain if needopenstack domain create --description &quot;mystack&quot; mystack# create projectopenstack project create --domain default \--description &quot;Service Project&quot; serviceopenstack project create --domain default \--description &quot;Demo Project&quot; demo# create useropenstack user create --domain default \--password-prompt demo# create roleopenstack role create demo# set role for useropenstack role add --project service --user demo demo 验证操作 12345678910111213# unset unset OS_AUTH_URL OS_PASSWORD# request new auth tokenopenstack --os-auth-url https://ops-ctr:5000/v3 \--os-project-domain-name default \--os-user-domain-name default \--os-project-name admin \--os-username admin token issueopenstack --os-auth-url https://ops-ctr:5000/v3 \--os-project-domain-name default \--os-user-domain-name default \--os-project-name demo \--os-username demo token issue 分别创建用户admin和demo的环境脚本 1234567891011121314151617181920admin-openrc---export OS_USERNAME=adminexport OS_PASSWORD=passwdexport OS_PROJECT_NAME=adminexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_DOMAIN_NAME=defaultexport OS_AUTH_URL=http://ops-ctr:5000/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2demo-openrc---export OS_USERNAME=demoexport OS_PASSWORD=passwdexport OS_PROJECT_NAME=demoexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_DOMAIN_NAME=defaultexport OS_AUTH_URL=http://ops-ctr:5000/v3export OS_IDENTITY_API_VERSION=3 安装镜像服务glance6 数据库 12create database glance;# set permission to glc_db on glance like keystone 创建用户 123openstack user create --domain default \--password-prompt glanceopenstack role add --project service --user glance admin 创建服务实体 123openstack service create \--name glance \--description &quot;OpenStack Image&quot; image 创建服务endpoint 1234openstack endpoint create --region RegionOne \image public http://ops-ctr:9292# create admin internal endpoint like public# add port 9292 by firewall-cmd 安装glance组件yum install openstack-glance -y 配置 123456789101112131415161718192021222324252627282930313233vi /etc/glance/glance-api.conf[database]connection=mysql+pymysql://glc_db:passwd@ops-ctr/glance[keystone_authtoken]www_authenticate_uri=http://ops-ctr:5000auth_url=http://ops-ctr:5000memcached_servers=ops-ctr:11211auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=glancepassword=passwd[paste_deploy]flavor=keystone[glance_store]stores=file,httpdefault_store=filefilesystem_store_datadir=/var/lib/glance/images/vi /etc/glance/glance-registry.conf[database]connection=mysql+pymysql://glc_db:passwd@ops-ctr/glancewww_authenticate_uri=http://ops-ctr:5000auth_url=http://ops-ctr:5000memcached_servers=ops-ctr:11211auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=glancepassword=passwd[paste_deploy]flavor=keystone 同步数据库glance-manage db_sync glance 启动服务12systemctl start openstack-glance-api openstack-glance-registrysystemctl enable openstack-glance-api openstack-glance-registry 如果glance-api服务启动失败，尝试修改/var/lib/glance/images和/var/log/glance/api.log的所属用户和组为glance:glance 验证操作123456789. admin-openrc# create image use cirros.imgopenstack image create &quot;cirros&quot; \--file /home/user/cirros-0.4.0-x86_64-disk.img \--disk-format qcow2 \--container-format bare \--public# show imageopenstack image list 安装计算服务nova8控制节点中安装nova服务 数据库 123456# create databasecreate database nova;create database nova_api;create database nova_cell0;# set permission like othersgrant all on nova.* to &apos;nva_db&apos;@&apos;localhost&apos; identified by &apos;passwd&apos;; 创建用户 123openstack user create --domain default \--password-prompt novaopenstack role add --project service --user nova admin 创建服务实体 12openstack service create --name nova \--description &quot;OpenStack Compute&quot; compute 创建endpoint 1234openstack endpoint create --region RegionOne \compute public http://ops-ctr:8774/v2.1# create internal admin endpoint like public# add port to firewall 安装nova组件yum install openstack-nova-api openstack-nova-conductor openstack-nova-novncproxy openstack-nova-scheduler openstack-nova-console -y 配置组件 1234567891011121314151617181920212223242526272829303132333435363738394041vi /etc/nova/nova.conf[DEFAULT]enabled_apis=osapi_compute,metadatatransport_url=rabbit://rbtmq:paswd@ops-ctrmy_ip=192.168.122.11use_neutron=truefirewall_driver=nova.virt.firewall.NoopFirewallDriver[database]connection=mysql+pymysql://nva_db:passwd@ops-ctr/nova[api_database]connection=mysql+pymysql://nva_db:passwd@ops-ctr/nova_api[api]auth_strategy=keystone[keystone_authtoken]auth_url=http://ops-ctr:5000/v3memcached_servers=ops-ctr:11211auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=novapassword=passwd[vnc]enabled=trueserver_listen=$my_ipserver_proxyclient_address=$my_ip[glance]api_servers=http://ops-ctr:9292[oslo_concurrency]lock_path=/var/lib/nova/tmp[placement]region_name=RegionOneauth_url=http://ops-ctr:5000/v3auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=placementpassword=passwd[scheduler]discover_hosts_in_cells_interval=300 同步数据库 123456/bin/sh -c &quot;nova-manage api_db sync&quot; nova/bin/sh -c &quot;nova-manage cell_v2 map_cell0&quot; nova/bin/sh -c &quot;nova-manage cell_v2 create_cell --name cell1 --verbose&quot; nova/bin/sh -c &quot;nova-manage db sync&quot; nova# show cellsnova-manage cell_v2 list_cells nova 启动服务 12systemctl start openstack-nova-api openstack-nova-consoleauth openstack-nova-scheduler openstack-nova-conductor openstack-nova-novncproxysystemctl enable openstack-nova-api openstack-nova-consoleauth openstack-nova-scheduler openstack-nova-conductor openstack-nova-novncproxy 将计算节点添加到cell数据库 1234# show compute serviceopenstack compute service list # discover compute node/bin/sh -c &quot;nova-manage cell_v2 discover_hosts --verbose&quot; nova 确认操作12345# show endpointopenstack catalog list# show imageopenstack image listnova-status upgrade check 计算节点中安装nova服务9 安装组件yum install openstack-nova-compute -y 配置 1234567891011121314151617181920212223242526272829303132333435363738vi /etc/nova/nova.conf[DEFAULT]enabled_apis=osapi_compute,metadatatransport_url=rabbit://rbtmq:passwd@ops-ctrmy_ip=192.168.122.12use_neutron=truefirewall_driver=nova.virt.firewall.NoopFirewallDriver[api]auth_strategy=keystone[keystone_authtoken]auth_url=http://ops-ctr:5000/v3memcached_servers=ops-ctr:11211auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=novapassword=passwd[vnc]enabled=trueserver_listen=0.0.0.0server_proxyclient_address=$my_ipnovncproxy_base_url=http://ops-ctr:6080/vnc_auto.html[glance]api_servers=http://ops-ctr:9292[oslo_concurrency]lock_path=/var/lib/nova/tmp[placement]region_name=RegionOneauth_url=http://ops-ctr:5000/v3auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=placementpassword=passwd[libvirt]virt_type=qemu 启用虚拟化 12egrep -c &apos;(vmx|svm)&apos; /proc/cpuinfo结果：0-qemu 启动服务 12systemctl start lilbvirtd openstack-nova-computesystemctl enable lilbvirtd openstack-nova-compute 安装网络服务neutron控制节点 数据库 123create database neutron;grant all on neutron.* to &apos;ntr_db&apos;@&apos;localhost&apos; identified by &apos;passwd&apos;;grant all on neutron.* to &apos;ntr_db&apos;@&apos;%&apos; identified by &apos;passwd&apos;; 创建用户 123openstack user create --domain default \--password-prompt neutronopenstack role add --project service --user neutron admin 创建服务实体和endpoint 创建网络 私有网络 安装组件yum install openstack-neutron openstack-neutron-ml2 openstack-neutron-linuxbridge ebtables -y 配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061vi /etc/neutron/neutron.conf[DEFAULT]core_plugin=ml2service_plugins=routerallow_overlapping_ips=truetransport_url=rabbit://rbtmq:passwd@ops-ctrauth_strategy=keystonenotify_nova_on_port_status_changes=truenotify_nova_on_port_data_changes=true[database]connection=mysql+pymysql://ntr_db:passwd@ops-ctr/neutron[keystone_authtoken]www_authenticate_uri=http://ops-ctr:5000auth_url=http://ops-ctr:5000memcached_servers=ops-ctr:11211auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=neutronpassword=passwd[nova]auth_url=http://ops-ctr:5000auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=novapassword=passwd[oslo_concurrency]lock_path=/var/lib/neutron/tmpvi /etc/neutron/plugins/ml2/ml2_conf.ini[ml2]type_drivers=flat,vlan,vxlantenant_network_types=vxlanmechanism_drivers=linuxbridge,l2populationextension_drivers=port_security[ml2_type_flat]flat_networks=provider[ml2_type_vxlan]vni_ranges=1:1000[securitygroup]enable_ipset=truevi /etc/neutron/plugins/ml2/linuxbridge_agent.ini[linux_bridge]physical_interface_mappings=provider:eth1[vxlan]enable_vxlan=truelocal_ip=192.168.122.11l2_population=true[securitygroup]enable_security_group=truefirewall_driver=neutron.agent.linux.iptables_firewall.IptablesFirewallDrivervi /etc/neutron/l3_agent.ini[DEFAULT]interface_driver=linuxbridgevi /etc/neutron/dhcp_agent.ini[DEFAULT]interface_driver=linuxbridgedhcp_driver=neutron.agent.linux.dhcp.Dnsmasqenable_isolated_metadata=true 配置元数据 1234vi /etc/neutron/metadata_agent.ini[DEFAULT]nova_metadata_host=ops-ctrmetadata_proxy_shared_secret=passwd 配置nova服务 123456789101112vi /etc/nova/nova.conf[neutron]url=http://ops-ctr:9696auth_url=http://ops-ctr:5000auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=neutronpassword=passwdservice_metadata_proxy=truemetadata_proxy_shared_secret=passwd 同步数据库 123ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini/bin/sh -c &quot;neutron-db-manage --config-file /etc/neutron/neutron.conf \--config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head&quot; neutron 启动服务 123systemctl restart openstack-nova-apisystemctl start neutron-server neutron-linuxbridge-agent neutron-dhcp-agent neutron-metadata-agent neutron-l3-agentsystemctl enable neutron-server neutron-linuxbridge-agent neutron-dhcp-agent neutron-metadata-agent neutron-l3-agent 验证操作openstack network agent list计算节点有1个服务，控制节点有4个服务 计算节点 安装组件yum install openstack-neutron-linuxbridge ebtables ipset -y 配置 123456789101112131415161718192021222324252627282930313233343536vi /etc/neutron/neutron.conf[DEFAULT]transport_url=rabbit://rbtmq:passwd@ops-ctrauth_strategy=keystone[keystone_authtoken]www_authenticate_uri=http://ops-ctr:5000auth_url=http://ops-ctr:5000memcached_servers=ops-ctr:11211auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=neutronpassword=passwd[oslo_concurrency]lock_path=/var/lib/neutron/tmpvi /etc/nova/nova.conf[neutron]url=http://ops-ctr:9696auth_url=http://ops-ctr:5000auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=neutronpassword=passwdvi /etc/neutron/plugins/ml2/linuxbridge_agent.ini[linux_bridge]physical_interface_mappings=provider:eth1[vxlan]enable_vxlan=truelocal_ip=192.168.122.12l2_population=true[securitygroup]enable_security_group=truefirewall_driver=neutron.agent.linux.iptables_firewall.IptablesFirewallDriver 启动服务 123systemctl restart openstack-nova-computesystemctl start neutron-linuxbridge-agentsystemctl enable neutron-linuxbridge-agent 安装UI服务horizon12 安装组件yum install openstack-dashboard -y 配置 123456789101112131415161718192021222324vi /etc/openstack-dashboard/local_settingsOPENSTACK_HOST=&quot;ops-ctr&apos;ALLOW_HOSTS=[&apos;*&apos;, ]SESSION_ENGINE=&apos;django.contrib.sessions.backends.cache&apos;CACHE=&#123; &apos;default&apos;:&#123; &apos;BACKEND&apos;:&apos;django.core.cache.backends.memcached.MemcachedCache&apos;, &apos;LOCALTION&apos;:&apos;ops-ctr:11211&apos;, &#125;&#125;OPENSTACK_KEYSTONE_URL=&quot;http://%s:5000/v3 % OPENSTACK_HOST&quot;OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT=True# True要使用大写，用小写会报错OPENSTACK_VERSIONS=&#123; &quot;identity&quot;:3, &quot;image&quot;:2, &quot;volume&quot;:3,&#125;OPENSTACK_KEYSTONE_DEFAULT_DOMAIN=&quot;default&quot;OPENSTACK_KEYSTONE_DEFAULT_ROLE=&quot;demo&quot;TIME_ZONE=&quot;Asia/Shanghai&quot;vi /etc/httpd/conf.d/openstack-dashboard.conf# addWSGIApplicationGroup %&#123;GLOBAL&#125; 重启服务 1systemctl restart httpd memcached 添加一个存储节点控制节点13 数据库 12create database cinder;grant all on cinder.* to &apos;cid_db&apos;@&apos;localhost&apos; identified by &apos;passwd&apos;; 创建用户、角色、endpoint和2个服务： cinderv2,cinderv3,类型分别是volumev2,volumev3 v2 endpoint地址http://ops-ctr:8776/v2/%\(project_id\)s v3 endpoint地址http://ops-ctr:8776/v3/%\(project_id\)s 安装组件yum install openstack-cinder -y 配置 12345678910111213141516171819vi /etc/cinder/cinder.conf[DEFAULT]transport_url=rabbit://rbtmq:passwd@ops-ctrauth_strategy=keystonemy_ip=192.168.122.11[database]connection=mysql+pymysql://cid_db:passwd@ops-ctr/cinder[keystone_authtoken]www_authenticate_uri=http://ops-ctr:5000auth_url=http://ops-ctr:5000memcached_servers=ops-ctr:11211auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=cinderpassword=passwd[oslo_concurrency]lock_path=/var/lib/cinder/tmp 同步数据库/bin/sh -c &quot;cinder-manage db sync&quot; cinder 配置计算服务使用块存储 1234vi /etc/nova/nova.conf# add[cinder]os_region_name=RegionOne 启动服务 123systemctl restart openstack-nova-apisystemctl start openstack-cinder-api openstack-cinder-schedulersystemctl enable openstack-cinder-api openstack-cinder-scheduler 检查操作openstack volume service list 存储节点 安装组件yum install lvm2 device-mapper-persistent-data -y 创建逻辑分区pvcreate /dev/vdb 创建逻辑卷组vgcreate cinder /dev/vdb 添加过滤器 12vi /etc/lvm/lvm.conffilter=[&quot;a/dev/vda/&quot;,&quot;a/dev/vdb/&quot;,&quot;r/.*/&quot;] 安装cinder组件yum install openstack-cinder targetcli python-keystone -y 配置 1234567891011121314151617181920212223242526vi /etc/cinder/cinder.conf[DEFAULT]transport_url=rabbit://rbtmq:passwd@ops-ctrauth_strategy=keytonemy_ip=192.168.122.13enabled_backends=lvmglance_api_servers=http://ops-ctr:9292[database]connection=mysql+pymysql://cid_db:passwd@ops-ctr/cinder[keystone_authtoken]www_authenticate_uri=http://ops-ctr:5000auth_url=http://ops-ctr:5000memcached_servers=ops-ctr:11211auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=cinderpassword=passwd[lvm]volume_driver=cinder.volume.drivers.lvm.LVMVolumeDrivervolume_group=cindertarget_protocol=iscsitarget_helper=lioadm[oslo_concurrency]lock_path=/var/lib/cinder/tmp 启动服务 12systemctl start openstack-cinder-volume targetsystemctl enable openstack-cinder-volume target 创建实例创建公有网络15 创建网络 123456. admin-openrc# 使用provider创建1个flat类型的网络，名称为provideropenstack network create \--share --external \--provider-physical-network provider \--provider-network-type flat provider 创建子网1234567使用创建的provider网络，创建1个192.168.0.200-240范围的子网openstack subnet create \--network provider \--allocation-pool start=192.168.0.200,end=192.168.0.240 \--dns-nameserver 192.168.0.1 \--gateway 192.168.0.1 \--subnet-range 192.168.0.0/24 provider 创建私有网络16 创建网络 123. demo-openrcopenstack network create selfservice 创建子网 12345openstack subnet create \--network selfservice \--dns-nameserver 192.168.0.1 \--gateway 192.168.100.1 \--subnet-range 192.168.100.0/24 selfservice 创建路由openstack router create self-router 将selfservice网络添加到路由中openstack router add subnet self-router selservice 在路由中设置公网网关openstack router set self-router --external-gateway provider 检查操作123. admin-openrcip netnsopenstack port list --router self-router 创建实例 创建最小规格的主机,内存64M，硬盘1G，名称m1.nanoopenstack flavor create --id 0 --vcpus 1 --ram 64 --disk 1 m1.nano 添加密钥对openstack keypair create --public-key ~/.ssh/id_rsa.pub mykey 添加安全规则到default安全组中1234# allow pingopenstack security group rule create --proto icmp default# allow ssh openstack security group rule create --proto tcp --dst-port 22 default 创建主机 私网主机17 123456. demo-openrcopenstack server create --flavor m1.nano \--image cirros \--nic net-id=c34add94-6f4d-4312-92f9-ac4ad426bce7 \--security-group default \--key-name mykey self-host 查看创建的主机openstack server list 虚拟终端访问主机openstack console url show self-host 远程访问主机123456# create float ipopenstack floating ip create provider# associate floating ip with self-hostopenstack server add floating ip self-host 192.168.0.234# show server listopenstack server list 参考 install database on centos install rabbitmq-server on centos install memcached on centos install etcd on centos install keystone on centos install glance on centos enable ssl on keystone install nova on centos compute server install nova install placement on centos incell neutron on centos install horizon on centos install cinder on centos for controller install cinder on centos for storage create provider network create self-service network create self-host in selfservice]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>OpenStack</tag>
        <tag>Stein</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS部署OpenStack过程-块存储服务]]></title>
    <url>%2F2019%2F06%2F12%2Fopenstack-install-cinder.html</url>
    <content type="text"><![CDATA[安装并配置控制节点安装条件 数据库1234567# loginmysql -u root -p# create databasecreate database cinder;# grant privilegegrant all privileges on cinder.* to &apos;cid_db&apos;@&apos;localhost&apos; identified by &apos;passwd&apos;;grant all privileges on cinder.* to &apos;cid_db&apos;@&apos;192.168.122.%&apos; identified by &apos;passwd&apos;; 获取admin凭证. admin-openrc 创建用户 123456# create useropenstack user create \--domain default \--password-prompt cinder# create roleopenstack role --project service --user cinder admin 创建服务实体块存储服务要求2个服务实体 123456# create cinderopenstack service create --name cinder \--description &quot;OpenStack Block Storge&quot; volume# create cinderv2openstack service create --name cinderv2 \--description &quot;OpenStack Block Storge&quot; volumev2 创建endpoint 123456# create volume endpoint for public internal adminopenstack endpoint create region RegionOne \volume public http://ops-cont:8776/v1/%\(tenant_id\)s# create volumev2 endpoint like volumeopenstack endpoint create region RegionOne \volumev2 public http://ops-cont:8776/v2/%\(tenant_id\)s 安装配置组件 安装组件yum install openstack-cinder -y 配置cinder.conf 123456789101112131415161718192021222324252627vi /etc/cinder/cinder.conf# configure database in [database]connection=mysql+pymysql://cid_db:db_passwd@ops-cont/cinder# configure [DEFAULT][DEFAULT]rpc_backend=rabbitauth_strategy=keystonemy_ip=192.168.122.200# configure rabbit in [oslo_messaging_rabbit][oslo_messaging_rabbit]rabbit_host=ops-contrabbit_id=openstackrabbit_password=passwd# configure auth in [keystone_authtoken][keystone_authtoken]auth_uri=http://ops-cont:5000auth_url=http://ops-cont:35357memcached_servers=ops-cont:11211auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=cinderpassword=passwd# configure lock path in [oslo_concurrency][oslo_concurrency]lock_path=/var/lib/cinder/tmp 同步数据库su -c /bin/sh -c &quot;cinder-manage db sync&quot; cinder 配置计算节点使用块存储在计算节点中执行的操作 1234vi /etc/nova/nova.conf# add region name in [cinder][cinder]os_region_name=RegionOne 启动服务 12345# restart nova-apisystemctl restart openstack-nova-api# start cinder servicesystemctl start openstack-cinder-api openstack-cinder-schedulersystemctl enable openstack-cinder-api openstack-cinder-scheduler 验证操作添加存储节点后，在控制节点中执行验证操作：cinder service-list 添加并配置存储节点lvm工具会扫描/dev目录，照抄包含卷的块存储设备，如果项目在系统卷上使用lvm，扫描工具检测到这些卷时会尝试缓存它们，这样会在底层操作系统和项目卷上产生问题。在配置lvm时，需要创建一个过滤器，只接受/dev/sdb设备，拒绝其他所有设备。过滤器组中的元素都以a开头，即accept,或以r开头，即reject,并且包括一个设备名称的正则表达式规则，以r/.*/结束，如果操作系统使用了lvm，也必须要把操作系统相关设备添加到过滤器中。 将新添加的硬盘添加到lvm中 12345678# create pvpvcreate /dev/vdb# create lvm groupvgcreate -s 4M cinder /dev/vdb# create filtervi /etc/lvm/lvm.conf# create a filterfilter=[&quot;a/vda/&quot;,&quot;a/vdb/&quot;,&quot;r/.*/&quot;] 安装并配置组件 1234567891011121314151617181920212223242526272829303132333435# installyum install centos-release-openstack-ocata -yyum install openstack-cinder targetcli python-keystone -yyum install mariadb python2-pymysql -y# configure cinder.confvi /etc/cinder/cinder.conf[DEFAULT]rpc_backend=rabbitauth_strategy=keystonemy_ip=192.168.122.90enabled_backends=lvmglance_api_servers=http://ops-cont:9292[database]connection=mysql+pymysql://cid_db:db_passwd@ops-cont/cinder[oslo_messaging_rabbit]rabbit_host=ops-contrabbit_id=openstackrabbit_password=passwd[keystone_authtoken]auth_uri=http://ops-cont:5000auth_url=http://ops-cont:35357memcached_servers=ops-cont:11211auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=cinderpassword=passwd[lvm]volume_driver=cinder.volume.drivers.lvm.LVMVolumeDrivervolume_group=cinderiscsi_protocol=iscsiiscsi_helper=lioadm[oslo_concurrency]lock_path=/var/lib/cinder/tmp 启动服务 12systemctl start openstack-cinder-volume targetsystemctl enable openstack-cinder-volume target 参考 计算节点配置块存储服务 添加一个块存储节点]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>OpenStack</tag>
        <tag>cinder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS部署OpenStack过程-Dashboard]]></title>
    <url>%2F2019%2F06%2F11%2Fopenstack-install-dashboard.html</url>
    <content type="text"><![CDATA[安装配置组件 安装组件yum install openstack-dashboard -y 配置 1234567891011121314151617181920212223242526272829vi /etc/openstack-dashboard/local_settings# configure dashboard_hostOPENSTACK_HOST=&quot;ops-cont&quot;# allow all host visit dashboardALLOWED_HOSTS=[&apos;*&apos;, ]# configure memcachedSESSION_ENGINE=&apos;django.contrib.sessions.backends.cache&apos;CACHES=&#123; &apos;default&apos;:&#123; &apos;BACKEND&apos;:&apos;django.core.cache.backends.memcached.MemcachedCache&apos;, &apos;LOCATION&apos;:&apos;ops-cont:11211&apos;， &#125;&#125;# enable auth v3OPENSTACK_KEYSTONE_URL=&quot;http://%s:5000/v3&quot; % OPENSTACK_HOSTS# enable domain supportOPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT=True# configure api versionOPENSTACK_API_VERSIONS=&#123; &quot;identity&quot;:3, &quot;image&quot;:2, &quot;volume&quot;:2,&#125;# configure default domainOPEMSTACK_KEYSTONE_DEFAULT_DOMAIN=&quot;default&quot;# configure default roleOPENSTACK_KEYSTONE_DEFAULT_ROLE=&quot;user&quot;# configure timezoneTIME_ZONE=&quot;Asia/Shanghai&quot; 重启web,memcached服务systemctl restart httpd memcached 验证操作打开浏览器，输入地址：http://ops-cont/dashboard 注意按照官方文档配置完成后，在执行登录操作的时候httpd日志会报如下错误提示： “Unable to create a new session key. “RuntimeError: Unable to create a new session key. It is likely that the cache is unavailable. 根据错误提示，需要修改SESSION_ENGINE,将其修改为&#39;django.contrib.sessions.backends.file&#39;即可正常登录。2 参考 安装dashboard openstack中dashboard页面RuntimeError]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>OpenStack</tag>
        <tag>dashboard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS部署OpenStack过程-网络服务]]></title>
    <url>%2F2019%2F06%2F11%2Fopenstack-install-neutron.html</url>
    <content type="text"><![CDATA[控制节点安装条件 数据库 1234567# loginmysql -u root# create databasecreate database neutron;# grant privilegegrant all privileges on neutron.* to &apos;nt_db&apos;@&apos;localhost&apos; identified by &apos;passwd&apos;;grant all privileges on neutron.* to &apos;nt_db&apos;@&apos;192.168.122.%&apos; identified by &apos;passwd&apos;; 创建用户 123456# create useropenstack user create \--domain default \--password-prompt neutron# create roleopenstack role add --project service --user neutron admin 创建服务实体 123openstack service create \--name neutron \--description &quot;OpenStack Networking&quot; network 创建endpoint 12345# create public endpointopenstack endpoint create \--region RegionOne \network public http://ops-cont:9696# create internal admin endpoint like public 创建私有网络 安装组件yum install openstack-neutron openstack-neutron-ml2 openstack-neutron-linuxbridge ebtables -y 配置组件 12345678910111213141516171819202122232425262728293031323334353637vi /etc/neutron/neutron.conf# configure database in [database]connection=mysql+pymysql://nt_db:db_passwd@ops-cont/neutron# configure ml2 plugin、rabbit、keystone_authtoken in [DEFAULT]core_plugin=ml2service_plugins=routerallow_overlapping_ips=Truerpc_backend=rabbitauth_strategy=keystonenotify_nova_on_port_status_changes=Truenotify_nova_on_port_data_changes=True# configure [oslo_messaging_rabbit][oslo_messaging_rabbit]rabbit_host=ops-contrabbit_userid=openstackrabbit_password=passwd# configure keystone_auth in [keystone_authtoken]auth_uri=http://ops-cont:5000auth_url=http://ops-cont:35357memcached_servers=ops-cont:11211auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=neutronpassword=passwd# configure nova in [nova]auth_url=http://ops-cont:35357auth_type=passwordregion_name=RegionOneproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=novapassword=passwd# configure lock path in [oslo_concurrency]lock_path=/var/lib/neutron/tmp 配置ml2插件 123456789101112131415161718vi /etc/neutron/plugins/ml2/ml2.conf.ini# enable flat,vlan,vxlan in [ml2]# enable linuxbridge l2# disable other type_drivers[ml2]type_drivers=flat,vlan,vxlantenant_network_types=vxlanmechanism_drivers=linuxbridge,l2populationextension_drivers=port_security#configure flat network in [ml2_type_flat][ml2_type_flat]flat_networks=provider# configure vxlan in [ml2_type_vxlan][ml2_type_vxlan]vni_ranges=1:1000# enable ipset in [securitygroup][securitygroup]enabled_ipset=true 配置linuxbridge代理 12345678910111213vi /etc/neutron/plugins/ml2/linuxbridge_agent.ini# link nic with eth in [linux_bridge][linux_bridge]physical_interface_mappings=provider:eth1# configure vxlan in [vxlan][vxlan]enable_vxlan=truelocal_ip=192.168.122.200l2_population=true# configure security in [securitygroup][securitygroup]enable_security_group=truefirewall_driver=neutron.agent.linux.iptables_firewall.IptablesFirewallDriver 配置l3代理 1234vi /etc/neutron/l3_agent.ini[DEFAULT]interface_driver=neutron.agent.linux.interface.BridgeInterfaceDriverexternal_network_bridge= 配置DHCP 12345vi /etc/neutron/dhcp_agent.ini[DEFAULT]interface_driver=neutron.agent.linux.interface.BridgeInterfaceDriverdhcp_driver=neutron.agent.linux.dhcp.Dnsmasqenable_isolated_metadata =true 配置元数据代理1234vi /etc/neutron/metadata_agent.ini[DEFAULT]nova_metadata_ip=ops-contmetadata_proxy_shared_secret=passwd 为计算节点配置网络 配置nova.conf 12345678910111213vi /etc/nova/nova.conf[neutron]url=http://ops-cont:9696auth_url=http://ops-cont:35357auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultregion_name=RegionOneproject_name=serviceusername=neutronpassword=passwdservice_metadata_proxy=truemetadata_proxy_shared_secret=passwd 创建1个链接ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini 同步数据库su -c /bin/sh -c &quot;neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/nemutron/plugins/ml2/ml2_conf.ini upgrade head&quot; neutron 重启api服务systemctl restart openstack-nova-api 启动网络服务 12systemctl start neutron-server neutron-linuxbridge-agent neutron-dhcp-agent neutron-metadata-agent neutron-l3-agentsystemctl enable neutron-server neutron-linuxbridge-agent neutron-dhcp-agent neutron-metadata-agent neutron-l3-agent 验证操作在计算节点中的操作完成后，在控制节点中执行验证操作neutron ext-list neutron agent-list,输出结果应该包含控制节点上4个代理和每个计算节点上1个代理 计算节点 安装组件yum install openstack-neutron-linuxbridge ebtables ipset -y 配置组件 123456789101112131415161718192021222324vi /etc/neutron/neutron.conf# disable all connection in [database]# configure rabbit in [DEFAULT] [oslo_messaging_rabbit]# configure auth[DEFAULT]rpc_backend=rabbitauth_strategy=keystone[oslo_messaging_rabbit]rabbit_host=ops-contrabbit_userid=openstackrabbit_password=passwd[keystone_authtoken]auth_uri=http://ops-cont:5000auth_url=http://ops-cont:35357memcached_servers=ops-cont:11211auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultregion_name=RegionOneproject_name=serviceusername=neutronpassword=passwd[oslo_concurrency]lock_path=/var/lib/neutron/tmp 为计算节点配置网络服务 1234567891011vi /etc/nova/nova.conf[neutron]url=http://ops-cont:9696auth_url=http://ops-cont:35357auth_type=passwordprojrct_domain_name=defaultuser_domain_name=defaultregion_name=RegionOneproject_name=serviceusername=neutronpassword=passwd 重启计算服务systemctl restart openstack-nova-compute 启动linuxbridge服务12systemctl start neutron-linuxbridge-agentsystemctl enable neutron-linuxbridge-agent 注意按照官方文档配置后，在计算节点中启动neutron-linuxbridge-agent服务时，会有如下报错： neutron.plugins.ml2.drivers.linuxbridge.agent.linuxbridge_neutron_agent [-] Tunneling cannot be enabled without the local_ip bound to an interface on the host. Please configure local_ip None on the host interface to be used for tunneling and restart the agent. 根据提示，需要在/etc/neutron/plugins/ml2/linuxbridge-agent.ini中的[vxlan]部分中添加本地ip地址local_ip=192.168.122.100 参考 为控制节点配置网络 为计算节点配置网络]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>OpenStack</tag>
        <tag>neutron</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS部署OpenStack过程-计算服务部署]]></title>
    <url>%2F2019%2F06%2F11%2Fopenstack-install-nova-compute.html</url>
    <content type="text"><![CDATA[基本信息OpenStack计算组件请求OpenStack Identiy服务进行认证;请求OpenStack Image服务提供磁盘镜像;为OpenStack dashboard提供用户管理域管理员接口。OpenStack计算服务主要的组件： nova-api服务 接受和响应来自最终用户的计算API请求 nova-api-metadata服务 接受来自虚拟机发送的元数据请求，一般在安装nova-network服务的多主机模式下使用 nova-compute服务 一个持续工作的守护进程，通过Hypervior来创建和销毁虚拟机实例 nova-scheduler服务 拿到一个来自队列请求虚拟机实例，然后决定哪台计算服务器主机来运行 nova-conductor模块 媒介作用于nova-compute服务域数据库之间 nova-cert模块 服务器守护进程向Nova Cert服务提供X509证书 nova-network worker守护进程 与nova-compute类似，从队列中接受网络任务，并且操作网络。执行任务例如创建网桥接的接口或改变iptables规则 nova-consoleauth守护进程 授权控制台代理所提供的用户令牌 nova-novncproxy守护进程 提供一个代理，用于访问正在运行的实例，通过VNC协议，支持基于浏览器的novnc客户端 nova-spicehtml5proxy 提供一个代理，用于访问正在运行的实例，通过SPICE协议，支持基于浏览器的html5客户端 nova-xvpvncproxy 提供一个代理，用于访问正在运行的实例，通过VNC协议，支持OpenStack特定的java客户端 nova-cert守护进程 X509证书 nova client 用于用户作为租户管理员或最终用户来提交命令 队列 一个在守护进程之间传递消息的中央集线器 SQL数据库 存储构建时和运行时的状态，为云基础设施 安装配置控制节点执行操作安装条件 创建数据库 12345678910# login mysqlmysql -u root -p# create datbasecreate database nova_api;create database nova;# grant privilegesgrant all privileges on nova.* to &apos;nva_db&apos;@&apos;localhost&apos; identified by &apos;passwd&apos;;grant all privileges on nova.* to &apos;nva_db&apos;@&apos;192.168.122.%&apos; identified by &apos;passwd&apos;;grant all privileges on nova_api.* to &apos;nva_db&apos;@&apos;localhost&apos; identified by &apos;passwd&apos;;grant all privileges on nova_api.* to &apos;nva_db&apos;@&apos;192.168.122.%&apos; identified by &apos;passwd&apos;; 获取admin凭证. admin-openrc 创建服务证书 123456789# 创建服务用户openstack user create \--domain default \--password-prompt nova# 添加角色openstack role add --project serice --user nova admin# 创建服务实体openstack service create --name nova \--description &quot;OpenStack Compute&quot; compute 创建计算服务API端点 123456# create public endpointopenstack endpoint create --region RegionOne \compute public http://ops-comp:8774/v2.1/%\(tenant_id\)s# create internal endpoint# create admin endpoint# create firewall rule 安装placement34 创建数据库 1234567# login mysqlmysql -u root -p# create databasecreate database placement;# grant privilegegrant all privileges on placement.* to &apos;plc_db&apos;@&apos;localhost&apos; identified by &apos;passwd&apos;;grant all privileges on placement.* to &apos;plc_db&apos;@&apos;192.168.122.%&apos; identified by &apos;passwd&apos;; 创建用户 12345# createopenstack user create --domain default \--password-prompt placement# add role to useropenstack role add --project service --user placement admin 创建服务实体 123openstack service create \--name placement \--description &quot;OpenStack Plancement&quot; placement 添加endpoint 12345# add public endpointopenstack endpoint create \--region RegionOne \placement public http://ops-cont:8778# add internal admin endpoint like public 安装placementyum install openstack-nova-placement-api -y 安装nova 安装 1yum install openstack-nova-api openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler -y 配置nova.conf 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556i /etc/nova/nova.conf# enable compute and metadata in [DEFAULT]enabled_apis=osapi_compute,metadata# configure database in [api_database] and [database][api_database]connection=mysql+pymysql://nva_db:db_passwd@ops-cont/nova_api[database]connection=mysql+pymysql://nva_db:db_passwd@ops-cont/nova# configure Rabbitmq in [DEFAULT] and [oslo_messageing_rabbit][DEFAULT]rpc_backend=rabbit[oslo_messageing_rabbit]rabbit_host=ops-contrabbit_userid=openstackrabbit_password=passwd# configure auth in [DEFAULT] and [keystone_authtoken][DEFAULT]auth_strategy=keystone[keystone_authtoken]auth_uri=http://ops-cont:5000auth_url=http://ops-cont:35357memcached_servers=ops-cont:11211auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=novapassword=passwd# configure manager interface in [DEFAULT][DEFAULT]my_ip=192.168.122.100# enable neutrion and use nova firewall rule in [DEFAULT][DEFULT]use_neutron=Truefirewall_driver=nova.virt.firewall.NoopFirewallDriver# configure vnc ip address in [vnc][vnc]vncserver_listen=$my_ipvncserver_proxyclient_address=$my_ip# configure glance in [glance][glance]api_servers=http://ops-cont:9292# configure lock_path in [oslo_concurrency]lock_path=/var/lib/nova/tmp# configure placement in [placement][placement]auth_uri=http://ops-cont:5000/v3auth_url=http://ops-cont:35357/v3os_region_name=RegionOneproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceuser_name=placementpassword=passwd# configure rabbit transport url in [DEFAULT]transport_url=rabbit://openstack_user:user_passwd@ops-cont 同步数据库 123456su -s /bin/sh -c &quot;nova-manage api_db sync&quot; novasu -s /bin/sh -c &quot;nova-manage db sync&quot; novasu -s /bin/sh -c &quot;nova-manage cell_v2 map_cell0&quot; novasu -s /bin/sh -c &quot;nova-manage cell_v2 create_cell&quot; nova# show cellsnova-manage cell_v2 list_cells 启动服务 123456# startsystemctl start openstack-nova-api openstack-nova-consoleauth openstack-nova-scheduler openstack-nova-conductor openstack-nova-novncproxy# enablesystemctl enable openstack-nova-api openstack-nova-consoleauth openstack-nova-scheduler openstack-nova-conductor openstack-nova-novncproxy# add firewall rulefirewall-cmd --zone=public --add-rich-rule=&apos;rule family=&quot;ipv4&quot; source address=&quot;192.168.122.0/24&quot; port port=&quot;5672&quot; protocol=&quot;tcp&quot; accept&apos; --permanent 计算节点连接成功后执行nova-manage cell_v2 discover_hosts 验证操作在计算节点上的操作完成后执行验证，查看服务组件是否全部启动openstack compute service list 计算节点执行操作 安装yum install openstack-nova-compute -y 配置nova.conf 123456789101112131415161718192021222324252627282930313233343536373839404142# configure rabbitmq in [DEFAULT] and [oslo_messaging_rabbit]# configure manager ip address# configure neutron[DEFAULT]rpc_backend=rabbitmy_ip=192.168.0.100use_neutron=Truefirewall_driver = nova.virt.firewall.NoopFirewallDriver[oslo_messaging_rabbit]rabbit_host=ops-contrabbit_userid=openstackrabbit_password=passwd# configure auth in [DEFAULT] and [keystone_authtoken][keystone_authtoken]auth_uri=http://ops-cont:5000auth_url=http://ops-cont:35357memcached_servers=ops-cont:11211auth_type=passwordproject_domian_name=defaultuser_domain_name=defaultproject_name=serverusername=novapassword=passwd# configure vnc in [vnc]enabled=Truevncserver_listen=0.0.0.0vncserver_proxyclient_address=$my_ipnovncproxy_base_url=http://ops-cont:6080/vnc_auto.html# configure iamges in [glance][glance]api_servers=http://ops-cont:9292# configure lock path in [oslo_concurrency]lock_path=/var/lib/nova/tmp# configure placement in [placement]auth_uri=http://ops-cont:5000/v3auth_url=http://ops-cont:35357/v3os_region_name=RegionOneproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceuser_name=placementpassword=passwd 使用kvm虚拟化 123vi /etc/nova/nova.conf# enable kvm in [libvirt]virt_type=kvm 启动服务 1234# startsystemctl start libvirtd openstack-nova-compute# enable systemctl enable libvirtd openstack-nova-compute 注意按照官方文档安装OpenStack-nova(ocata)时，在启动计算节点时会报如下错误： ERROR oslo_service.service PlacementNotConfigured: This compute is not configured to talk to the placement service. Configure the [placement] section of nova.conf and restart the service. 根据提示，我们首先需要在控制节点中安装配置placement组件，同时需要将placement组件的配置信息写入到计算节点中的nova.conf文件中。 参考 安装并配置控制节点 安装和配置计算节点 Install and configure Placement for Red Hat Enterprise Linux and CentOS Install and configure controller node]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>OpenStack</tag>
        <tag>nova</tag>
        <tag>compute</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS部署OpenStack过程-镜像服务Glance]]></title>
    <url>%2F2019%2F06%2F08%2Fopenstack-install-glance.html</url>
    <content type="text"><![CDATA[基础镜像服务glance允许用户发现、注册和获取虚拟镜像。它提供一个RESETAPI允许查询虚拟机镜像的metadata并获取一个现存的镜像。 安装和配置安装条件 创建数据库 1234567# login mysqlmysql -u root -p# create database named glancecreate database glance;# grant privilegesgrant all privileges on glance.* to &apos;glc_db&apos;@&apos;localhost&apos; identified by &apos;passwd&apos;;grant all privileges on glance.* to &apos;glc_db&apos;@&apos;192.168.122.%&apos; identified by &apos;passwd&apos;; 获得admin凭证获取只有管理员才能执行的命令的权限. admin-openrc 创建服务证书 创建glance用户 12openstack user create --domain default \--password-prompt glance 为用户添加admin角色 1openstack role add --project service --user glance admin 创建glance实体 123openstack service create \--name glance \--description &quot;OpenStack Image&quot; image 创建镜像服务的API端点123456789101112# create public image apiopenstack endpoint create \--region RegionOne \image public http://ops-cont:9292# create internal image apiopenstack endpoint create \--region RegionOne \image internale http://ops-cont:9292# create admin image apiopenstack endpoint create \--region RegionOne \image admin http://ops-cont:9292 安装配置组件 安装yum install openstack-glance -y 配置glance-api.conf 1234567891011121314151617181920212223242526cd /etc/glancevi glance-api.conf# config database in [database][datebase]# addconnection = mysql_pymysql://glc_db:db_passwd@ops-cont/glance# config auth in [keystone_authtoken] and [paste_deploy][keystone_authtoken]# addauth_url=http://ops-cont:5000auth_url=http://ops-cont:35357memcached_servers=ops-cont:11211auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=glancepassword=user_passwd[paste_deploy]# addflavor=keystone# config image postion in [glance_store][glance_store]stores=file,httpdefault_store=filefilesystem_store_datadir=/var/lib/glance/images/ 配置glance-registry.conf 123456789101112131415161718# configure database in [database][database]# addconnection=mysql+pymysql://glc_db:db_passwd@opc-cont/glance# configure auth in [keystone_token] and [paste-deploy][keystone_authtoken]# addauth_uri=http://ops-cont:5000auth_url=http://ops-cont:35357memcached_servers=ops-cont:11211auth_type=passwordproject_domain_name=defaultuser_domain_name=defaultproject_name=serviceusername=glancepassword=user_passwd[paste_deploy]flavor=keystone 写入镜像服务数据库/bin/sh -c &quot;glance-manage db_sync&quot; glance 启动服务12systemctl start openstack-glance-api openstack-glance-registrysystemctl enable openstack-glance-api openstack-glance-registry 验证服务 获得admin凭证. admin-openrc 下载cirros镜像wget https://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img cirros040.img将下载的镜像移动到/var/lib/glance/images 使用qcow2格式，bare容器格式上传镜像到镜像服务并设置公共可见12345openstack image create &quot;cirros&quot; \--file cirros040.img \--disk-format qcow2 \--container-format bare \--public 查看镜像属性openstack image list 参考1.OpenStack安装和配置镜像服务]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>OpenStack</tag>
        <tag>glance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS部署OpenStack过程-基础和认证服务]]></title>
    <url>%2F2019%2F06%2F08%2Fopenstack-install-keystone.html</url>
    <content type="text"><![CDATA[基本信息 步骤启用网络事件协议NTP 控制节点 12345678910# installsudo yum install chrony -y# configsudo vi /etc/chrony.conf# modifyserver Server_Name or IP iburstallow 192.168.122.0/24# start servicesudo systemctl start chronydsudo systemctl enable chronyd 计算节点 123456789# installsudo yum install chrony -y# configsudo vi /etc/chrony.conf# modify 注释掉除server外的字段server cont_server iburst# start servicesudo systemctl start chronydsudo systemctl enable chronyd 安装OpenStack库 安装OpenStack库sudo yum install centos-release-openstack-ocata -y 安装OpenStack客户端sudo yum install python-openstackclient -y 安装SELinux策略文件sudo yum install openstack-selinux -y 数据库MySQL数据库OpenStack服务使用SQL数据库来存储信息，数据库运行在控制节点上。 安装软件sudo yum install yum install mariadb mariadb-server python2-PyMySQL -y 配置 123456789101112cd /etc/my.cnf.dsudo vi mariadb.cnf# mysqld part[mysqld]# change ip to controller node ip addressbind-address=192.168.122.200 # set store-engine and characterdefault-storage-engine = innodbinnodb_file_per_tablemax_connections = 4096collation-server = utf8_general_cicharacter-set-server = utf8 启动服务 1234567# startsudo systemctl start mariadbsudo systemctl enable mariadb# set firewall rulesu rootfirewall-cmd --zone=public --add-rich-rule=&apos;rule famliy=ipv4 source address=192.168.122.100 port port=3306 protocol=tcp accept&apos; --permanentfirewall-cmd --reload 检查MySQL数据库安全mysql_secure_installation NoSQL数据库Telemetry服务使用NoSQL数据库来存储信息，该服务运行在控制节点上。 安装sudo yum install mongodb-server mongodb -y 配置 123456cd /etcsudo vi mongod.conf# modfiybind_ip=192.168.122.200# limit log size 128Msmallfiles=true 启动服务 123# startsudo systemctl start mongodsudo systemctl enable mongod 消息队列RabbitMQOpenStack使用 message_queue 协调操作和个服务的状态信息。消息队列服务运行在控制节点上。OpenStack支持多种消息队列服务，包括：RabbitMQ、Qpid和ZeroMQ。 安装sudo yum install rabbitmq-server -y 启动服务 12sudo systemctl start rabbitmq-serversudo systemctl enable rabbitmq-server 添加用户 1234# create rabbitmq user with passwdrabbitmqctl add_user openstack Passwd# grant privilegerabbitmqctl set_permissions openstack &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; Memcached认证服务认证缓存使用Memcached缓存令牌，缓存服务Memcached运行在控制节点上。 安装sudo yum install memcached python-memcached -y 启动服务12sudo systemctl start memcachedsudo systemctl enable memcached 认证服务当openStack服务收到来自用户的请求时，该服务询问Identity服务，验证该用户是否有权限进行此次请求。身份服务包含的组件有：服务器、驱动、模块。在安装认证服务前，需要线创建数据库和管理员令牌。 安装条件 创建数据库 12345678# login Mysqlmysql -u root -p# create keystone databasecreate database keystone;# grant privilege on keystonegrant all privileges on keystone.* to &apos;ks_db&apos;@&apos;localhost&apos; identified by &apos;Passwd&apos;;grant all privileges on keystone.* to &apos;ks_db&apos;@&apos;192.168.122.%&apos; identified by &apos;passwd&apos;;flush privileges; 生成一个随机值在初始配置中作为系统管理员令牌openssl rand -hex 10记下刚生成的随机码：e7d81bfae3c2884d8ea1 安装认证服务 安装sudo yum install openstack-keystone httpd mod_wsgi -y使用mod_wsgi来服务认证服务请求，端口号为 5000 35357 配置 12345678910cd /etc/keystonesudo vi keystone.conf# modify [default] replace with copybordadmin_token = e7d81bfae3c2884d8ea1# modify [database] replace db_passwd with mysql database passwd# replace db_server with db-server-name or ip# mysql+pymysql://db_user:db_passwd@db-server/db_nameconnection = mysql+pymysql://ks_db:db_passwd@db-server/keystone# modify [token]provider = fernet 初始化身份认证数据库su -s /bin/sh -c &quot;keystone-manage db_sync&quot; keystone 初始化Fernet Keyskeystone-manage fernet_setup --keystone-user keystone --keystone-group keystone 配置Apache服务器 编辑httpd.conf文件/etc/httpd/conf/httpd.confServerName ops-cont 创建wsgi-keystone.conf文件 12345678910111213141516171819202122232425262728293031323334cd /etc/httpd/conf.d &amp;&amp; sudo touch wsgi-keystone.conf# addListen 5000Listen 35357&lt;VirtualHost *:5000&gt; WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%&#123;GROUP&#125; WSGIProcessGroup keystone-public WSGIScriptAlias / /usr/bin/keystone-wsgi-public WSGIApplicationGroup %&#123;GLOBAL&#125; WSGIPassAuthorization On ErrorLogFormat &quot;%&#123;cu&#125;t %M&quot; ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined &lt;Directory /usr/bin&gt; Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt;&lt;VirtualHost *:35357&gt; WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone group=keystone display-name=%&#123;GROUP&#125; WSGIProcessGroup keystone-admin WSGIScriptAlias / /usr/bin/keystone-wsgi-admin WSGIApplicationGroup %&#123;GLOBAL&#125; WSGIPassAuthorization On ErrorLogFormat &quot;%&#123;cu&#125;t %M&quot; ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined &lt;Directory /usr/bin&gt; Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt; 启动服务 123456789# startsudo systemctl start httpdsudo systemctl enable httpd# add firewall rulesu rootfirewall-cmd --zone=public --add-port=80/tcp --permanentfirewall-cmd --zone=public --add-port=5000/tcp --permanentfirewall-cmd --zone=public --add-port=35357/tcp --permanentfirewall-cmd --reload 创建服务实体和API端点身份认证服务提供服务的目录和他们的位置，每个添加到OpenStack环境中的服务在目录中需要一个service实体和一些API_endpoints。 安装条件默认情况下，身份认证服务数据库不包含支持传统认证和目录服务的信息，需要使用临时身份验证令牌来初始化服务实体和API端点。 配置认证令牌为环境变量export OS_TOKEN=e7d81bfae3c2884d8ea1 配置端点URLexport OS_URL=http://ops-cont:35357/v3 配置认证API版本export OS_IDENTITY_API_VERSION=3 创建服务实体和API端点在OpenStack环境中，认证服务管理服务目录，使用这个目录来决定环境中可用的服务。OpenStack使用三个API端点代表每种服务：admin,internal，public。默认情况下，管理API端点允许修改用户和租户而内部和公众APIs不允许这些操作。公众API是为了让用户管理自己的云在互联网上是可见的;内部API网络回被限制在包含OpenStack服务的主机上。 创建服务实体和身份认证服务12openstack service create \--name keystone --description &quot;OpenStack Identity&quot; identity 所有端点和默认RegionOne区域都使用管理网络identity与创建的服务实体认证中的identity对应123456789# create public APIopenstack endpoint create --region RegiOne \identity public http://ops-cont:5000/v3# create internal APIopenstack endpoint create --region RegionOne \identity internal http://ops-cont:5000/v3# create admin APIopenstack endpoint create --region RegionOne \identity admin http://ops-cont:5000/v3 创建域、项目、用户和角色创建的角色都映射到每个OpenStack服务配置文件目录下的policy.json文件中。 创见域default 1openstack domain create --description &quot;Default Domain&quot; default 创建项目、用户和角色 12345678910# 创建admin项目,项目要包含在域中openstack project create --domain default \--description &quot;Admin Project&quot; admin# 创建用户,输入用户密码openstack user create --domain default \--password-prompt admin# 创建角色openstack role create admin# 将角色添加到用户上,执行后无输出openstack role add --project admin --user admin admin 创建每个服务独有用户的service项目12openstack project create --domain default \--description &quot;Service Project&quot; service 创建非管理无特权的项目和用户12345678910# 创建demo项目openstack project create --domain default \--description &quot;Demo Project&quot; demo# 创建用户openstack user create --domain default \--password-prompt demo# 创建角色openstack role create user# 将角色添加到用户上openstack role add --project demo --user demo user 验证操作在安装其他服务之前需要在控制节点上确认身份认证服务。 关闭临时认证令牌服务 123456cd /etc/keystonevi keystone-paste.ini# remove admin_token_auth from below[pipeline:public_api][pipiline:admin_api][pipeline:api_v3] 重置环境变量unset OS_URL OS_TOKEN 请求用户admin认证令牌，输入用户认证密码 12345openstack --os-auth-url http://ops-cont:35357/v3 \--os-project-domain-name default \--os-user-domain-name default \--os-project-name admin \--os-username admin token issue 请求用户demo认证令牌，注意与用户admin区分端口号 12345openstack --os-auth-url http://ops-cont:5000/v3 \--os-project-domain-name default \--os-user-domain-name default \--os-project-name demo \--os-username demo token issue 创建环境脚本 创建admin脚本admin-openrc 12345678export OS_PROJECT_DOMAIN_NAME=defaultexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_NAME=adminexport OS_USERNAME=adminexport OS_PASSWORD=user_passwdexport OS_AUTH_URL=http://ops-cont:35357/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2 创建demo脚本demo-openrc 12345678export OS_PROJECT_DOMAIN_NAME=defaultexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_NAME=demoexport OS_USERNAME=demoexport OS_PASSWORD=user_passwdexport OS_AUTH_URL=http://ops-cont:5000/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2 使用脚本 12. admin-openrcopenstack token issue 参考1.OpenStack-DOC]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>OpenStack</tag>
        <tag>keystone</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS-KVM配置Bridge-Vlan]]></title>
    <url>%2F2019%2F06%2F04%2Fkvm-bridge-vlan.html</url>
    <content type="text"><![CDATA[基本信息 在使用Vlan之前，需要加载8021qmodprobe 8021q 如果系统已经加载可以不用执行以上步骤lsmod | grep 8021q 使用epel-release安装vconfigsudo yum install vconfig -y 停用NetworkManager服务sudo systemctl disable NetworkManager &amp;&amp; sudo systemctl stop NetworkManager 操作步骤 分别创建2个网卡eth0.1,eth0.2和2个网桥br0.1,br0.2 12cd /etc/sysconfig/network-scriptsudo touch ifcfg-eth0.1 ifcfg-eth0.2 ifcfg-br0.1 ifcfg-br0.2 分别配置网卡eth0.1和eth0.2 12345678TYPE=EthernetDEVICE=eth0.1# DEVICE=eth0.2ONBOOT=yesBOOTPROTO=staticVLAN=yesBRIDGE=br0.1# BRIDGE=br0.2 分别配置网桥br0.1,br0.2 123456TYPE=BridgeDEVICE=br0.1# DEVICE=br0.1ONBOOT=yesBOOTPROTO=staticDELAY=0 分别将创建的网卡和网桥启动 12345sudo ifdown eth0 &amp;&amp; sudo ifup eth0sudo ifup eth0.1sudo ifup eth0.2sudo ifup br0.1sudo ifup br0.2 查看网桥信息brctl show 创建3个虚拟机，分别连接到各自的网桥 12345678910sudo virt-install \--name cirros \ # cirros cirros-1 cirros-2--virt-type qemu \--os-variant cirros0.4.0 \--graphics vnc \--memory 512 \--vcpus 1 \--network bridge=br0.1,model=virtio # cirros cirros-1连接br0.1，cirros-2连接br0.2--disk /path/cirros.qcow2 \--import 分别启动3个vm 123sudo virsh start cirrossudo virsh start cirros-1sudo virsh start cirros-2 分别设置vm的ip地址 cirros:192.168.1.10 cirros-1:192.168.1.11 cirros-2: 192.168.2.101234567891011121314151617cd /etc/networksudo vi interface---cirros---iface eth0 inet staticaddress 192.168.1.10netmask 255.255.255.0----------cirros-1---iface eth0 inet staticaddress 192.168.1.11netmask 255.255.255.0----------cirros-2---iface eth0 inet staticaddress 192.168.2.10netmask 255.255.255.0------- 查看网桥信息，可以看到vm的网卡的挂载情况brctl show 测试网络连通情况cirroscirros-1ping 192.168.1.12cirroscirros-2ping 192.168.2.10cirros-1cirros-2]]></content>
      <categories>
        <category>VLAN</category>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>vlan</tag>
        <tag>bridge</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS使用X11VNC轻量界面并启用SSH加密]]></title>
    <url>%2F2019%2F06%2F04%2Fcentos-install-x11vnc.html</url>
    <content type="text"><![CDATA[安装X11VNC程序 x11vnc轻量级VNC服务程序 Xvfb轻量级Xorg服务程序 1sudo yum install xorg-x11-xauth xterm libXi libXp libXtst libXtst-devel libXext libXext-devel Xvfb x11vnc -y 配置SSH服务 安装ssh服务，并启用key验证; ssh使用的端口号为9022 配置x11vnc服务 x11vnc服务使用的端口号为9033 使用ssh端口转发，为x11vnc服务提供加密服务 在服务器中启用x11nvc服务,并监听本地sudo x11vnc -localhost -rfbport 9033 -passwd VncPasswd -create 还可以将x11vnc服务在后台中运行nohup x11vnc -localhost -rfbport 9033 -passwd VncPasswd -create &gt; /dev/null 2&amp;&gt;1 &amp; 本地主机先通过ssh登录到服务器，将VNC服务监听的端口转发到本地主机ssh -p 9022 remote_server -L 9033:localhost:9033 在本地使用VNC Viewer软件连接到本地的9033端口 参考 CentOS 7.2搭建VNC远程桌面服务 Arch-wiki-x11vnc)]]></content>
      <categories>
        <category>CentOS</category>
        <category>VNC</category>
      </categories>
      <tags>
        <tag>VNC</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[商用机器人产品线_部分]]></title>
    <url>%2F2019%2F06%2F02%2Fservice_robot.html</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>机器人</category>
      </categories>
      <tags>
        <tag>机器人</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS搭建syslog-ng服务器并启用加密传输]]></title>
    <url>%2F2019%2F05%2F14%2Finstall_syslog_ng_with_tls.html</url>
    <content type="text"><![CDATA[操作步骤Server端配置syslog-ng 安装syslog-ngsudo yum install epel-release -y &amp;&amp; sudo yum install syslog-ng -y 配置syslog-ng在syslog-ng的配置文件中，有5种不同的项目，每个项目以1个特殊关键词开头： options 用来调节syslog-ng的守护进程。 source 告知syslog-ng从什么地方收集日志。source内容可以包括Unix套接字、TCP或UDP套接字、文件或管道。 destination 用来决定syslog-ng将向哪些地方发送日志，可以指定为文件、管道、Unix套接字、TCP或UDP套接字、TTY或程序等。 filter 结合source、destination和filter使用，选择syslog程序和日志级别。 log 将以上关键字和log结合使用，可以精确定义消息日志保存的地方。 1234567891011121314151617cd /etc/syslog-ng &amp;&amp; sudo cp syslog-ng.conf syslog-ng.conf.oldsudo vi syslog-ng.conf# add# add a log sourcesource s_remote &#123; tcp(ip(&quot;192.168.0.104&quot;) port(20514) max-connections(500));&#125;;# collect log from remote hostdestination d_remote &#123; # write log file to hostname path,log file named by y.m.d.log file(&quot;var/log/syslog-ng/$&#123;HOST&#125;/$&#123;YEAR&#125;.$&#123;MONTH&#125;.$&#123;DAY&#125;.log&quot; perm(0644));&#125;;# add to loglog [ source(s_remote); destination(d_remote);&#125;; 修改日志路径的SELinux策略semanage fcontext -a -t syslog-ng &quot;/var/log/syslog-ng(/.*)?&quot; &amp;&amp; restorecon /var/log/syslog-ng 添加防火墙firewall-cmd --zone=public --add-rich-rule=&#39;rule family=ipv4 source address=192.168.0.1/24 port port=20514 protocol=tcp accept&#39; --permanent 启动服务sudo systemctl start syslog-n 如果启动失败使用命令/usr/sbin/syslog-ng -F -p /var/run/syslogd.pid 查看具体的错误提示 Agent端配置syslog-ng 安装sudo yum install epel-release -y &amp;&amp; sudo yum install syslog-ng -y 修改配置文件 123456789101112131415161718192021cd /etc/syslog-ng/ &amp;&amp; sudo cp syslog-ng.conf syslog-ng.conf.oldsudo vi syslog-ng.conf# add# add a log sourcesource s_net &#123; # tell syslog-ng read log from /dev/log;/dev/log link to /var/run/log unix-dgram(&quot;/dev/log&quot;); # if use systemctl start syslog-ng start faild,use this # unit-stream(&quot;/dev/log&quot;); # create message by internal internal();&#125;;# send log to remote syslog serverdestination d_net &#123; tcp(&quot;192.168.0.104&quot; port(20514) max-connections(10));&#125;;# add to loglog [ source(s_net); destination(d_net);&#125;; 启动服务sudo systemctl start syslog-ng 配置加密服务为保证日志传输的安全性，为syslog-ng日志的传递设置加密服务。使用服务器生成的公钥，对client端传送的日志文件进行加密，所有client端使用相同的公钥进行加密。 在server端生成加密证书和私钥 123# cert dircd /etc/syslog-ng &amp;&amp; sudo mkdir cert &amp;&amp; cd certsudo openssl -x509 -nodes -days 365 -newkey rsa:2048 -outkey syslog_pri.key -out syslog_pub.crt 配置server的syslog-ng.conf文件 12# destination在tcp或udp条目中添加tcp(ip(&quot;192.168.0.104&quot;) port(20514) tlc(key-file(&quot;/etc/syslog-ng/cert/syslog_pri.key&quot;) cert-file(&quot;/etc/syslog-ng/cert/syslog_pub.crt&quot;) peer-verify(optional-untrusted))); 将公钥分别复制到client端的 /etc/syslog-ng/cert 目录中,并生成哈希名的链接openssl -x509 -noout -hash -in syslog_pub.crt &amp;&amp; ln -s syslog_pub.crt xxxx.0 配置client端的syslog-ng.conf文件 1tcp(&quot;192.168.0.104&quot; port(20514) tlc(ca-dir(&quot;/etc/syslog-ng/cert&quot;))); 配置完成后，对server和client分别重启syslog-ng服务sudo systemctl restart syslog-ng 注意如果系统开启了SELinux，在使用tcp或udp进行日志传输时，选择端口时要注意SELinux中对syslog-ng端口的设定，如果使用自定的端口，还需要将端口加入到SELinux中。semanage port -a -t syslogd_port_t -p tcp 30514]]></content>
      <categories>
        <category>syslog-ng</category>
      </categories>
      <tags>
        <tag>syslog-ng</tag>
        <tag>tls</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix监控图形中文不显示或乱码的解决方法]]></title>
    <url>%2F2019%2F05%2F11%2Fzabbix_Chinese_font.html</url>
    <content type="text"><![CDATA[基本信息Zabbix版本号为4.0.7LTS，在使用系统管理员创建用户，为用户分配中文语言环境后，新用户登录系统后，出现在监控图形中中文显示为方框或不显示的情况。 解决方法下载中文字体，将原字体用下载的字体替换。我们使用雅黑字体替换原文件。下载雅黑字体wget https://www.wfonts.com/download/data/2014/06/01/microsoft-yahei/microsoft-yahei.zip，解压unzip microsoft-yahei.zip -d ./yahei 替换原字体 找到原字体find / -name &quot;graphfont*&quot; 替换字体字体文件位于/usr/share/zabbix/fonts/graphfont.ttf ,查看该字体详情会发现该字体链接到/etc/alternatives/zabbix-web-font 替换链接文件ln -s ./yahei.ttf /etc/alternatives/zabbix-web-font 或者直接替换graphfont.ttf文件mv /usr/share/zabbix/fonts/graphfont.ttf /usr/share/zabbix/fonts/graphfont.ttf.old &amp;&amp; cp ./yahei.ttf /usr/share/zabbix/fonts/graphfont.ttf 如果该方法不生效，可以参考另一种方法 安装新字体 将雅黑字体复制cp ./yahei/yahei.ttf /usr/share/zabbix/fonts/ 使用雅黑字体1234# 修改文件defines.inc.phpcp defines.inc.php defines.inc.php.old# sed 查找关键词,将graphfont替换为yaheised -i &apos;s/graphfont/yahei/g&apos; ./defines.inc.php 打开监控图形，中文字体显示正常]]></content>
      <categories>
        <category>Zabbix</category>
      </categories>
      <tags>
        <tag>Zabbix</tag>
        <tag>中文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KVM-QEMU创建支持UEFI启动的虚拟主机]]></title>
    <url>%2F2019%2F05%2F08%2Fkvm_uefi.html</url>
    <content type="text"><![CDATA[基本信息在KVM/QEMU中支持UEFI启动的是OVMF(Open Virtual Machine Firmware),它从EDK2演变而来。 操作步骤 从Fedora repo安装UEFI需要安装的软件是edk2-ovmf,如果系统中已经安装了Qemu，并且在系统中有OVMF_CODE.secboot.fd文件，系统中就已经安装完成该软件了。如果没有，则执行以下命令完成安装： 123sudo dnf install dnf-plugins-coresudo dnf config-manager --add-repo https://www.kraxel.org/repos/firmware.reposudo dnf install edk2.git-ovmf-x64 配置libvirtd支持UEFI在Fedora22版本以后，libvirtd已经配置好对UEFI的支持，如果需要修改可以通过修改文件/etc/libvirt/qemu.conf中nvram选项。 创建虚拟机通过命令行创建虚拟机时，操作步骤与其他无二，只需要额外添加一条--boot uefi 创建UEFI分区在安装虚拟机系统时，为UEFI指定单独分区并挂载到/boot/efi 注意在启用了uefi启动模式，在为虚拟机创建snapshot的时候会出错，目前该问题尚未解决：error: Operation not supported: internal snapshots of a VM with pflash based firmware are not supported因此，在启用uefi和使用snapshot功能之间要自行权衡后选择使用。 参考 Using UEFI with QEMU]]></content>
      <categories>
        <category>kvm</category>
      </categories>
      <tags>
        <tag>kvm</tag>
        <tag>uefi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Zabbix扩展脚本监控Mysql数据库]]></title>
    <url>%2F2019%2F05%2F07%2Fzabbix_agent_monitoring_mysql.html</url>
    <content type="text"><![CDATA[基本信息使用扩展的脚本监控Mysql数据库，脚本中包含10项较为常用的Mysql指标监控，并且该脚本是独立的，在Zabbix Server安装即用。在使用脚本之前，需要在受监控的Mysql服务器中安装jq，支持的版本号是1.5+。 操作Mysql服务器中进行的操作 安装jq 12sudo yum whatprovides jqsudo yum install jq-1.5-1.el7.x86_64 -y 配置Zabbix-Agent配置文件在文件/etc/zabbix/zabbix_agentd.conf 中添加以下内容： 1UserParameter=Mysql.Server-Status, mysql --defaults-file=/etc/zabbix/.my.cnf --defaults-group-suffix=_monitoring -N -e &quot;show global status&quot; | jq -c &apos;. | split(&quot;\n&quot;)[:-1] | map (split(&quot;\t&quot;) | &#123;(.[0]) : .[1]&#125; ) | add &apos; -R -s 需要注意的是：mysql的认证登录信息默认存储位置设置为/etc/zabbix/.my.cnf，zabbix用户需要对mysql的passwd文件具有读的权限。.my.cnf文件的所有者属于mysql:mysql,可以将zabbix用户附加到mysql用户组中，将该文件的权限设置为640 。 将用户和密码信息加入文件.my.cnf中,并设置用户/组和权限 12345678910111213141516cd /etc/zabbixsudo touch .my.cnfsudo vi .my.cnf# add[mysql]socket=/var/lib/mysql/mysql.sockuser=dbpassword=MysqlPasswd@2019[client]socket=/var/lib/mysql/mysql.sockuser=dbpassword=MysqlPasswd@2019# set ownersudo chown mysql:mysql .my.cnfsudo chmod 640 .my.cnfsudo usermod -a -G mysql zabbix 重启Agent服务sudo systemctl restart zabbix-agent Zabbix监控服务器中进行的操作 下载扩展脚本并导入wget https://raw.githubusercontent.com/nitzien/zabbix-misc/master/templates/mysql/mysql_template.xml打开Zabbix Web导入脚本，操作路径：Configuration-&gt;Templates-&gt;Import 将导入的脚本链接到需要监控的Mysql服务器 查看最新的监控数据 遇到的坑在操作完成后，在Web中查看最新数据时，总是提示ERROR 2002 (HY000): Can’t connect to local MySQL server through socket ‘/var/lib/mysql/mysql.sock’ (13),但是在Agent端Mysql的服务是正常启动的，而且在本地都可以登录，重试了几次都没有解决，最后尝试将SELinux设置为pemissive,重启后问题得到解决。如果启用SELinux需要安装Zabbix-Agent的SELinux模块，具体操作：1234# show SELinuxgrep zabbix_agent_t /var/log/audit/audit.loggrep zabbix_agent_t /var/log/audit/audit.log | audit2allow -M zabbix_agent_custom# setsemodule -i zabbix_agent_custom.pp 参考 Mysql and Mysql Slave Monitoring Zabbix Template zabbix-misc]]></content>
      <categories>
        <category>Zabbix</category>
      </categories>
      <tags>
        <tag>Zabbix_Agent</tag>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS批量安装Zabbix-agent并自动注册]]></title>
    <url>%2F2019%2F05%2F06%2Fzabbix_auto_registration.html</url>
    <content type="text"><![CDATA[基本信息使用shell脚本自动安装zabbix-agent，并启用主动模式和自动注册。自动注册主要参数是ServerActive和HostMetadataItem,主动模式的主要参数是StartAgents。 操作步骤 将脚本文件和agent配置模板文件上传到服务器中，执行自动化安装scp install_agent.sh template.conf user@server:~/需要注意的是：在agent的配置文件中，如果启用自动注册，需要设置HostMetadataItem=system.uname 使用root用户执行脚本文件sh ./install_agent.sh 打开Zabbix Web界面，添加自动注册动作操作路径:Configuration-&gt;Actions-&gt;Auto registration-&gt;create action 添加动作名称和操作需要注意的是在添加条件的时候，需要选择Host Metadata contains Linux,监控的服务器是Linux的就填写Linux，是Windows是就选择Windows，其他的根据实际情况填写。 添加自动注册的操作为自动注册的主机添加相关的操作，例如添加到主机、主机组、连接到监控模板、发送消息等。 查看自动注册后添加的主机路径：Configation-&gt;Hosts 脚本文件install_agent.tar.gz解压命令：openssl des3 -d -k passwd -salt -in install_agent.tar.gz | tar xzf -]]></content>
      <categories>
        <category>Zabbix</category>
      </categories>
      <tags>
        <tag>Zabbix_Agent</tag>
        <tag>auto_registration</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS安装Zabbix监控软件]]></title>
    <url>%2F2019%2F05%2F05%2Finstall_zabbix_with_shll.html</url>
    <content type="text"><![CDATA[安装 添加Zabbix 安装源sudo rpm -ivh https://repo.zabbix.com/zabbix/4.0/rhel/7/x86_64/zabbix-release-4.0-1.el7.noarch.rpm 安装zabbix-server,frontend,agent(if need)sudo yum install zabbix-server-mysql zabbix-web-mysql zabbix-agent -y 安装Mysql数据库 初始化数据库解压create.sql.gz文件cd /usr/share/doc/zabbix-server-mysql-4.0.7 &amp;&amp; sudo gzip -d create.sql.gz 1234567891011# login in mysqlmysql-u root -p# create database for zabbixcreate database zabbix character set utf8 collate utf8_bin;# create user and grant all privileges grant all privileges on zabbix.* to &apos;db_za&apos;@&apos;localhost&apos; identified by &apos;Zabbix_Passwd@2019&apos;;# flushflush privileges;# import create.sqluse zabbix;source /usr/share/doc/zabbix-server-mysql-4.0.7/create.sql; 配置Zabbix数据库 123456789# 复制原文件cd /etc/zabbix/ &amp;&amp; sudo cp zabbix_server.conf zabbix_server.conf.old# 编辑配置文件sudo vi zabbix_server.conf# add database infoDBHost=localhostDBName=zabbixDBUser=db_zaDBPassword=Zabbix_Passwd@2019 配置时区关于时区的配置文件存放在/etc/httpd/conf.d/zabbix.conf中 1234cd /etc/httpd/conf.d/ &amp;&amp; sudo cp zabbix.conf zabbix.conf.oldsudo vi zabbix.conf# timezonephp_value date.timezone Asia/Shanghai 配置与Zabbix有关的SELinux 1 123456789101112su rootgrep zabbix_t /var/log/audit/audit.log | audit2allow -M zabbix_server_customsemodule -i zabbix_server_custom.pp# 查看zabbix需要启用的策略yum install policycoreutils-python -ygetsebool -a | grep zabbixsetsebool -P zabbix_can_network=1setsebool -P httpd_can_connect_zabbix=1setsebool -P zabbix_run_sudo=1# 如果使用了远程数据库还需要进行以下设置setsebool -P httpd_can_network_connect=1setsebool -P httpd_can_network_connect_db=1 启动Zabbix服务sudo systemctl start zabbix-server zabbix-agent httpd 启动界面安装打开浏览器，输入Zabbix地址http://ip/zabbix,输入数据库信息，使用默认的用户名和密码登录Admin/zabbix Zabbix安装脚本使用tar打包压缩，并使用openssl des3 -salt密码加密-&gt;install_zabbix_shell.tar.gz加密压缩：tar -czvf - install_zabbix.sh zabbix.sql | openssl des3 -salt -k passwd -out install_zabbix_shell.tar.gz加密解压：openssl des3 -d -k passwd -salt -in install_zabbix_shell.tar.gz | tar xzf - 最新数据中，监控条目出现“no data”如果Zabbix Agent使用的是主动模式，监控模板使用的是默认的Linux系统监控模板，那么我们需要clone一个模板，并将clone后的模板中的type修改为zabbix agent(acvite),这样就可以接收到数据。 参考 SELinux And Zabbix]]></content>
      <categories>
        <category>Zabbix</category>
      </categories>
      <tags>
        <tag>Zabbix</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nagios使用check_mysql_health插件监控Mysql主机]]></title>
    <url>%2F2019%2F05%2F03%2Fnagios_plugins_check_mysql_health.html</url>
    <content type="text"><![CDATA[基本信息 Nagios：Nagios core 4.4.3 Nagios Plugins：check_mysql_health 2.2.2 Mysql-server: 192.168.0.91 db user：db 操作流程：下载插件-&gt;安装插件-&gt;配置command-&gt;添加主机-&gt;添加服务 安装插件 下载wget https://labs.consol.de/assets/downloads/nagios/check_mysql_health-2.2.2.tar.gz 配置、编译、安装12345678tar -xzvf check_mysql_health-2.2.2.tar.gzcd check_mysql_health-2.2.2# configure./configure --prefix=/usr/local/nagios/libexec --with-nagios-user=nagios --with-nagios-group=nagios --with-perl=/usr/bin/perl# makesudo make# installsudo make install 配置配置插件 添加check命令 1234567cd /usr/local/nagios/etc/objects/sudo vi commands.cfg## adddefine command &#123; command_name check_mysql_health command_line $USER1$/check_mysql_health -H $ARG1$ --username $ARG2$ --password $ARG3$ --port $ARG4$ --mode $ARG5$ &#125; 创建主机配置文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152cd /usr/local/nagios/etc/objectssudo touch mysql92\1.cfg &amp;&amp; sudo chown nagios:nagios mysql91.cfg# add# define a host use template linux-serverdefine host &#123; use linux-server host_name mysql91 alias mysql server 91 address 192.168.0.91&#125;# define a new hostgroupdefine hostgroup &#123; hostgroup_name mysql-server alias mysql-server members mysql91&#125;# define services with template generic-service# mysql_conn_timedefine service &#123; use generic-service host_name mysql91 service_description mysql_conn_time check_command check_mysql_health!192.168.0.91!db!MysqlPasswd2019!3306!connection-time!&#125;# mysql_threads_connecteddefine service &#123; use generic-service host_name mysql91 service_description mysql_threads_connected check_command check_mysql_health!192.168.0.91!db!MysqlPasswd2019!3306!threads-connected!&#125;# mysql_slow_queriesdefine service &#123; use generic-service host_name mysql91 service_description mysql_slow_queries check_command check_mysql_health!192.168.0.91!db!MysqlPasswd2019!3306!slow-queries!&#125;# mysql_encdedefine service &#123; use generic-service host_name mysql91 service_description mysql_sql check_command check_mysql_health!192.168.0.91!db!MysqlPasswd2019!3306!encode!&#125;# mysql_open_filesdefine service &#123; use generic-service host_name mysql91 service_description mysql_open_files check_command check_mysql_health!192.168.0.91!db!MysqlPasswd2019!3306!open-files!&#125; 更多Mysql检查check_mysql_health插件通过修改--mode的参数来设定检查项，其他检查可以参考check_mysql_health mode参数 检查Nagios配置文件sudo /usr/local/nagios/bin/nagios -v /usr/local/nagios/etc/nagios.cfg 重启Nagios服务，打开web界面查看新增的主机和服务sudo systemctl restart nagios 测试报警将被监控主机的mysql服务关闭，查看Nagios Web平台中的报警。 参考 check_mysql_health website]]></content>
      <categories>
        <category>Nagios</category>
      </categories>
      <tags>
        <tag>nagios</tag>
        <tag>nagios_plugins</tag>
        <tag>check_mysql_health</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS源码安装、配置Nagios+Plugins]]></title>
    <url>%2F2019%2F05%2F01%2Fcentos_install_nagios.html</url>
    <content type="text"><![CDATA[禁用SELinux根据官方的指导文件需要禁用SELinux，首先我们获取SELinux状态，如果处于启用状态，切换为宽容模式permissive。12345# 获取SELinux状态getenforce# 修改为宽容模式sudo vi /etc/selinux/config# 修改enforing为permissive 安装Nagios 安装依赖程序sudo yum install gcc glibc glibc-common wget unzip httpd php gd gd-devel perl postfix -y 下载Nagios core源码文件cd ~/ &amp;&amp; wget https://github.com/NagiosEnterprises/nagioscore/archive/nagios-4.4.3.tar.gz 解压源码文件cd ~/ &amp;&amp; tar -xzvf nagios-4.4.3.tar.gz 编译源文件 123456# 切换到源码文件根目录cd ~/nagios-4.4.3# 配置./configure# 编译文件sudo make all 创建nagios用户和组,并修改apache用户的默认组sudo make install-groups-users &amp;&amp; sudo usermod -a -G nagios apache 安装sudo make install 将nagios安装为服务sudo make install-daemoninit &amp;&amp; sudo systemctl enable httpd.service 安装命令行模式sudo make install-commandmode 安装配置文件sudo make install-config 安装Apache配置文件sudo make install-webconf 配置防火墙 1234567# 切换到root用户su root# 添加80端口到public，防蚊源根据实际情况，选择单一ip地址或ip地址范围firewall-cmd --zone=public --add-rich-rule=&apos;rule family=&quot;ipv4&quot; source address=&quot;192.168.0.90&quot; port port=&quot;80&quot; protocol=tcp&quot; accept&apos; --permanentfirewall-cmd --reload# 切换到管理员用户su kim 创建Nagios管理员用户na_adminsudo htpasswd -c /usr/local/nagios/etc/htpasswd.users na_admin需要输入管理员密码，再重新输入一次密码。创建系统管理员后，如果还需要额外创建其他用户，使用命令sudo htpasswd /usr/local/nagios/etc/htpasswd.users na_user,主要不要使用参数-c否则会替换原来的系统管理员。 启动Apache服务和Nagios服务 1234# 启动Apachesudo systemctl start httpd.service# 启动Nagiossudo systemctl start nagios.service 测试Nagios打开本地浏览器，输入Nagios服务器的域名或ip地址，http://ip/nagios 安装Nagios-pluginsNagios插件的安装目录是/usr/local/nagios/libexec/ 安装依赖sudo yum install gcc glibc glibc-common make gettext automake autoconf wget openssl-devel net-snmp net-snmp-utils epel-release -ysudo yum install perl-Net-SNMP -y 下载plugin源码 123cd ~/wget -O nagios-plugins-release-2.2.1.tar.gz https://github.com/nagios-plugins/nagios-plugins/archive/release-2.2.1.tar.gztar -xzvf nagios-plugins-release-2.2.1.tar.gz 编译安装 12345678cd ~/nagios-plugins-release-2.2.1# 配置sudo ./tools/setup./configure# 使用多线程编译sudo make -j10# 安装sudo make install 配置Nagios基本信息Nagios的配置文件位于/usr/local/nagios/etc,配置文件以.cfg结尾，主要的文件是nagios.cfg。check_external_comands参数控制是否直接通过web接口运行Nagios，参数为1为启用。Nagios的主机、服务等相关的配置文件位于/usr/local/nagios/etc/object目录中，关于本机的相关配置位于文件localhost.cfg中。对于Nagios配置文件进行检查可以通过sudo /usr/local/nagios/bin/nagios -v /usr/local/nagios/etc/nagios.cfg。cgi.cfg文件中还需要设定允许哪些用户运行外部命令。将新添加的Nagios管理员用户添加到cgi.cfg文件中，否则Nagios Web会提示用户权限的错误。 添加远程监控主机 使用localhost主机的模板文件，添加1个Linux服务器： 1234567891011# 复制localhost模板cd /usr/local/nagios/etc/objectsudo cp localhost.cfg v-centos.cfg# 编辑复制的配置文件sudo vi v-centos.cfg# 按照实际情况修改文件# 模板,根据需要从template.cfg中选择或添加use linux-serverhostname v-centosaddress 192.168.0.91# hostgroup和service根据实际情况修改 将新增的配置文件添加到nagios.cfg文件中echo &quot;cfg_file=/usr/local/nagios/etc/objects/v-centos.cfg&quot; &gt; /usr/local/nagios/etc/nagios.cfg 检查Nagios配置文件sudo /usr/local/nagios/bin/nagios -v /usr/local/nagios/etc/nagios.cfg 配置文件无错误，重启Nagios服务sudo systemctl restart nagios 在远程主机中开启5666端口 1234su rootfirewall-cmd --zone=public --add-rich-rule=&apos;rule family=&quot;ipv4&quot; source address=&quot;192.168.0.92&quot; port port=&quot;5666&quot; protocol=&quot;tcp&quot; accept&apos; --permanentfirewall-cmd --zone=public --add-rich-rule=&apos;rule family=&quot;ipv4&quot; source address=&quot;192.168.0.92&quot; port port=&quot;5666&quot; protocol=&quot;udp&quot; accept&apos; --permanentfirewall-cmd -reload 打开Nagios Web界面查看新增的主机 修改检查ssh的默认端口在我们新增的主机上，ssh的端口被修改为9022,在Nagios默认的命令中，是使用默认的22端口来检查。修改的方法是在命令后添加!-p 9022即可。123456789101112# 修改v-centos.cfg文件# 定位到检查ssh的servicedefine service &#123; use local-service host_name v-centos service_description SSH check_command check_ssh!-p 9022 notifications_enabled 0&#125;# 重启Nagios服务sudo systemctl restart nagios 参考 Install Nagios on CentOS7 Nagiso remote monitor]]></content>
      <categories>
        <category>Nagios</category>
      </categories>
      <tags>
        <tag>nagios</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS下使用openswan配置IPsec]]></title>
    <url>%2F2019%2F04%2F30%2Fopenswan_ipsec.html</url>
    <content type="text"><![CDATA[基础信息使用开源的openswan搭建Host_to_Host基于Ipsec RSA加密的Tunnel，用于保障主机之间数据传输的机密性。openswan由2个组建构成：KLIPS和Pluto。KLIPS是执行加密解密数据的内核级代码，同时管理SPD(Security Policy Databases,安全策略数据库);Pluto是用户登录守护进程，控制IKE(Internet Key Exchange,因特网密钥交换)协商。 安装Openswan 使用yum安装sudo yum whatprovides openswansudo yum install libreswan-3.25-4.1.el7_6.x86_64 -y 启动/检查IPsec 启动ipsec服务sudo systemctl start ipsec 检查ipsecsudo ipsec verify对[FAILED]条目进行检查 启用ip_forwardecho 1 &gt; /proc/sys/net/ipv4/ip_forward 禁用send_redirectscd /proc/sys/net/ipv4/conf/default &amp;&amp; echo 0 &gt; send_redirects 禁用其他选项，使用echo 0 &gt;到其他文件中 重新检查ipsecsudo ipsec verify 配置openswan配置文件：/etc/ipsec.conf和/etc/ipsec.secrets初始化NSS数据库:123cd /etc/ipsec.drm -fr ./*.dbipsec initnss 两个主机之间的机密隧道 生成rsa密钥在left主机中生成密钥/usr/libexec/ipsec/newhostkey --output /etc/ipsec.d/v_to_test.secrets查看生成的密钥idipsec showhostkey --list 配置文件v_to_test.conf 1234567891011conn v_to_test left=192.168.0.91 leftid=@v-centos lefersasigkey=lef_rsa_key #leftnexthop=%defaultroute right=192.168.0.92 rightid=@centos-test rightrsasigkey=right_rsa_key authby=rsasig #rightnexthop=%defaultroute auto=start 将生成的密钥添加到配置文件中123456# 查看密钥的idipsec showhostkey --list# 根据id查看rsa公钥-leftipseck showhostkey --left --ckaid ckaid_id# 根据id查看rsa公钥-rightipseck showhostkey --right --ckaid ckaid_id 在left端启动v_to_host的ipsecipsec auto --up v_to_host 连同测试12345# 在right端启动tcpdump抓取网络端口enp0s3上的esp标签数据包tcpdump -n -i enp0s3 esp# 在left端启动ping测试ping centos-test# 查看right端数据包抓取情况 tunnel使用状况ipsec whack --trafficstatus 参考 https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/security_guide/sec-securing_virtual_private_networks]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>ipsec</tag>
        <tag>openswan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件特殊权限SUID、SGID、SBIT]]></title>
    <url>%2F2019%2F04%2F29%2Flinux_SUID_SGID_SBIT.html</url>
    <content type="text"><![CDATA[SUIDSUID的标识符为s,当该符号出现在文件拥有者权限的x位置时，表明该文件具有SUID权限。SUID的权限仅对二进制程序有效，对shell脚本是无效的，并且SUID仅在执行过程中有效。SUID的权限数字为4,设置的方法是在原来wrx的权限数字前再加上1个4,例如：原文件权限为511,加SUID的权限则4511。 SGID当符号s出现在文件拥有者群组x位置时，则称为SGID。SGID可以设置二进制程序，也可以设置目录，如果对目录设置了SGID权限，使用者可以在具有rx权限时进入该目录;使用者在该目录下的有效群组将会变成该目录的群组;使用者在该目录具有w权限，新建的文件群组与该目录的群组相同。SGID的权限数字为2,设置方法同SUID。 SBITSBIT仅对目录有效，对文件无效。对目录的作用是：使用者对目录有wx权限，在该目录下创建文件或目录时，仅有自己和root有权限删除。SGID的权限数字为1,设置方法同SUID。 查找 查找根目录下的特殊文件(SUID和SGID)find / \(-perm -4000 -o -perm -2000\) -exec ls -la {} \; ‘perm -4000’ find命令中根据文件权限查找，-4000表示所有包括SUID权限的文件]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>SUID</tag>
        <tag>SGID</tag>
        <tag>SBIT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux运维工程师面试题-部分]]></title>
    <url>%2F2019%2F04%2F26%2Flinux_om.html</url>
    <content type="text"><![CDATA[查看当前目录(包含子目录)下的文件数sudo ls -lR | grep &quot;^-&quot; -c或sudo ls -lR | grep &quot;^-&quot; | wc -l 参数R表示递归子目录 &quot;^-&quot;表示以符号-开头，-c计算符合条件的数目 如果计算目录数，把正则表达式修改&quot;^d&quot; grep工具是按行搜索 查看当前系统每个IP的连接数netstat -n | grep &quot;^tcp&quot; | awk &#39;{print $5}&#39; | awk -F : &#39;{print $1}&#39; | sort | uniq -c | sort -rnnetstat -nt | awk &#39;{print $5}&#39; | awk -F : &#39;{print $1}&#39; | sort | uniq -c | sort -rn netstat使用-t参数可以之间显示与tcp有关的数据,’-n显示地址和端口号’,-l处于listen状态，-u表示与udp有关的数据，-p显示pid awk参数中的-F :表示以符号：分隔，默认以tab或空格键，按行搜索并把每行分为多个部分 sort参数中-r表示反向排序,-n表示以纯数字进行排序，-u表示相同数据仅出现一行 uniq参数中-c表示计数 shell中生成32位随机密码cat /dev/urandom | head -n1 | md5sum | head -c32 &gt; /tmp/pass head参数中-n后跟数字表示显示几行，如果是负数表示列出总行数-x行前的,-c表示前多少字符 除md5sum运算外，还可以选择sha1sum,sha256sum ps命令中的Aux与VSZ ps命令可以查看某个时间点的程序运行状况，比较常用的是ps -l和ps aux,前者表示只列出自己bash相关的程序，后者表示所有bash的程序 STAT状态 R-&gt;Running S-&gt;Sleep，可以被唤醒 D-&gt;不可被唤醒，可能在等待I/O T-&gt;Stop背景暂停或除错状态 Z-&gt;Zombie僵尸状态，已经终止但是无法被移除内存外 TTY表示登录者的终端位置，远程登录则使用pts/n,与终端无关的显示为?,tty1-6为本机 VSZ程序使用的虚拟内存(Kb) RSS程序使用的固定内存(Kb) %MEM程序占用的实体内存百分比 top与ps的区别 top命令是动态观察程序的变化，ps是某个时间点的程序状态 load average分别是1min，5min，15min的负载，如果该数除以逻辑cpu的数量大于5表示系统超负荷 -d 跟秒数，top更新的秒数，默认为5s -p 指定pid P 按cpu资源排序 M 以memeory资源排序 N 以pid排序 T使用cpu时间累计排序 1数字1可以查看每个逻辑cpu的状况，再按一次返回 b高亮显示当前进程 m切换显示内存信息 q离开top PR表示程序执行的优先顺序，越小越早被执行 NInice的缩写，越小越早被执行 VIRT进程使用的虚拟内存总量，单位kb，VIRT=swap+res RES进程使用的物理内存 SHR共享内存 shell内取1-39的随机整数expr ${RANDOM} % 39 + 1 expr是一个手工命令行计数器，用于求表达式变量的值，可以进行+ - * / %的运算， 进行乘法运算时需要将符号*转义\*,数字与元算符号之间有空格 RANDOM是系统变量，会产生0～32767之间的数值 显示文件/etc/ssh/sshd_config文件中以#开头，并且后面跟一个或多个空白字符，而后又跟任意非空字符，并显示行号grep -n &quot;^# \{1,\}[^ ]&quot; /etc/ssh/sshd_config grep中参数-n用于显示行号 ^万用字符表示以某个字符开头，$万用字符表示以某个字符结尾，[^ ]表示非空白字符 X\{1,\}表示一个以上X字符，\用于转义符号{},X\{2,5\}表示 使用shell批量创建用户和默认密码，并保存用户名和密码到文件中123456789101112131415#!/bin/bash# create group if not existgrep test /etc/shadow[ $? -eq 0 ] || groupadd test# create user for i in &apos;seq -f&quot;%02g&quot; 1 10&apos;do # create user use command:useradd useradd -s /bin/bash -g test user$&#123;i&#125; &gt; /dev/null 2&gt;&amp;1 user_passwd=&quot;`echo $&#123;RANDOM&#125;|md5sum|head -c8`&quot; # change passwd use command:passwd --stdin echo &quot;$&#123;user_passwd&#125;&quot;|passwd --stdin user$&#123;i&#125; &gt; /dev/null 2&gt;&amp;1 # save user_name and user_passwd echo &quot;user$&#123;i&#125; :$&#123;user_passwd&#125;&quot; &gt;&gt; /home/kim/user_passwd.txtdone echo 在输出变量还有其他字符时，需要使用双引号&quot;&quot;,如果输出的内容需要调用其他命令还需要使用字符 seq格式化%02g保留2位，不足位用0补充 使用判断符号[]要注意左右的空格 标准输出和标准错误输出2&gt;&amp;1 垃圾桶/dev/null 查找/var/log目录下后缀格式为log，大小超过1M的文件数目find /var/log -name &quot;*.log&quot; -type -f -size +1024k | wc -l 查找/tmp目录下10天内未修改的文件，并删除find /tmp !-mtime -10 -exec rm -fr {} \; 查找当前目录下的空文件夹/文件并删除find ./ -type d -empty -exec rm -fr {} \;find ./ -type f -size 0c -exec rm -fr {} \; 显示磁盘使用率超过50%的分区df -h |awk &#39;+$5&gt;50&#39; 使用一元加运算+$5表示第五列乘以+1(正1)，一元减表示乘以负1 打包本目录下的所有文件为web.tar.gz，排除文件夹log和文件testtar -czvf web.tar.gz ./* --exclude=./log/ --exclude=./test umask022代表的意思umask代表创建文件或目录的默认权限，计算方式为777分别减去umask的值 ，umask022代表创建文件或目录的默认权限为755,即u为rwx，g为rx，o为rx 查看某个进程/用户打开的文件 123456789101112131415161718192021222324\# 1个或多个进程打开的文件\## 根据进程名lsof -c processlsof | grep processlsof -c process1 -c process2 \## 根据进程号lsof -p pid,pid1,pid2\# 用户打开的文件lsof -u user\# 除用户user外打开的文件lsof -u ^user\# 查看正在使用文件lsof /path/filename\# 查看网络信息\## tcplsof -i tcp\## udplsof -i udp\## 端口号lsof -i:1080\## 使用端口号的tcplsof -i tcp:1080\## 用户的所有活跃的网络端口lsof -a -u user -i 常用服务的端口号 http/https:80 ftp:21 ssh:22 sftp:22 smtp:25/465/993 pop3:110/995 imap:143/993]]></content>
      <categories>
        <category>Linux</category>
        <category>运维</category>
      </categories>
      <tags>
        <tag>运维</tag>
        <tag>面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LVM添加硬盘操作过程]]></title>
    <url>%2F2019%2F04%2F24%2Flvm_add_partition.html</url>
    <content type="text"><![CDATA[基本信息系统中原有一个lvm0的卷组，卷组中有3个lv，名称和挂载分别是:home-&gt;/home,root-&gt;/,swap-&gt;swap新添加的硬盘是/dev/sdb大小为465.8G，需要新增1个卷组，名称为lvm1,划分为2个lv，名称和大小分别是work-&gt;200G,personal–&gt;200G,分区格式为xfs，分别挂载到~/work和~/personal 操作过程 新建分区1234# 使用fdisk分区sudo fdisk /dev/sdb# 创建1个分区，大小分别是401G按p输出当前分区信息，d删除当前分区，g创建1个新的GPT分区表，n新建1个主分区，p查看分区信息，w保存推出 新建pv分区12345# 查看pv分区sudo pvscan# 将新分区添加到pv分区sudo pvcreate /dev/sdb&#123;1,2&#125;# 查看新pv分区 新建逻辑卷组lvm1sudo vgcreate -s 4M lvm1 /dev/sdb{1,2} 创建lv123456# 新建lv用于work,增加到200G，添加到lvm1卷组sudo lvcreate -L 200G -n work lvm1# 新建lv用于personal,增加到200G，添加到lvm1卷组sudo lvcreate -L 200G -n personal lvm1# 查看lv分区sudo lvdisplay 格式化lv分区123# 分别将lv分区格式化为xfs格式sudo mkfs.xfs /dev/lvm1/worksudo mkfs.xfs /dev/lvm1/personal 自动挂载分区1234567# 备份原文件cd /etc &amp;&amp; sudo cp fstab fstab.old# 编辑fstab文件sudo vi fstab# 将lv分区添加/dev/mapper/lvm1-work /home/kim/work xfs defaults 0 0/dev/mapper/lvm1-personal /home/kim/personal xfs defaults 0 0 新建挂载目录cd ~ &amp;&amp; mkdir work personal重启进入系统后，可以分别到work和personal目录下查看磁盘空间df -BG ./需要注意的是如果移除了某个lv分区，并且该分区在fstab文件中，同时需要在fstab文件中，将该分区移除，否则启动时会报错]]></content>
      <categories>
        <category>lvm</category>
      </categories>
      <tags>
        <tag>lvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用PAM控制登录访问]]></title>
    <url>%2F2019%2F04%2F23%2Fpam_access_time.html</url>
    <content type="text"><![CDATA[限制访问的来源pam_access模块可以限制登录的用户或用户组的来源。利用该模块，首先需要配置那些需要使用该模块的服务。通过编辑/etc/pamd.d中服务的pam配置文件来实现。 将pam_access模块添加到login服务中1234cd /etc/pam.d &amp;&amp; sudo cp login login.oldsudo vi login# 新增一行account required pam_access.so 编辑文件/etc/security/access.conf当一个服务配置文件中的一个条目调用了pam_access,它将彻底检查access.conf文件，并停留在第一个匹配的行上。access.conf文件配置的原则是：具体条目放在前面，通用条目放在后面。条目格式： 1234permission : users : originspermission #permission可以为+或-，分别表示授权访问，拒绝访问 : users #可以指定用户或用户组，可以使用空格分隔多个用户或组，还可以使用user@host的形式，host表示被登录主机的本地主机名 : origins #可以使用主机名来表示远程主机源，LOCAL关键字用来表示本地访问，还可以使用ALL和EXCEPT关键字。还可以使用ip地址，域名 禁止用户test1本地登录主机 1234cd /etc/securitysudo cp access.conf access.conf.oldsudo vi access.conf-:test1:ALL 禁止用户kim通过workpc主机ssh登录主机1234567891011121314# 启用pam_accesscd /etc/pam.d &amp;&amp; sudo cp sshd sshd.old# 添加pam_access模块auth required pam_access.so# 添加用户cd /etc/security/ &amp;&amp; sudo vi access.conf# 添加一行数据-:kim:workpc# 只允许通过workpc登录-:kim:EXCEPT workpc# 允许workpc和192.168.0.99登录+:kim:workpc+:kim:192.168.0.99-:kim:ALL Login_From_workpcLogin_From_192.168.0.99Login_From_192.168.0.98 限制访问时间限制访问时间使用pam_time模块来实现，通过修改文件/etc/security/time.conf来进行设置。配置格式：services;devices;users;times service条目可以通过’/etc/pam.d’内的文件获取; devices可以使用!ttyp*表示控制台，ttyp*用来表示远程设备，tty*用来表示所有设备 users列表可以使用符号|分隔; times每个时间范围由2个字符缩写，前面的用来指示规则应用日期，后面一个用来指示在哪几个小时之间Mo\Tu\We\Th\Fr\Sa\Su，Wk表示工作日，Wd表示周末，A1表示每周的每一天 在需要启用的服务中添加pam_time模块 123cd /etc/pam.d &amp;&amp; sudo vi sshd# 添加模块account required pam_time.so 添加详细条目 1234cd /etc/security &amp;&amp; sudo cp time.conf time.conf.oldsudo vi time.conf# 添加条目,禁止用户test1在周二的13：30-14：00期间所有终端登录sshd;*;test1;!Tu1330-1400]]></content>
      <categories>
        <category>安全</category>
      </categories>
      <tags>
        <tag>pam_access</tag>
        <tag>pam_time</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql数据库导入CSV数据]]></title>
    <url>%2F2019%2F04%2F21%2Fmysql_import_csv.html</url>
    <content type="text"><![CDATA[创建数据库 新建1个数据库create database test_db; 授权用户权限;grant all on test_db.* to &#39;user&#39;@&#39;host&#39; idedntified by &#39;Paaswd#2019&#39;; 创建基本表 新建一个数据表 123456789use test_db;create table movie ( id char(10) primary key, movie_name char(20), movie_star float(1), movie_people int, movie_time char(20), movie_country char(10) ); 查看新创建的数据表show columns from movie; 导入数据12345load data local infile &apos;/home/use/move.csv&apos; into table movie fields terminated by &apos;,&apos; lines terminated by &apos;\n&apos; ignore 1 rows; 如果出现secure-file-priv相关的提示需要手动设置secure-file-priv;123456sudo vi /etc/my.cnf# 添加secure-file-priv=&quot;&quot; #NULL表示限制mysql不允许导入导出，/tmp表示只允许导入导出到/tmp目录，为空时表示不限制mysql导入导出到任意目录# 重启mysql服务sudo systemctl restart mysqld 查看导入的数据select * from movie where id=1; 参考 https://chartio.com/resources/tutorials/excel-to-mysql/]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>csv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7启用SELinux和Firewall修改ssh端口号]]></title>
    <url>%2F2019%2F04%2F21%2Fcentos_ssh_port.html</url>
    <content type="text"><![CDATA[基本信息CentOS：CentOS Linux release 7.6.1810 (Core)SELinux：enforcedFirewall：enforcing 生成ssh密钥对 生成密钥对ssh-keygen -t rsa -b 2048 #默认存放的位置是/home/user/.ssh，使用的是公钥id_rsa.pub 从服务器中将私钥复制到本机 或者使用本地生成的密钥对，把公钥复制到服务器中 修改ssh配置文件 备份原文件cd /etc/ssh &amp;&amp; sudo cp sshd_config sshd_config.old 修改ssh配置文件sudo vi sshd_config对以下选项进行修改：12345678910111213# 修改端口号port 9022# 如果服务器有多个网卡，需要修改ssh服务监听地址ListenAddress 192.168.0.91# 禁用ssh root登录PermitRootLogin no# 错误登录次数MaxAuthTries 4# 使用自定义的ssh_key登录AuthorizedKeysFile /home/user/.ssh/id_rsa.pub# 禁止使用密码和空密码登录PasswordAuthencation noPermitEmptyPasswords no 添加SELinux策略 如果不添加SELinux策略，启动sshd服务会有Permission denied报错; 查看ssh的SELinux端口sudo semanage port -l | grep ssh如果提示没有semanage命令，使用命令sudo yum whatproides /usr/sbin/semanage查看需要安装的软件包;安装软件包sudo yum install policycoreutils-python -y,之后再重新查看ssh的端口号; 添加策略sudo semanage port -a -t ssh_port_t -p tcp 9022sudo semanage port -l | grep ssh 添加Firewall策略 只允许本机或某个网段通过指定的端口号ssh登录到服务器1234su rootfirewall-cmd --zone=public --add-rich-rule=&apos;rule family=&quot;ipv4&quot; source address=&quot;192.168.0.90&quot; port port=&quot;9022&quot; protocol=&quot;tcp&quot; accept&apos; --permanentfirewall-cmd --reloadfirewall-cmd --zone=public --list-all 重启sshd服务 使用systemctl重启sshd服务sudo systemctl restart sshd 远程ssh登录 Linux下ssh登录ssh -p 9022 user@server_host或者使用Linux本机下的ssh密钥对ssh-copy-id user@server_host Windows下ssh登录使用putty，配置服务器地址和端口号，选择ssh_key验证登录]]></content>
      <categories>
        <category>CentOS</category>
        <category>ssh</category>
      </categories>
      <tags>
        <tag>ssh</tag>
        <tag>SELinux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql学习笔记]]></title>
    <url>%2F2019%2F04%2F16%2Fsql-learning.html</url>
    <content type="text"><![CDATA[结构化查询语言SQL学习笔记- 基础知识基础知识 主键：主键是唯一的，一个数据表中只能包含一个主键，可以使用主键来查询数据; 外键：外键用来关联2个表; 符合键：符合键(组合键)将多个列作为一个索引键; 索引：使用索引可快速访问数据表中的特定信息。索引是数据表中一列或多列的值进行排序的一种结构; SQL常用动词 数据查询：select 数据定义：create，drop，alter 数据操纵：insert，update，delete 数据控制：grant，revoke SQL对关系数据库模式的支持：外模式，模式，内模式。3个模式的基本对象有表，视图和索引; 外模式：外模式对应视图和部分基本表; 模式：模式对应基本表; 内模式：内模式对应存储文件; SQL的数据定义语句 模式:定义模式实际上是定义了一个命名空间，可以在这个空间中进一步定义该模式包含的数据库对象; 创建：create schemacreate schema &lt;schema_name&gt; authorization &lt;user_name&gt; 删除：drop schemadrop schema &lt;schema_name&gt; &lt;cascade|restrict&gt; cascade级联模式：在删除模式的同时把该模式中所有的数据库对象全部删除; restrick限制模式：如果在定义的模式中包含数据库对象，则拒绝删除该模式，近当该模式下没有数据库对象时才执行删除; 表：基本表是本身独立存在的表，一个关系就对应一个基本表。一个或多个基本表对应一个存储文件，一个表可以带若干索引，索引也存放在存储文件内中; 创建：create table 删除： drop table 修改：alter table 视图：视图是从一个或几个基本表导出的表。它本身不独立存储在数据库中，数据库中只存放视图的定义而不存放视图对应的数据。这些数据仍存放在导出视图的基本表中，视图是一个虚表 创建：create view 删除：drop view 索引 创建：create index 删除：drop index 基本表的定义、删除与修改 定义基本表create table &lt;table_name&gt; (&lt;列名&gt; &lt;数据格式&gt; &lt;约束条件&gt;,&lt;列名&gt; &lt;数据格式&gt; &lt;约束条件&gt;) foreign key可以用于预防破坏表之间连接的动作，也可以防止非法数据插入外键列 *创建数据库create database if not exists test_db default charset utf8 collate utf8_general_ci; collate 意思是在排序时个怒utf-8编码格式来排序 default charset 设置默认的字符集]]></content>
      <categories>
        <category>sql</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7使用repo安装Mysql-community]]></title>
    <url>%2F2019%2F04%2F15%2Fcentos-install-mysql57.html</url>
    <content type="text"><![CDATA[下载mysql-repo 通过https://dev.mysql.com/downloads/repo/yum/下载RHL7的repo软件源; 如果需要，则上传下载的repo文件到服务器中; 在Centos中安装repo rpm -ivh mysql80-community-release-el7-2.noarch.rpm 如果系统中已经安装mysql-community的repo，则使用rpm -Uvh mysql80-community-release-el7-2.noarch.rpm来更新; 使用yum-config-manager选择mysql版本 安装的repo默认启用mysql80-community版本，如果需要安装mysql57-community则使用yum-config-manager来启用 启用mysql57安装源 安装yum-utils yum install yum-utils -y 禁用mysql80安装源 yum-config-manager --disable mysql80-community 启用mysql57安装源 yum-config-manager --enable mysql57-community 查看启用的mysql安装源 yum repolist enabled | grep mysql 安装Mysql57-community-server 安装 yum install mysql-community-server -y 启动mysql systemctl start mysqld 查看mysql默认密码 cat /var/log/mysqld.log | grep passw 修改root密码 mysql -u root -p #使用默认的密码登录mysql alter user &#39;root&#39;@&#39;localhost&#39; identified by &#39;NewPasswd2019!&#39;; #新的密码要符合mysql的密码策略，否则会出错 flush privileges; 授权用户 create user db_user identified by &#39;newPasswd&#39;; create database test; grant all on test.* to &#39;db_user&#39;@&#39;192.168.0.90&#39; identified by &#39;newPasswd&#39;; flush privileges; 设置防火墙策略 查看防火墙状态 firewall-cmd --zone=public --list-all 添加rich-rule firewall-cmd --zone=public --add-rich-rule=&#39;rule family=&quot;ipv4&quot; source address=&quot;192.168.0.90/24&quot; port port=&quot;3306&quot; protocol=&quot;tcp&quot; accept&#39; –permanent firewall-cmd --reload 远程连接数据库 使用nmap工具扫描服务器 nmap -A 192.168.0.91 远程连接数据库 使用命令行或者workbench工具连接数据库 参考 https://dev.mysql.com/doc/mysql-yum-repo-quick-guide/en/]]></content>
      <categories>
        <category>centos</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用convert命令创建自动添加水印的脚本]]></title>
    <url>%2F2019%2F01%2F25%2Fconvert-watermark.html</url>
    <content type="text"><![CDATA[convertconvert是ImageMagic中的，如果要使用convert的相关功能，首先系统中安装ImageMagic。脚本实现的是基础功能，将一张图片作为水印，添加到当前文件夹中的所有图片中，图片支持的格式有jpg|png|jpeg|gif,添加水印后的照片被保存到当前目录下的w_pic。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/bin/bash# date:2019-01-17# function: Add watermark to all picture in this dir by command convert.# version: 1.0# start script# envhome_dir=$&#123;PWD&#125;cd $&#123;home_dir&#125;# copyright copy_png=/home/kim/Documents/img/copy.png# test list_file and w_pic exit or notif [ -e &quot;list_file&quot; ]; then rm -fr list_fileelif [ -d &quot;w_pic&quot; ]; then rm -fr w_picfi# search imge file,print filename to list_filetouch list_filefind ./ -name &quot;*.png&quot; 1&gt; ./list_filefind ./ -name &quot;*.png&quot; 1&gt;&gt; ./list_filefind ./ -name &quot;*.gif&quot; 1&gt;&gt; ./list_filemkdir w_pic# start convert# f_num=1cat $&#123;home_dir&#125;/list_file | while read f_namedo convert $&#123;f_name&#125; label: $&#123;copy_png&#125; -gravity center -append $&#123;home_dir&#125;/w_pic/$&#123;f_name&#125; # rm -fr $&#123;f_name&#125; echo &quot;$&#123;f_name&#125; complate!&quot; # f_num=$(($&#123;f_num&#125;+1))done# echo $&#123;f_num&#125;## print if [ $?==0 ]; then echo &quot;New Pic location is $&#123;home_dir&#125;/w_pic&quot; exit 0else echo &quot;ERROR!&quot;fi# clean rm -f $&#123;home_dir&#125;/list_file exit 1]]></content>
      <categories>
        <category>convert</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>convert</tag>
        <tag>水印</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fedora使用Proxychains代理终端程序]]></title>
    <url>%2F2019%2F01%2F16%2Fproxychains_fedora.html</url>
    <content type="text"><![CDATA[基础Proxychins以下简称Pcs，是全局代理服务，可以在命令行中通过Pcs代理各种软件，支持的协议有：http/https/socks4/socks5。在Fedora中使用的是proxychains-ng,使用命令查看软件包的相关信息dnf info proxychains-ng. 安装与配置 安装dns install proxychains -y 配置Pcs默认的配置文件是proxychains.conf,默认存放的位置是/etc/proxychains.conf,配置文件查找的先后顺序是: ./proxychains.conf ~/.proxychains/proxychains.conf /etc/proxychains.confPcs支持多种代理模式，默认的是strict_chain: dynamic_chain:动态模式，按照代理列表先后顺序，逐级连接，组成一条连接，如有失效服务器，自动排除； strict_chain:严格模式，严格按照代理列表先后顺序，逐级连接，组成一条连接，所有服务器必须有效； round_robin_chain:轮询模式，自动跳过不可用代理； random_chain:随即模式，随机使用代理列表中的服务器；代理列表：123[ProxyList]socks5 127.0.0.1 1080https 127.0.0.1 1090 使用PCS的使用主要在命令行中，使用的语法是proxychains4 command options在当前bansh中执行的任意命令都通过Pcsproxychains4 -q /bin/bash还可以通过别名的方式，缩短程序名称alias pcs4=&#39;proxychains4&#39;]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>proxychains</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NFS服务器的学习笔记]]></title>
    <url>%2F2019%2F01%2F14%2Fnfs-server-learning.html</url>
    <content type="text"><![CDATA[基础NFS是Network FileSystem的缩写，用于网络中不同操作系统之间的文件共享，NFS中不同的功能会启用不同的端口(小于1024)，由于这个特性，就需要远程过程调用RPC服务。RPC的主要功能是指定每个NFS功能所对应的端口号，并且回报给客户端，让客户端连接到正确的端口号。在NFS启动时会随即取用多个端口，并主动向RPC注册，因此RPC可以知道每个端口对应的功能，RPC使用固定的111端口来监听客户端的请求。NFS服务器主要任务是进行文件的分享，文件系统的分享与权限有关。NFS启动至少需要2个daemons，一个管理客户端能否登录，一个管理客户端能够取得的权限。 安装 安装RPC主程序yum install rpcbind 安装NFS主程序yum install nfs-utils 配置 NFS配置文件 NFS的配置文件位于/etc/exports,如果系统中没有该文件，需要自行创建; 在配置文件exports中，每一行的最前面代表要分享的目录,根据不同的权限可以分配给不同的客户端,在主机或网段后面直接用括号将对应的全县括起来，多个权限的可以使用符号,分割，主机名称可以使用通配符号* ?； 123/tmp 192.168.0.0/24(ro) 192.168.0.100(rw) kvm-centos(ro,sync)# 在192.168.0.0/24网段中对/home/tmp具有读写权限，不在该网段的只有读权限/home/tmp 192.168.0.0/24(rw) *(ro) 主机对应的权限： rw 分享的权限为read和write ro 分享的权限为read only sync 数据会同步写入到内存和磁盘中 async 数据会暂存内存中，不写入磁盘 no_root_squash/root_squash 预设情况下，客户端以root登录的，NFS会由root_squash压缩为nfsnobody，如果需要开放客户端root登录，需要设定no_root_squash all_squash 不管客户端以什么身份登录，都被压缩为nfsnobody NFS管理 启动NFS 12345678# 启动rpcsystemctl start rpcbind# 开启自启动systemctl enable rpcbind# 启动nfssystemctl start nfs# 开启自启动systemctl enable nfs 查看NFS和rpc启动的端口号netstat -tulnp | grep -E &#39;(rpc|nfs)&#39; 查看本机的rpc注册情况rpcinfo -p localhost 本机测试联机情况showmount -e localhost # 可用参数有2个:a/e，前者表示查看主机与客户端的挂载情况 修改exports文件后需要重新载入 重新载入用到的命令是exportfs a 全部挂载/卸载文件内的设定 r 重新挂载文件内的设定，同时同步更新exports /var/lib/nfs/xtab中的内容 u 卸载某一个目录 v 将分享的目录输出到屏幕 固定NFS使用的端口号 通过修改文件/etc/sysconfig/nfs中关于端口号的选项,主要有mountd/rquotad/nlockmgr这三个服务，分别对应的是RQUOTAD_PORT/LOCKD_TCPPORT/LOCKD_UDPPORT/MOUNTD_PORT 将nfs和rpc服务加入到防火墙中 修改nfs的端口号为固定，使用rpcinfo查询出注册的mountd|rquotad|nlockmgr的端口号，将端口号分别加入到防火墙中 rpcinfo -p localhost | grep -E &#39;(mountd|rquotad|nlockmgr)&#39;12345firewall-cmd --zone=public --add-service=rpc-bind --permanentfirewall-cmd --zone=public --add-service=nfs --permanentfirewall-cmd --zone=public --add-rich-rule=&apos;rule family=&quot;ipv4&quot; source address=192.168.0.0/24 port port=32802-32804 protocol=tcp accept&apos; --permanentfirewall-cmd --zone=public --add-rich-rule=&apos;rule family=&quot;ipv4&quot; source address=192.168.0.0/24 port port=32802-32804 protocol=udp accept&apos; --permanentfirewall-cmd --reload NFS的挂载 在客户端中需要启动rpcbind，执行挂载命令mount -t nfs ip or server_name:/path /mount_path 使用额外的参数 suid/nosuid 挂载/不挂载suid rw/ro 挂载为读写或只读 dev/nodev 保留/不保留挂载设备的特殊功能 exec/noexec 挂载的文件具有/不具有执行权限 user/nouser 允许/不允许用户对文件进行挂载与卸载 auto/noauto mount -a 时自动/不自动挂载]]></content>
      <categories>
        <category>学习笔记</category>
        <category>NFS</category>
      </categories>
      <tags>
        <tag>NFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fedora安装与配置Samba服务]]></title>
    <url>%2F2019%2F01%2F12%2Ffedora-samba.html</url>
    <content type="text"><![CDATA[安装dnf install samba -y 配置 samba的配置文件位于/etc/samba/smb.conf 需要配置的文件有2部分[global]和[share] 1234567891011121314151617181920[global] workgroup=samba netbios name=samba_server #需要与workgroup不同 server string=samba_server log file=/var/log/samba/%T_%I_%m.log max log size=50 load printers=no security=USER passdb backend=tdbsam lanman auth=yes ntlm auth=yes hosts allow=192.168.0.[share] comment=home path=/path browseable=yes writeable=yes writelist=user1,@group1 create mode=0644 directory mode=0755 测试配置文件testparm 将系统中的用户添加到samba中pdbedit -a -u user 添加防火墙规则firewall-cmd --zone=public --add-rich-rule=&#39;rule family=&quot;ipv4&quot; source address=192.168.0.0/24 port port=139 protocol=tcp accept&#39; --permanentfirewall-cmd --zone=public --add-rich-rule=&#39;rule family=&quot;ipv4&quot; source address=192.168.0.0/24 port port=445 protocol=tcp accept&#39; --permanentfirewall-cmd --reload 添加SELinux配置 12345su rootsetsebool -P samba_export_all_ro=1 samba_export_all_rw=1getsebool –a | grep samba_exportsemanage fcontext –at samba_share_t &quot;/home/user/path(/.*)?&quot;restorecon /home/user/path 启动samba服务systemctl start smb.servicesystemctl enable smb.service 在Windows系统中映射网络磁盘 在Linux中挂载磁盘:1234567# 使用smbclient观察samba服务smbclient -L server_ip -U user_name# 以FTP的方式的登陆smbclient &apos;//ip/share&apos; -U user_name# 以网络磁盘的方式挂载mount -t cifs //ip/share /mnt -o username=user,password=passwd,vers=1.0*如果出现不能挂载的情况，需要指定vers版本号为1.0*]]></content>
      <categories>
        <category>fedora</category>
        <category>samba</category>
      </categories>
      <tags>
        <tag>samba</tag>
        <tag>fedora</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo备份到git]]></title>
    <url>%2F2019%2F01%2F12%2Fhexo-backup-git.html</url>
    <content type="text"><![CDATA[如果本地hex的工作目录使用的是 xxx.github.io的方式命名的，需要将文件夹重命名hexo(名称自定)； 在git中新建一个repo，名称与本地文件夹相同； 初始化本地文件夹git init 设置不需要推送到git的文件和文件夹： 12345 touch .gitignore # 在.gitignore文件夹中加入 *.log /public*.deploy/ 删除主题文件夹中的.git文件夹； 在本地添加远程仓地址git remote add origin git@github.com:kim0x/hexo.git; 将本地源文件推送到git中：123git add .git commit -m &quot;backup&quot;git push origin master 使用脚本定时备份和更新1234567891011121314#!/bin/bash# envour_home=/home/kim/Documents/kim1024.github.iobackup_home=/home/kim/Documents/hexonow_date=`date &quot;+%Y%m%d&quot;`# test log file exist or not[ !-e /home/kim/tmp/git_update.log ] || rm -f /home/kim/tmp/git_update.log#copy file from our_home to backup_homecp -r $&#123;our_home&#125;/* $&#123;backup_home&#125;/# push update to githubcd $&#123;backup_home&#125;git add . &gt; /home/kim/tmp/git_update.log 2&gt;&amp;1git commit -m &quot;$&#123;now_date&#125;&quot; &gt;&gt; /home/kim/tmp/git_update.log 2&gt;&amp;1git push origin master &gt;&gt; /home/kim/tmp/git_update.log 2&gt;&amp;1 将脚本文件命为update_git.sh,添加执行权限chmod u+x update_git.sh,添加到用户的crontab定时执行中:1234567891011# 打开crontabcrontab -e# 添加任务计划,每周五11点执行0 11 * * 5 /home/kim/tmp/update_git.sh# 保存文件:wq!# 查看用户crontab计划crontab -l# 删除当前的任务计划crontab -e删除文件中的所有计划]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>git</tag>
        <tag>备份</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用KVM创建Windows虚拟环境]]></title>
    <url>%2F2019%2F01%2F12%2Fkvm-windows.html</url>
    <content type="text"><![CDATA[经过测试，使用kvm创建的Windows环境，在使用体验和性能上不如Virtualbox，如非特殊需求，可以在Virtualbox中虚拟Windows环境。 创建kvm 通过命令行模式创建基于kvm的全虚拟化Windows XP环境: 1234567891011virt-install \ --name winxp \ --virt-type kvm \ --hvm \ --os-variant winxp \ --memory 2048 \ --vcpus 2 \ --graphics vnc \ --network bridge=br0,model=virtio \ --cdrom /home/user/ios/winxp.iso \ --disk path=/home/user/kvm/disk/winxp25G.qcow2,size=25,bus=ide 在创建虚拟环境时，有以下几点需要注意： 关于全虚拟化，可以根据需要选择性开启，如果开启，添加--hvm,如果不开启，则将--hvm移除； 设置网络时，注意添加网络模式virtio; 在添加磁盘时，格式选择为qcow2,同时要指定bus模式为ide，如果指定为virtio模式，启动虚拟环境时，会出现蓝屏； 安装完成后，需要安装Windows的网络驱动: 首先需要下载virtio iso格式的驱动 ，驱动的下载地址https://docs.fedoraproject.org/en-US/quick-docs/creating-windows-virtual-machines-using-virtio-drivers/; 将下载的iso格式的驱动文件挂载到虚拟机中virsh edit winxp编辑虚拟机的文件，找到cdrom选项，将驱动的iso文件路径替换掉原来的，在进行编辑之前，首先要将原来的xml文件备份virsh dumpxml winxp &gt; /home/user/kvm/winxp.xml; 启动虚拟机，安装网卡驱动，测试网络；]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS主机防火墙Firewalld学习笔记]]></title>
    <url>%2F2019%2F01%2F12%2Ffirewall-cmd-learning.html</url>
    <content type="text"><![CDATA[目前在CentOS中使用firewalld替代iptables，firewalld是iptables的前段控制器，与iptables的区别是：前者使用区域和服务而不是链式规则，它动态管理规则集，允许更新规则而不破坏现有的会话和连接。firewalld的配置文件主要存放在2个位置：/usr/lib/Firewalld和/etc/firewalld,前者存放的是默认配置，后者存放的是自定义配置。 基本命令12345678systemctl start/stop/enable/disable/status firewalld # 启动/关闭/开机启动/取消开机启动/状态firewall-cmd --state # 运行状态firewall-cmd --get-active-zones # 查看被激活的zonefirewall-cmd --get-zones # 查看预设zonefirewall-cmd --get-zone-of-interface=eth0 # 查看接口eth0的zone信息firewall-cmd --zone=public --list-all # 查看指定zone的所有信息firewall-cmd --get-service # 查看所有级别被允许的信息firewall-cmd --zone=public --get-target --permanent # 获取public的target 规则管理命令1234567firewall-cmd --panic-on # 终止所有数据包firewall-cmd --panic-off # 取消终止firewall-cmd --query-panic # 查看终止状态firewall-cmd --reload # 更新规则，不重启服务firewall-cmd --complete-reload # 更新规则，重启服务firewall-cmd --zone=public --add-interface=eth0 --permanent # 将eth0接口设置为public级别，永久生效firewall-cmd --set-default-zone=public # 设置public为默认级别 端口管理命令123firewall-cmd --zone=public --list-ports # 现实public级别被允许进入的端口firewall-cmd --zone=public --add-port=8080/tcp # 允许tcp端口8080至public级别firewall-cmd --zone=public --add-port=5060-5090/udp --permanent # 允许udp端口5060-5090范围内的端口至public级别，并永久生效 端口转发命令1234firewall-cmd --zone=public --add-masquerade # 首先打开端口转发firewall-cmd --zone=public --add-forward-port=port=22:proto=tcp:toport=8022 # 将tcp22端口转发到8022firewall-cmd --zone=public --add-forward-port=port=22:proto=tcp:toaddr=192.168.0.100 # 将22端口转发到另一个地址上的同一个端口firewall-cmd --zone=public --add-forward-port=22:prototcp:toport=8022:toaddr=192.168.0.100 # 转发22端口到另一个地址的8022端口 网卡管理命令1234firewall-cmd --zone=public --list-interfaces # 显示级别为public的所有网卡firewall-cmd --zone=public --add-interface=the0 --permanent # 将eth0的级别设置为public，并永久生效firewall-cmd --zone=dmz --change-interface=eth0 --permanent # 将网卡eth0从原来的级别修改为dmz，并永久生效firewall-cmd --zone=public --remove-interface=eth0 --permanent # 将网卡eth0从public级别中永久删除 系统服务管理命令12firewall-cmd --zone=public --add-service=smtp # 添加服务smtp到public级别中firewall-cmd --zone=public --remove-service=smtp # 从public级别中移除smtp服务 zonefirewall能将不同的网络连接归类到不同的信任级别，zong提供以下几个级别 drop 丢弃所有进入的包 block 拒绝所有外部发起的连接，允许内部发起的连接 public 允许指定的连接进入 external 一般用于路由转发 dmz 允许受限制的连接进入 work 允许信任的计算机被限制的进入连接 home 家庭网络，仅仅接收经过选择的连接 internal 内部网络，仅仅接收经过选择的连接 trusted 信任所有连接 过滤规则过滤规则的优先顺序是source&gt;interface&gt;firewalld.conf,支持的过滤规则有： source 源地址 interface 网卡 service 服务名 port 端口 icmp-block icmp类型 masquerade ip地址伪装 forward-port 端口转发 rule 自定义规则 traget当区域处理它的源或接口上的一个包时，但是没有处理该包的显式规则时，这时区域目标的target决定了该行为，tariget有以下几种： ACCEPT 通过该包 REJECT 拒绝该包 DROP 丢弃该包 default 不做任何事情 富文本规则使用富文本规则，可以用比较直接接口方式更容易理解的方法建立复杂防火墙规则，还可以永久保留设置。 富文本的命令格式和结构： 1234567rule [family=&quot;&lt;rule family&gt;&quot;] [ source address=&quot;&lt;address&gt;&quot; [invert=&quot;True&quot;] ] [ destination address=&quot;&lt;address&gt;&quot; [invert=&quot;True&quot;] ] [ &lt;element&gt; ] [ log [prefix=&quot;&lt;prefix text&gt;&quot;] [level=&quot;&lt;log level&gt;&quot;] [limit value=&quot;rate/duration&quot;] ] [ audit ] [ accept|reject|drop ] 一个规则是关联某个zone的，一个zone可以有多个分区，如果几个规则相互影响或冲突，则执行和数据包相匹配的第一个规则； 规则系列可以限定ipv4或ipv6，如果没有指定，将同时为ipv4和ipv6增加规则； 规则命名 source 指定源地址,可以使用invert=”True”或invert=”Yes”来颠倒源地址； denstination 指定目的地址，同源地址用法 service 服务名称 port 端口，可以是单个端口，也可以是端口范围n1-n2,协议可以是tcp或udp port=8022-8033 protocol=tcp protocol 协议，可以是协议ID数字，也可以是协议名称 protocol value=icmp log 注册含有内核记录的新的连接请求到规则中，可以定义一个前缀文本，记录等级可以是emerg/alert/crit/error/warning/notice/info/debug 等级用正的自然数表示，继续时间的单位s/m/h/d,最大限定值是1/d accept/reject/drop 执行的动作可以是accept/reject/drop中的1个，选择drop所有数据包会被丢弃，并且不会向来源地发送任何信息 操作示例 将ssh服务添加到internal级别中，仅允许192.168.0.0/24通过22端口访问,但是禁止192.168.0.92禁止登陆ssh 123456firewall-cmd --list-all # 查看firewall-cmd --zone=public --remove-service=ssh --permanent # 从public中永久移除ssh服务firewall-cmd --zone=internal --add-service=ssh --permanent # 将ssh服务添加到internal级别中firewall-cmd --zone=internal --add-source=192.168.0.0/24 --permanent # 添加源地址firewall-cmd --zone=internal --add-rich-rule=&apos;rule family=&quot;ipv4&quot; source address=&quot;192.168.0.92&quot; service name=&quot;ssh&quot; drop&apos; --permanent # 添加富文本规则firewall-cmd --reload 将ssh服务从zone中删除，在public中，仅允许192.168.0.100通过22端口访问 123firewall-cmd --zone=internal --remove-service=ssh --permanentfirewall-cmd --zone=public --add-rich-rule=&apos;rule family=ipv4 source address=192.168.0.100 port port=22 protocol=tcp accept&apos; --permanentfirewall-cmd --reload 将samba服务添加到internal，允许192.168.0.0/24中的用户访问,samba会使用tcp/139/445、udp/137/138端口 123456789firewall-cmd --zone=internal --set-target=default --permanentfirewall-cmd --zone=internal --add-interface=eth1 firewall-cmd --zone=internal \ --add-rich-rule=&apos;rule family=&quot;ipv4&quot; source address=&quot;192.168.0.0/24&quot; port port=139 protocol=tcp accept&apos; --permanentfirewall-cmd --zone=internal \ --add-rich-rule=&apos;rule family=&quot;ipv4&quot; source address=&quot;192.168.0.0/24&quot; port port=445 protocol=tcp accept&apos; --permanentfirewall-cmd --zone=internal \ --add-rich-rule=&apos;rule family=&quot;ipv4&quot; source address=&quot;192.168.0.0/24&quot; port port=137-138 protocol=udp accept&apos; --permanentfirewall-cmd --reload]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>firewall-cmd</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux主机iptables学习笔记]]></title>
    <url>%2F2019%2F01%2F12%2Fiptables-learning.html</url>
    <content type="text"><![CDATA[在Linux中使用和核心内建的封包过滤机制Netfilter,Netfilter提供了iptables这个软件作为防火墙封包过滤的指令。程序控管TCP Wrappers是另一种抵挡封包进入的方法，这种方法是通过服务器程序的外挂tcpd来处置的。与封包过滤不同的是，这种机制主要是分析谁对某程序进行存取，然后透过规则去分析该服务器程序谁能够联机、谁不能。TCP Wrappers是透过客户端想要链接的程序文件名，然后分析客户端的IP地址，看是否放行。封包过滤式防火墙主要是分析OSI七层协议中的2/3/4层，可以进行的分析工作有： 拒绝让Internet的封包进入主机的某些端口 拒绝让某些来源IP的封包进入 拒绝让某些带有特殊标记Flag的封包进入 分析硬件地址MAC来决定是否允许进入 TCP WrappersTCP Wrappers是透过/etc/hosts.allow /etc/hosts.deny这2个文件来管理的，但是并不是所有的软件都可以用来管控，目前可以管控的程序有： 由super daemon(xinetd)所管理的服务； 配置文件在/etc/xinetd.d里面的服务就是xinetd管控 系统中需要安装xinetd程序，使用chkconfig –list查看xinetd管控的程序 有支持libwrap.so模块的服务；规则执行的顺序是：先对比hosts.allow中的规则，再对比hosts.deny中的规则，如果2个文件中都不符合，则放行 iptables预设iptables至少有3个表格：filter、nat、mangle filter主要与进入主机的封包有关 INPUT与进入主机的封包有关 OUTPUT与主机所要发送的封包有关 FORWARD与传递封包到后段的主机中，与nattable相关 nat主要在进行来源目的ip或port的转换，主要与主机后的局域网内的主机相关 PREROUTING在进行路由判断之前所要进行的规则 POSTROUTING在进行路由判断后所要进行的规则 OUTPUT与发送的封包有关 mangle与特殊封包的路由标记有关 iptable语法规则查看iptables [-t tables] [-L] [-nv] or iptables-save [-t table],后者会列出完整的防火墙规则 -t 后接table，默认是filter -L 列出目前的table的规则 -n 不进行ip与hostname的反查 -v 显示详细信息 每个Chain代表1个链，括号内的policy就是预设政策 target 代表进行的动作，ACCEPT放行，REJECT拒绝，DROP丢弃 prot 代表使用的封包协议，主要有tcp\udp\icmp三种 opt 额外的选项说明 source 代表规则是针对哪个来源IP进行限制 destination 代表规则针对哪个目标IP进行限制 *开头的是表格 : 开头的是链 规则清除iptables [-t table] [-FXZ] -F 清除所有的已定的规则 -X 杀掉所有使用者自定义的chain -Z 将所有的chain的计数与流量统计归零 定义预设政策 在重新定义防火墙时，需要先将规则清除，操作最好是在本机执行；iptables [-t table] -P [INPUT,OUTPUT,FORWARD] [ACCEPT,DROP] -P 定义政策，注意大写 规则定义iptables [-AI 链名] [-io 网络接口] [-p 协议] [-s 来源ip] [--sport port][-d 目标ip] [--dport port] -j [ACCEPT|DROP|REJECT|LOG] -AI 链名针对某链进行规则的累加或插入，链名有3个:INPUT\OUTPUT\FORWARD -A 新增一条规则到原有规则的最后 -I 插入一条规则，如未指定顺序，默认为第一条 -io 网络接口，设定封包进出的接口规范 -i 封包进入的网络接口 -o 封包所传出的网络接口 -p 协议 主要的封包格式有:tcp\udp\icmp\all -s 来源ip，设定规则生效的来源，可以是单个ip，也可以是网段，如果是不允许某个ip或网段，则在地址前面加! –sport 限制来源的端口号，端口号可以是连续的n1:n2，如果使用了–sport或–dport必须要加-p参数，否则会出错 -d 目的ip或网段 –dport 限制到目的的端口号 -j 后面接动作，主要的动作有ACCEPT\DROP\REJECT\LOG 需要注意的是，网络接口lo需要设置为信任装置iptables -A INPUT -i lo -j ACCEPT 规则添加的顺序非常重要，例如我们利用iptables限制ssh的登陆iptables -A INPUT -i eth0 -s 192.168.0.100 -p tcp --dport 22 -j ACCEPTiptables -A INPUT i eth0 -p tcp --dport 22 -j REJECT iptables还支持syn的处理方式，一般来说，client启用的端口号都是大于1024的，服务端启用的端口号都是小于1023的，我们使用iptables将从1：1023发起的主动连接到本机的请求全部拒绝iptables -A INPUT -i eth0 -p tcp --sport 1:1023 --dport 1:1023 --syn -j REJECT 使用iptables禁用主机的ping响应iptables -A INPUT -i eht0 -p icmp --icmp-type 8 -j REJECT # 禁用所有的主机pingiptables -I INPUT -i eht0 -s 192.168.0.100 -p icmp --icmp-type 8 -j ACCEPT #允许ping的主机]]></content>
      <categories>
        <category>Linux</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux操作环境target与runlevel等级]]></title>
    <url>%2F2019%2F01%2F12%2Ftarget-runlevel.html</url>
    <content type="text"><![CDATA[在核心载入完毕、完成硬件侦测与驱动载入后，核心会主动调用第一个程序systemd。systemd的主要功能是准备软件执行的环境，包括系统的主机名称、网络设置、语系设置、文件系统及其他服务的启动。所有的动作都会通过systemd的默认启动服务集合/etc/systemd/system/default.target来规划。默认的操作环境default.target主要项目有：multi-user.target和graphical.target;使用命令systemctl get-default获取当前的运行级别；使用命令systemctl set-default [target] 设置系统的默认运行级别 runlevel与systemd的对应关系 System V systemd init 0 poweroff init 1 rescue init [234] multi-user.target init 5 graphical.target init 6 reboot 忘记root密码新版本的systemd，默认的rescue模式无法直接取得root权限，所以无法通过rescue模式重置root密码。可以通过rd.break核心参数来处理，该核心参数是Ram Disk里面的操作系统状态，不能直接取得原本的操作系统环境，还需要chroot的支持。 重置root密码流程 按下电源启动，进入开机画面后，选择开机菜单，按下e进入编辑模式,找到第一个linux16开头的内容，在末尾添加rd.break,ctrl+x执行开机； 进入Ram Disk环境，原本的系统被挂载到/sysroot目录下; 首先检查挂载点，找到原系统的挂载目录mount 将系统目录重新挂载为可读写mount -o remount,rw /sysroot 使用chroot切换到根目录chroot /sysroot 重置root密码passwd --stdin root 新建.autorelabel文件touch /.autorelabel 很重要的一步 因为修改了root用户的密码，文件/etc/shadown文件内容发生改变，所以这个文件的SELinux安全文本会被取消，如果没有让系统开机时自动的回复SELinux的安全文本，将会出现无法登陆的问题； 退出/sysroot exit 重启reboot]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>runlevel</tag>
        <tag>root密码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件搜寻指令Which/whereis/find]]></title>
    <url>%2F2019%2F01%2F12%2Fwhereis-which-find.html</url>
    <content type="text"><![CDATA[常用 在根目录下查找后缀格式为log的文件，并删除删除3天前的find / -name &quot;*.log&quot; -mtime +3 -exec rm -fr {} \; 在根目录下查找后缀格式为log的文件，并且3天内没有修改过find / -name &quot;*.log&quot; !-mtime -3 在当前目录下查找大于1M,后缀格式为log的文件，并将文件移动到/tmp/1M文件夹中find ./ -name &quot;*.log&quot; -size +1024k -exec mv {} /tmp/1M \; 查找当前目录下的空文件夹并删除find ./ -type d -empty -exec rm -fr {} \; 查找当前目录下的空文件(普通文件)并删除find ./ -type f -size 0c -exec rm -fr {} \;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>whereis</tag>
        <tag>which</tag>
        <tag>find</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell-script的学习和使用]]></title>
    <url>%2F2018%2F12%2F11%2Fshell-note.html</url>
    <content type="text"><![CDATA[shell-script的学习和使用注意事项 指令的执行是从上而下、从左到右; 指令、选项、参数间的多个空白都会被忽略; 空白行也会被忽略，Tab空白也视为空白行; 如果读取到一个Enter(CR)符号，就开始执行该行命令; 如果一行的内容太多，可以使用\Enter 来延伸到下一行; # 是注释符号，该符号后面的数据全部被视为注释文字; 脚本需要有rx权限才能被执行，脚本的执行可通过绝对路径、相对路径、变量PATH实现，除此之外，还可以通过bash程序来执行bash script.sh or sh script.sh; 主文件夹下的~/bin目录默认被设置到${PATH}中，也可以将脚本存放到~/bin下，直接输入script.sh执行脚本; 基础 脚本第一行声明脚本使用的shell名称，声明的方式是#!/bin/bash ; 声明的符号注意与注释符号#区分; 在脚本中强烈建议加注内容与功能、版本信息、作者与联络方式、创建日期、历史记录等信息 主环境变量的声明PATH=${PATH},并全局生效export PATH ; 执行结果可以使用变量$?来观察，我们可以使用exit指令让程序中断，并回传一个数值给系统，$?=0表示指令成功执行; 数值的运算格式$((x运算符)) 可使用的运算符有+ - * / % test指令 test指令常用的选项 -e 该文件是否存在 -f 该文件是否存在并且为文件 -d 该文件是否存在并且为目录 -z 判断字符串是否为0,若为空字符，则为true ！ 反状态 -rwx 文件权限的判断 -s 文件名是否存在并且为非空白文件 -a 条件and -o 条件or -eq 相等 -ne 不相等 -gt 大于 -lt 小于 -qe 大于等于 -le 小于等于 判断符号[] 除了使用test指令判断外，还可以使用符号[]来进行判断，使用符号判断，需要注意以下几点： 在符号内的每个元素都需要使用空白格分割; 在符号内的变量，最好都使用双引号括起来; 在符号内的常数，最好使用单引号或双引号括起来; 命令格式：[ &quot;变量1&quot; == &quot;变量2&quot; ]，多个条件的可以使用-a -o 连接 脚本的默认变量 $# 脚本的参数个数 $@ 脚本的全部参数 ${n} n代表数字，第几个参数 ${0} 代表脚本程序名 脚本变量的偏移使用命令shift ,在命令符后可接数字，代表偏移的数量 条件判断 if…then单层判断命令格式s 123if [条件判断]; then commandfi if…then多层判断命令格式 1234567if [条件判断1]; thencommand1elif [条件判断1]; thencommand2elsecommand3fi case ..esac判断 用于多个既定变量内容的判断，命令格式：1234567891011case $&#123;变量&#125; in&quot;内容1&quot;)command1;;&quot;内容2&quot;)command2;;*)command3;;esac function功能 使用function可以创建函数，创建的函数需要在脚本的最前面，函数的格式：123function fname ()&#123; command&#125; 循环loop功能 不定循环while..do..done当条件成立时，执行循环，当条件不成立时，停止循环。命令格式： 1234while [ 条件 ] # 中括号内是判断式do # 开始循环 commanddone # 结束循环 不定循环until..do..done当条件成立时，终止循环，否则就持续进入循环。命令格式： 1234until [ 条件 ]do commanddone 固定循环for..do..done,已经知道循环几次。命令格式： 1234for var in con1 con2 con3do commanddone 固定循环for的另一种格式： 1234for ( ( 初始值; 限制值; 执行步阶 ) )do commanddone 使用循环和function功能测试局域网内存活的主机ip地址 sh的debug 命令格式sh [-nvx] shell.sh -n 不执行脚本，仅检查语法 -v 在执行脚本前，先将脚本的内容输出到屏幕中 -x 将使用到的脚本内容显示到屏幕上]]></content>
      <categories>
        <category>Shell</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>Shell</tag>
        <tag>脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[变量内容的删除、取代与替换]]></title>
    <url>%2F2018%2F12%2F09%2Fshell-del-replace.html</url>
    <content type="text"><![CDATA[从前往后删除 变量的删除使用符号#和##,区别是：前者代表删除最短，后者代表删除最长的; 使用的格式是${变量#/xxxx}or ${变量##/xxxx} ，删除从最前面开始向右删除 示例，变量test_path的内容与PATH相同，分别删除最短的一个路径和删除最长的一个路径(仅剩下一个最短的) 从后往前删除 从后往前删除变量内容，使用符号%和%%,区别是：前者删除最短，后者删除最长; 命令格式${变量%xxxx} or ${变量%xxx} 示例 变量内容的取代 取代变量内容使用符号/xxx/,命令格式${变量/原内容/取代内容}，取代第一个匹配项 若取代变量内容中全部匹配向，使用符号//xxx/,命令格式${变量//原内容/取代内容} 变量内容的替换 有时候我们需要判断变量内容是否存在，若存在就使用存在值，若不存在，则使用默认值替换; 命令格式${变量-默认值}，如果变量是空字符，则不是用默认值，输出空字符; 命令格式${变量:-default}，使用了符号:表示若变量内容为空或未设置，则使用default值 变量内容替换的更多使用方法，可以参考下图：]]></content>
      <categories>
        <category>Shell</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>变量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sed-awk-printf-diff-cmp工具的基础使用]]></title>
    <url>%2F2018%2F12%2F07%2Fsed-awk-printf-diff-cmp.html</url>
    <content type="text"><![CDATA[sed工具 sed是一个管线命令，可以分析标准输入，而且可以将数据取代、删除、新增、撷取特定行; sed工具的命令格式：sed [-nefre] &#39;[n1,n2][command]&#39; -n : 使用安静模式silent,在sed的一般应用中，所有来自标准输入的数据都会列到屏幕中，如果使用了安静模式，只有经过sed处理的那一行数据才会被输出到屏幕中; -e : 直接在命令行界面上进行sed的动作编辑; -f : 直接将sed的动作写在一个文件内，使用该参数指定文件路径名称; -r : sed动作支持的是延伸型正则表达式，不使用参数，默认使用基础正则表达式语法; -i ： 直接修改读取的文件内容，而不是由屏幕输出 n1,n2 : 选择进行动作的行数，连同后接的命令需要用单引号括住 command中可以使用的命令有： a 新增到下一行，后接字符串,新增多行的，需要在每行最后添加换行符号\在最后一行后新增内容sed -i &#39;$a new add content&#39; filename c 取代n1,n2之间的行，后接字符串,ex: nl ~/test | sed ‘2,5c This is a replace contend’`使用This is a replace contend取代2-5行的全部内容 d 删除,删除空白行sed &#39;/^$/d&#39; i 插入到上一行，后接字符串 p 打印输出,可以用于取出特定行的内容,ex: nl /etc/shadow | sed -n &#39;2,5p&#39; s 取代，直接进行取代的工作，格式s/要取代的字符/新字符/g 使用sed工具取出eth0的ip地址ip addr | grep &#39;eth0$&#39; | sed ‘.inet.//g’ | sed ‘s/\24.$//g’ printf格式化打印 命令格式printf &#39;format&#39; content 打印格式 \a 警告声音输出 \b 倒退键backspace \f 清除屏幕form feed \n 输出新行 \r Enter键 \t 水平Tab键 \v 垂直Tab键 \xnn 将2位数字nn转换为字符 %ns n个字符 %ni n个整数 %N.nf 浮点数全长N为，其中n个小数位，1位小数点，整数位N-n-1 awk工具 awk工具将一行中的数据分成多段进行处理，默认的分段分割符是空白格或Tab，以行为1次处理单位，以字段为最小的处理单位，命令格式awk &#39;option1{command} option2{command}&#39; filename awk的变量 $n n代表数字，表示一行中的第n个字段，如果为0表示整行 NF 内置变量，每行拥有的字段总数 NR 内置变量，当前处理行 FS 内置变量，目前的分割字符，默认是空白格 获取last的前5行数据的第一个字段名称，并输出当前行和当前行的字段总和last -n 5 | sed &#39;6,7d&#39; | awk &#39;{prinft $1 &quot;\t linnum:&quot; NR &quot;\t fnum:&quot; NF &quot;\t&quot;}&#39; awk逻辑运算1234561. &gt; # 大于2. &lt; # 小于3. &gt;= # 大于等于4. &lt;= # 小于等于5. == # 等于6. != # 不等于 文件比对工具 diff diff工具常用来比对2个文件之间的差异，并且是以行为单位的，一般是用在ASCII纯文本文件的对比，除了比对文件外，diff还可以用来比对目录之间的差异。命令格式diff [-bBi] from-file to-file -b 忽略一行中，仅有多个空白的差异 -B 忽略空白行的差异 -i 忽略大小写差异 cmp cmp以字节为单位比对，命令格式：cmp [-l] file1 file2 -l 将所有的不同点的字节都列出，默认只输出第一个发现的不同点]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Shell</tag>
        <tag>sed</tag>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell万用字符与特殊符号]]></title>
    <url>%2F2018%2F12%2F07%2Flinux-shell-regalur.html</url>
    <content type="text"><![CDATA[bash常用 * : 代表0到无穷个任意字符 ? : 代表一定有一个任意字符 []: 代表一定有一个括号内的字符 [x-x] : 代表有一个x到x范围内的字符 [^] : 代表反向，没有括号内的字符 正则表达常用 ^word : 以word开头 word$ : 以word结尾 . : 任意一个字符，可以是空白格,一定有一个 \ : 转义字符 * : 重复0到无穷个前一个字符，任意字符的表示.*,注意与bash中的区分 [] : 集合内的一个字符 [x-x] : x-x范围内的一个字符 [^] : 没有范围内的字符 x\{n1,n2\} : 连续n1或n2个x字符,使用符号\将符号{}进行转义]]></content>
      <categories>
        <category>Shell</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bash进站与欢迎信息]]></title>
    <url>%2F2018%2F12%2F06%2Fbash-login-welcome.html</url>
    <content type="text"><![CDATA[bash的进站与欢迎信息，通过文件/etc/issue和/etc/motd2个文件实现 CentOS默认的进站信息如下：12\SKernel \r on an \m issue文件中，使用符号\调用变量，各个变量的内容如下： \d 本地端时间 \l 显示第几个终端机接口 \m 现实硬件的等级 \n 显示主机的网络名称 \O 显示domain name \r 显示操作系统的版本–&gt; uname -r \t 显示本地端时间 \S 显示操作系统的名称 \v 操作系统的版本 终端登陆CentOS显示的bash进站信息 通过telnet方式登陆主机时，bash的进站的信息通过文件/etc/issue.net修改 用户通过bash登陆后会有欢迎信息，该信息通过文件/etc/motd修改]]></content>
      <categories>
        <category>bash</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统资源观察]]></title>
    <url>%2F2018%2F12%2F03%2Flinux-sys-cmd.html</url>
    <content type="text"><![CDATA[内存观察 使用命令free可以观察内存的使用情况，命令格式如下： free [-bmgh] [-t] [-sNc] b 直接输入free命令时，显示的单位是K，可以使用的单位有b(bit),m(M),k(KB),g(G),除此之外还可以使用-h让系统自动指定单位 t 在输出最终结果的时候，显示实体内存与swap总量 s 可以让系统以每几秒输出一次，不间断输出 c 通常与参数s一起使用，表示连续输出多少次 网络观察 使用命令netstat可以观察系统和程序的网络信息，命令格式如下： netstat -[atunlp] a 使用inactive/active取代buffer/cache t 列出tcp数据 u 列出udp命令 n 列出程序的段口号 l 列出目前正在 监听的服务 p 列出网络程序的pid 开机信息的观察 使用dmesg命令可以查看，系统开机时的信息，例如查看开机时cpu信息： dmesg | grep cpu 系统资源观察 使用命令vmstat可以观察系统资源的运行情况，命令格式如下： vmstat [-a] [延迟 [总计侦次数]] # cpu/内存信息 使用inactive/active活跃与否取代baffer/cache的内存输出信息 vmstat -fs # 内存相关 f 开机到现在,系统复制fork的程序数 s 开机后的哦呵之内存变化的情况列表 S 指定显示数据的单位，支持k/K m/M vmstat -d # 磁盘信息 vmstat -p # 分区信息 vmstat -a 1 3 # 每秒输出1次，共输出3次 字段 procs 程序字段： r表示等待运行中的程序数量；b表示不可被唤醒的程序数量。这2个项目越多，代表系统越繁忙 memory 内存字段： swpd虚拟内存被使用的容量；free未被使用的内存容量；buffer用于缓冲内存；cache用于高速缓存内存 swap内存交换：si由磁盘中将程序取出的量；so由于内存不足而将没用到的程序写入到磁盘的swap的容量。如果si/so数值太大，表示内存的数据常常得在磁盘与内存之间传递，性能会变差 system系统项目：in每秒被中断的程序次数;cs每秒进行的事件切换次数；in/cs数值大，达标系统与周边设备的沟通非常频繁，周边设备包括磁盘、网卡、时间钟等 CPU项目：us非核心层的cpu使用状态；sy和下层使用的cpu状态；id闲置的状态；wa等待IO所耗费的cpu状态;st被虚拟机盗用的cpu]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>Linux</tag>
        <tag>命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件磁盘阵列在xfs文件系统中的使用]]></title>
    <url>%2F2018%2F11%2F27%2Fraid-with-xfs.html</url>
    <content type="text"><![CDATA[概述 磁盘阵列由硬件磁盘阵列(Raid卡)和软件磁盘阵列，RAID可以通过技术，将多个较小的磁盘合成一个较大的磁盘设备;这个磁盘设备除了具有存储功能，还有数据保护功能; 根据RAID等级的不同，常见的可以分为RAID0、RAID1、RAID1+0/0+1、RAID5、RAID6，最为常用的是RAID1+0 RAID-0等量模式RAID-0等量模式(stripe)性能最佳，数据损坏风险高 这种模式是使用相同型号与容量的磁盘来组成时，效果较佳; 工作的原理是：磁盘先切除等量的区块(chunk,一般4k-1M)，当文件要写入RAID时，文件会依据chunk的大小切割好，然后依序放到各个磁盘中; 每个磁盘会交错的存放数据，文件被写入RAID时，数据会等量的放置在各个磁盘上;因为数据已经被切割并且放置在不同的磁盘中，因此每个磁盘负责的数据量都降低了; 使用RAID0存储数据，整个可用的磁盘空间受最小的一颗磁盘限制，当容量最小的一颗磁盘已经写满，那么其他磁盘也不再写入数据;同样的，如果其中一颗磁盘损坏，导致数据丢失了一块，那么存放在RAID中的文件将不能读取 RAID-1映射模式RAID-1映射模式(mirror)完整备份，数据损坏风险低，磁盘可用容量低 RAID-1模式最好使用一模一样的磁盘，磁盘的可用容量由最小的一颗决定; 工作的原理是：一份数据传送到RAID-1后，在I/O总线后被复制多份到各个磁盘，每个磁盘都存有同一份文件，整个磁盘的可用容量只有50%;由于每个磁盘内都有一个文件的副本，任何一颗磁盘损坏时，数据还是可以完整的保存下来; 使用软件磁盘阵列时，在大量写入的情况下，RAID-1的写入性能会变得很差; RAID 1+0/0+1模式RAID 1+0/0+1模式是RAID-0和RAID-1的组合模式，RAID 1+0就是先让2颗硬盘组成RAID-1模式，这样的设置有2组，再将这2组RAID-1组成一组RAID-0;RAID 0+1就是先组成RAID-0,再组成RAID-1模式。 RAID 5/6RAID5需要至少3颗磁盘，类似于RAID0性能和数据备份的均衡考虑，在每次循环写入的过程中,在每颗磁盘还加入一个同位检查数据，这个数据会记录其他磁盘的备份数据，用于当磁盘损坏时的救援; 使用RAID5时，每次循环写入时，都会有部分的同位检查码被记录，并且记录的同位检查码每次都记录在不同的磁盘中，因此，任何一个磁盘损坏时都能由其他磁盘的检查码来重建原本磁盘内的数据; RAID5的总可用磁盘数量是总磁盘数量减1，默认仅能支持1颗磁盘的损坏情况，当由2颗以上的磁盘出现损坏时，RAID5的数据就损毁了;RAID6是在RAID5的基础上使用2颗磁盘的容量作为检查码的存储，因此整体磁盘就少2颗，允许出错的磁盘数量就可以达到2颗 预备磁盘 当磁盘阵列的磁盘损坏时，需要将坏掉的磁盘拔掉，换一颗新的磁盘。更换新的磁盘后并启动磁盘阵列，磁盘阵列会主动开始重建原本坏掉的那颗磁盘上的数据到新的磁盘上。这个过程如果系统支持热插拔，可以直接在线更换，如果不支持需要关机后操作。 为了让系统可以实时在硬盘损坏时主动重建，就需要预备磁盘的辅助。所谓预备磁盘就是再1颗或多颗没有包含再原本磁盘阵列等级中的磁盘，平时这颗磁盘不会被磁盘阵列所用，当磁盘阵列中由磁盘损坏时，这颗预备磁盘会主动的进入磁盘阵列，并将坏掉的磁盘移除，然后重建数据。 RAID对比 软件磁盘阵列 软件磁盘阵列是使用软件的方式，仿真模拟磁盘阵列的功能，这种方式会损耗系统资源。在CentOS中较为常用的软件磁盘阵列工具是mdadm,该工具以partition或disk为磁盘单位，不需要2颗以上的磁盘，只需要2个以上的分区就可以设计出磁盘阵列。 设置 利用4个partition组成RAID5,每个partition为1G，设置1个partition为预备磁盘，chunk设置为256k(一般为64k或512k)，将RAID5设备挂载到/srv/raid上; mdadm命令格式mdadm --create /dev/md[0-9] --auto=yes --level=[015] --chunk=nK --raid-devices=n --spare-devices=n /dev/sd{n1,n2,n3} ‘create’ 创建RAID auto=yes 决定创建后面界的软件磁盘阵列设备 chunk=nK chunk大小，一般64K、512K raid-devices=n` 使用几个磁盘作为软件磁盘阵列设备 spare-devices=n 预备磁盘的数量 level[015] 磁盘阵列的等级，常用的是0,1,5 detail 显示设备的磁盘阵列详细信息 利用mdadm创建磁盘阵列 mdam --create /dev/md0 --auto=yes --level=5 --chunk=256K --raid-devices=4 --spare-devices=1 /dev/vda{5,6,7,8,9} 查看创建的raid设备信息 mdadm --detail /dev/md0 还可以使用命令cat /proc/mdstat 格式化与挂载RAID mkfs.xfs -f -d su=256k,sw=3 -r extsize=768k /dev/md0# su指定chunk的大小，与创建raid时相同，sw指定可用磁盘容量个数(raid共4个，可用的是4-1)，extsize的计算是256k*3 mount /dev/md0 /srv/raid df -Th /srv/raid RAID的救援 RAID管理常用的命令mdadm --manage /dev/md[0-9] [--add devices] [--remove devies] [-fail devices] add 将设备加入到md中 remove 将设备从md中移除 fail 将md中的设备设置为出错状态 写入数据到raid设备md0中 cp ~/test/* /srv/raid 将磁盘设置为错误状态 mdadm --detail/dev/md0 # 查看当前raid设备的信息 目前使用的磁盘是5,6,7,8 ，9号磁盘作为预备磁盘，我们将7号磁盘设置为错误 mdadm --manage /dev/md0 --fail /dev/vda7设置后再查看raid设备的信息，9号预备磁盘已经替换7号磁盘，并重建数据中 将出错的磁盘移除并加入新的磁盘 将7号磁盘从raid设备中移除 mdadm --manage /dev/md0 --remove /dev/vda7 系统关机，将出错的磁盘拔除，并安装新的磁盘替换7号磁盘 将新加入的磁盘加入到raid设备中 mdadm --manage /dev/md0 --add /dev/vda7 RAID关闭 获取raid设备的uuid mdadm --detail /dev/md0 | grep -i uuid,从文件夹中卸载设备umount /srv/raid 编辑/etc/fstab文件，将以uuid开头的raid设备相关的注释掉,没有可忽略 使用dd命令分别将raid设备中的数据覆盖dd if=/dev/zero of=/dev/md0 bs=1M count=63 关闭raid设备mdadm --stop /dev/md0 使用dd命令将磁盘中的数据覆盖或使用fdisk格式化磁盘]]></content>
      <categories>
        <category>XFS</category>
      </categories>
      <tags>
        <tag>RAID</tag>
        <tag>磁盘阵列</tag>
        <tag>存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XFS文件系统中quota的使用]]></title>
    <url>%2F2018%2F11%2F27%2Fquota-with-xfs.html</url>
    <content type="text"><![CDATA[概述 在xfs文件系统中，使用quota可以针对用户、群组、文件夹进行磁盘限额; 在限额的操作中，使用最多的命令就是xfs_quota -x -c ; quota的限制是针对文件系统的，跨文件系统是无法实现quota的; quota的使用应该尽量避免在根目录下使用，如果前期为规划文件系统，后期需要对某个目录进行限额，可以将原目录完整的移动到/home下，然后利用ln -s /home/dir /old/dir创建一个软连接，在/home下对文件夹进行限额 操作 文件系统 xfs文件系统支持quota核心功能，在使用quota限额时，避免在根目录下使用 查看home分区的文件系统信息df -hT /home 修改ftab-xfs文件系统的quota在挂载时就已经宣告，无法使用remount来重新启动quota功能，因此要写入fstab中，或者在初始故在过程中加入这个项目，否则不会生效; 修改fstab文件cp /etc/fstab /etc/fstab.old &amp;&amp; vi /etc/fstab,找到home分区，在第四段内容后添加usrquota,grpquota2项内容;需要注意的是：在修改完成后需要测试，若存在错误需要立刻处理，否则不能开机;在卸载分区前，需要将一般账号全部退出，否则不能卸载，who查看当前在线的用户，pkill -kill -t pts/n命令强制用户离线。 - 针对quota限额的使用有3项： 1. usrquota:针对使用者账号 2. grpquota:针对群组 3. prjquota:针对单一目录，但是**不能与grpquota同时存在** 重新挂载分区 umount /home &amp;&amp; mount -a mount | grep home # 查看home分区的挂载信息 使用xfs_quota命令查看quota报告 xfs_quota命令格式xfs_quota -x -c &quot;comm&quot; [mount_dir] -x专家模式，只有使用了该参数，才能使用-c指定命令 -c指定命令 print 列出目前主机内的文件系统参数等数据 df 与系统的df命令一样 report 列出目前的quota项目，有ugr(user/group/project)及bi等数据 state 说明目前支持quota的文件系统的信息 限额的设置 限额用户和用户组 设定系统中test用户的限额为200M/300M,群组共500M/600M的容量，同时grace time为7天 限额的命令格式xfs_quota -x -c &quot;limit [-ug] b[soft|hard]=N i[soft|hard]=N name&quot; xfs_quota -x -c &quot;timer [-ug] [-bir] Ndays&quot; limit 指定限定的项目，可以针对user和group限制 bsoft|bhard isoft|ihard block(磁盘容量)和inode(文件数量)的hard与soft值 通常hard要比soft限额高，hard表示使用者的用量绝对不会超过这个限额;soft表示在使用者低于限额可以正常使用，若高于soft低于hard，每次登陆系统时，系统会主动发送磁盘即将爆满的警告，并且会有一个宽限时间grace time，如果在grace time时间内不进行任何磁盘关联，soft会取代hard值，达到hard值后，磁盘使用权将会被锁住无法新增文件 xfs_quota -x -c &quot;limit -u bsoft=200M bhard=300M test&quot; /home # 限定用户test xfs_quota -x -c &quot;limit -g bsoft=500M bhard=600M test&quot; /home # 限定用户组test xfs_quota -x -c &quot;timer -u -b 7days&quot; # 设定用户宽限时间为7天 xfs_quota -x -c &quot;timer -u -b 7days&quot; # 设定用户组宽限时间为7天 分别测试bsoft|bhard|grace time，使用dd命令在用户testhome下写入220M数据，查看quota报告 限额目录 在使用限额目录功能前，需要取消group的限额，取消grpquota加入prjquota 限定用户test家目录下web文件夹的限额400M/500M 首先取消grpquota,添加prjquota vi /etc/fstab 重载分区 umount /home &amp;&amp; mount -a mount | grep /home # 查看home分区的挂载信息 xfa_quota -x -c &quot;state&quot; /home # 查看quota状态，grpquota已经关闭 限额目录需要指定项目名称和识别码，项目名称webquota,识别码26 指定项目识别码与对应目录echo &quot;26:/home/test/web&quot; &gt;&gt; /etc/projects 指定项目识别码与名称 echo &quot;webquota:26&quot; &gt;&gt; /etc/projid 初始化项目名称 xfs_quota -x -c &quot;project -s webquota&quot; xfs_quota -x -c &quot;print&quot; /home xfs_quota -x -c &quot;report -pbih&quot; /home 设置项目限额 xfs_quota -x -c &quot;limit -p bsoft=400M bhard=500M webquota&quot; /home xfs_quota -x -c &quot;report -pbih&quot; /home 在文件夹web下写入450M的数据，测试quota的project dd if=/dev/zero of=./project.img bs=1M count=450 如果需要限定其他目录的磁盘使用，只需要创建一个项目并将该项目初始化，再执行项目限额的操作即可 xfs_quota的管理常用的管理命令 disable 暂时取消quota的限制，系统还在计算quota，只是没有管制; enabled 恢复到正常的管理状态，在使用了disable后可以使用该命令恢复到正常的状态; off 完全关闭quota的限制，使用了该命令后只有重载文件系统，才能重新启用quota; remove 杂子off状态才可以使用该命令，如果要取消webquota项目的限制,可以直接使用remove -p,需要注意的是，使用该命令会移除所有项目的限制]]></content>
      <categories>
        <category>XFS</category>
      </categories>
      <tags>
        <tag>XFS</tag>
        <tag>quota</tag>
        <tag>磁盘限额</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LVM-thin-Volume学习和基础使用]]></title>
    <url>%2F2018%2F11%2F24%2Flvm-thin-pool.html</url>
    <content type="text"><![CDATA[thin Volume 导图 操作 创建thin pool设备 lvcreate -L 2G -T centos/thinpool# 在centos卷组中创建一个实际存储空间为2G，设备名为thinpool 查看设备使用信息 lvs centos 创建1个虚拟空间为4G的设备 lvcreate -V 4G - T centos/thinpool -n thin-tmp 创建文件设备 mkfs.xfs /dev/centos/thin-tmp 挂载到系统 mount /dev/centos/thin-pool /tmp 向新建的设备中分别写入500M、1G的数据 dd if=/dev/zore of=/tmp/test.img bs=1M count=500]]></content>
      <categories>
        <category>LVM</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>LVM</tag>
        <tag>thinpool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LVM基础学习和使用]]></title>
    <url>%2F2018%2F11%2F23%2Flvm-doc.html</url>
    <content type="text"><![CDATA[LVM,PV,PE,VG,LV的学习和基础操作LVM LVM全名是Logical Volume Manager逻辑卷管理，LVM的作法是将几个实体的partitions或disk通过软件组合成为一块看起来是独立的大磁盘(VG),然后将这会大磁盘再经过分区成为可以使用的分区(LV),最终就可以使用了。LVM可以创建和u管理逻辑卷，而不是直接使用物理硬盘，它可以在不损坏已存储的数据的情况下弹性管理逻辑卷的大小，不需要重启系统就可以让系统内核知道分区的存在。 PV PV全名是Physical Volume物理卷，我们实际的partitions或disk实体磁盘，需要调整系统识别码system ID成为8e LVM识别码，然后再经过pvceate的指令将他转成LVM最底层的物理卷PV，之后才能将这些PV加以使用。 调整系统识别码system ID的方法就是通过gdisk 常用命令 pvcreate 将paritition创建为PV pvscan 搜寻系统中的PV pvdisplay 显示当前系统中PV信息 pvremove 移除PV VG VG全名是Volume Group卷组，它由多个PV组成，类似于非LVM系统中的物理硬盘。 常用命令 vgcreate 创建VG vgscan 搜寻系统中的VG vgdisplay 显示当前系统中VG信息 vgremove 移除VG vgextend 在VG附加新的PV vgreduce 从VG中移除PV vgchange 设置VG是否启动 PE PE全名是Physical Extent物理块，每个PV被划分成为PE的基本单元，具有唯一编号的PE是可以被LVM寻址的最小单元。PE的大小是可以被配置的，默认为4M，PV的大小由同等的基本单元PE组成。 LV LV全名是Logical Volume逻辑卷，最终的VG会被划分为LV，LV的大小与在此LV内的PE总数有关。 常用命令 lvcreate 创建LV lvscan 搜寻系统中的LV lvdisplay 显示当前系统中LV信息 lvremove 移除LV lvextend 增加LV容量 lvreduce 从LV中减少容量 lvresize 调整LV容量的大小 LVM 操作 在虚拟机中新增了一个5G的硬盘，使用fdisk将硬盘划分2个分区，1个2G，1个3G 系统中原有VG centos和分区/boot，VG分配了3个LV分别是/dev/centos/root /dev/centos/swap /dev/centos/home 新建PV pvcreate /dev/vdb{1,2} 查看PV信息 pvscan 添加VG 分别将/dev/sdb1,2添加到当前的VG vgextend centos /dev/vdb1 vgextend centos /dev/vdb2 如果需要新建VG则使用以下命令,-s表示PE大小 vgcreate -s 4M centos1 /dev/vdb{1,2} 添加LV 新建1个LV，用于/tpm，将/home扩容 1 lvcreate -L 2G -n tmp centos# -L后接容量，单位可以是MGT，最小单位是PE，后面必须是PE的倍数，若不符，系统自动计算相近的容量,i指定PE的个数 lvextend -L +1G /dev/centos/home# 在参数-L后接的数字，如果在前面加+表示增加了多少，如果不添加符号表示增加到多少 xfs_growfs /home# home扩容1G 增大LV容量的可以直接使用命令lvresize -L +1G /dev/centos/home 如需减少将符号+替换为-,最后使用命令xfs_growfs在线扩容 注意：在xfs文件系统中，只能增加lv的大小，不能减小，在ext4文件系统中则可以 文件系统 格式化为xfsmkfs.xfs /dev/centos/tmp 挂载mount /dev/centos/tmp /tmp LVM快照 快照就是将当时的系统信息记录下来，未来若是有任何数据变更，则原始数据会被搬移到快照区，没有被变更的区域则由快照区与文件系统共享。 需要注意的是快照区与被快照的设备需要在同一个VG中 目前系统中还有509个PE可分配空间，分配150个PE用于快照/dev/centos/root设备，快照名称root-snap 快照操作 lvcreate -s -l 150 -n root-snap /dev/centos/root# -s创建快照设备，-n 快照设别名，/dev/centos/root要快照的设备 查看创建的快照设备，LV Size与要快照的设备相同 查看快照设备内的信息，与被快照的设别是一样的mount -o nouuid /dev/centos/root-snap /mnt # xfs文件系统中，不允许相同的uuid文件系统挂载 快照复原 利用快照区复原系统，需要注意的是要复原的数据量不能高于快照区所有负载的实际容量，在复原时，设备内的原始数据会被搬移到快照区，如果快照区不够大，若原始数据被变更的实际数据量比快照大，那么快照功能将会失效。 分别创建一个1G的data LV和一个1G的data-snap 快照LVlvcreate -L 1G -n data centos 格式化,挂载,创建快照mkfs.xfs /dev/centos/data &amp;&amp; mount /dev/centos/data /datalvcreate -s -L 1G -n data-snap /dev/centos/datamount /dev/centos/data-snap /mnt 还原操作 利用快照将原来的文件系统备份2xfsdump -l 0 -L lvm2 -M lvm2 -f /home/user/data.dump /mnt# xfsdump备份级别为0,所有文件全部备份 -f参数指定备份的dump文件位置， /mnt为要备份的文件为什么不直接格式化data分区，然后直接恢复快照呢？如果直接格式化，原本的文件系统所有数据都会被搬移到快照区内，快照区容量不够大，那么部分数据将无法复制到快照区呢，执行快照复原的时候，就会有数据不能还原，所以需要先备份原数据 卸载并移除快照区mount /mnt &amp;&amp; lvremove /dev/centos/data-snap 卸载复原分区umount /data 格式化复原分区mkfs.xfs -f /dev/centos/data 重新挂载复原分区mount /dev/centos/data /data 使用xfsrestore复原分区xfsrestore -f /home/user/data.dump -L lvm2 /data 参考 http://tldp.org/HOWTO/LVM-HOWTO/extendlv.html https://www.systutorials.com/docs/linux/man/8-xfsdump/]]></content>
      <categories>
        <category>LVM</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>LVM</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用HAproxy-Keepalived-MHA搭建高可以用Mysql服务集群]]></title>
    <url>%2F2018%2F11%2F22%2Fmha-keepalived-hapoxy-mysql.html</url>
    <content type="text"><![CDATA[环境 Mysql 使用Mysql5.7.*版本，在Slave和Master集群中安装 HAproxy 使用HAproxy1.8版本，在CentOS环境中安装，使用单独服务器，以提高性能 Keepalived 使用Keepalived1.3.5版本，在Master和Backup服务器上安装，使用漂移VIP -MHA 使用mha-mangaer-0.55版本，Manager单独安装服务器，Node安装在Master和Back中 操作安装配置 安装Mysql数据库 分别在slave01-03服务器和master\backup服务器中安装mysql57-community-server 安装步骤：导入mysql repo源，使用yum-config-manager禁用mysql80源,启用mysql57源，然后使用yum install mysql-community-server安装 配置数据库主从复制 master和slave使用异步主从复制，并在slave服务器中设置read_only=1 和 relay_log_opurge=0 master和backup使用半同步主从复制，会用到插件rpl_semi_sync_master/slave 在master数据库中授权用户通过服务器haproxy访问的权限 参考文章: Mysql数据库配置异步同步主从复制 Mysql数据库配置异步同步主从复制 安装配置keepalived 分别在master和backup中安装keepalived，设置漂移VIP地址为192.168.0.110，master和backup的级别、路由id、认证密码相同，state都设置为BACKUP,启用非抢夺模式 首先启动master服务器中的keepalived，再启动backup 参考文章： Mysql高可用架构之keepalived and MHA 安装配置MHA 分别在master，backup和slave中安装MHA-Node 在mha服务器中安装MHA-Manager，设置配置文件，修改故障转移脚本 在启用mha之前，使用命令masterha_check_ssh --conf=/etc/masterha/app1.cnf检查ssh信任，使用命令masterha_check_repl --conf=/etc/masterha/app1.cnf检查mysql的主从同步 mha启用后，使用命令masterha_check_status --conf=/etc/masterha/app1.cnf检查mha的状态 以daemon的方式启动MHA，使用命令行nohup masterha_manager --conf=/etc/masterha/app1.cnf --remove_dead_master_conf --ignore_last_failover &lt; /dev/null &gt; /var/log/masterha/app1/app1.log 2&gt;&amp;1 &amp; 安装配置haproxy 在haproxy服务器上安装haproxy，修改配置文件，启用状态检测 通过端口分离读写，3306读，3307写 在启用haproxy之前，可以使用命令haproxy -f /etc/haproxy/haproxy.cfg检查haproxy的配置是否正确 参考文章： HAProxy基础学习笔记 启用 启用Mysql数据库，检查主从同步复制 启用master中的keepalived，启用backup中的keepalived 启用haproxy，检查haproxy的统计信息 启用MHA-Manager，检查其状态 读写简单测试 在本地通过client连接Mysql数据库，使用3307段口号 mysql -u root -p -h haproxy -P 3307 查看Mysql的server_id show global variables like &#39;%server_id%&#39;; 在本地通过client连接Mysql数据库，使用3306段口号 mysql -u root -p -h haproxy -P 3307 查看Mysql的server_id 多次执行show global variables like &#39;%server_id%&#39;;查看每次获取的server-id]]></content>
      <categories>
        <category>Mysql</category>
        <category>高可用</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>keepalived</tag>
        <tag>高可用</tag>
        <tag>mha</tag>
        <tag>haproxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MHA在线主从切换]]></title>
    <url>%2F2018%2F11%2F20%2Fmha-m2s.html</url>
    <content type="text"><![CDATA[切换过程 检测复制设置和确定当前的主服务器; 确定新的主服务器; 阻塞写入到当前的主服务器; 等待所有从服务器同步完成; 授予写入到新的主服务器 重新设置从服务器 操作过程 把主服务器从centos2切换到centos 停止MHA监控masterha_stop --conf=/etc/masterha/app1.cnf 将centos服务器加入到app1.cnf配置文件中 12[server3]hostname=centos 在centos中配置主从复制，将数据与centos2中的数据保持一致 删除文件rm -f /var/log/masterha/app1/mha.failover.complete saved_master_binlog_from_*.binlog 在线切换 1234masterha_master_switch --conf=/etc/masterha/app1.cnf \--master_state=alive \--new_master_host=centos \--orig_master_is_new_slave --running_updates_limit=10000 首次切换时，会有一个错误提示,需要将master_ip_online_change152行中的FIXME_xxx_drop_app_user($orig_master_handler);注释掉Got Error: Undefined subroutine &amp;main::FIXME_xxx_drop_app_user called at /usr/local/bin/master_ip_online_change line 152. ![error-fixme](https://s1.ax1x.com/2018/11/20/F9PEcV.png) 检查服务器的Mysql状态 centos的slave hosts centos2的slave status]]></content>
      <categories>
        <category>MHA</category>
      </categories>
      <tags>
        <tag>MHA</tag>
        <tag>故障转移</tag>
        <tag>主从切换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql高可用架构之keepalived and MHA]]></title>
    <url>%2F2018%2F11%2F20%2Fmysql-keepalived-mha.html</url>
    <content type="text"><![CDATA[MHA介绍 MHA目前在mysql高可用方面是一个相对成熟的解决方案，它由日本DeNA公司youshimaton开发，是一套优秀的作为MySQL高可用性环境下故障切换和主从提升的高可用软件。在MySQL故障切换过程中，MHA能做到在0~30秒之内自动完成数据库的故障切换操作，并且在进行故障切换的过程中，MHA能在最大程度上保证数据的一致性，以达到真正意义上的高可用。 MHA由2部分组成：MHA Manager、MHA Node MHA Manager是单独部署在一台独立的机器上用来管理多个master-slave集群，也可以部署在一个slave结点上。MHA Manager定期探测集群中的master结点，当master出现故障，它可以自动将最新数据的slave提升为新的master，然后将其他slave重新指向新的master; MHA Manager工具 masterha_check_ssh 检查MHA的ssh配置状况 masterha_chck_repl 检查mysql复制状况 masterha_manger 启动MHA masterha_chck_status 检查当前MHA运行状态 masterha_master_monitor 检查master是否宕机 masterha_master_switch 控制故障转移(自动or手动) masterha_conf_hosot 添加或删除配置的server信息 MHA Node运行在每台Mysql服务器上; MHA支持一主多从，要求至少3台数据服务器，1台master，1台备用master，1台从; MHA Node工具 save_binary_logs 保存和复制master的二进制日志 apply_diff_relay_log 识别差异的中继日志时间并将其差异的事件应用到其他slave filter_mysqlbinlog 除去不必要的rollback事件MHA不再使用该工具 purge_relay_logs 清除中继日志(不会阻塞SQL线程) MHA工作步骤 从宕机的master保存二进制日志事件; 识别含有最新更新的slave; 应用差异的中继日至到其他slave; 应用从master保存的二进制日志事件; 提升一个slave为新的master; 使其他的slave连接到新的master进行复制; 为了检查服务器硬件损坏宕机造成数据丢失，在配置MHA时建议配置Mysql半同步主从复制 部署MHA环境 Master centos1 192.168.0.81 server-id:81 write MHA-Node Master Back centos2 192.168.0.82 server-id:82 read-only MHA-Node Slave centos3 192.168.0.83 server-id:83 read-only MHA-Node Monitor host centos4 192.168.0.84 server-id:null monitor MHA-Manager 安装 perl-DBD-MySQL yum install perl-DBD-MySQL -y MHA-Node,所有的结点都需要安装MHA-node 下载MHA-Node，执行安装命令rpm -ivh mha4mysql-node-0.54-1.el5.noarch.rpm MHA-Manager 安装依赖程序 yum install perl-DBD-MySQL perl-Config-Tiny perl-Log-Dispatch perl-Parallel-ForkManager -y 安装MHA-Manager rpm -ivh mha4mysql-manager-0.55-1.el5.noarch.rpm 安装附加脚本文件 cp mha-source/samples/scripts/* /usr/local/bin/ 配置服务器ssh登陆 不能禁用密码登陆，否则会出错 执行ssh-add时报错：Could not open a connection to your authentication agent,先执行ssh-agent bash命令，再执行ssh-add命令 ssh-keygen -t rsa生成密钥对，复制id_rsa.pub为authorized_keys,将生成的公钥复制到其他服务器ssh-copy-id [centos2/3/4],其他服务器中执行相同的操作 配置Mysql数据库主从复制 配置过程参考文章：Mysql配置异步同步主从复制 在centos2\3上设置数据库为只读，在数据库中设置，未将该设置添加到mysql配置文件中，在mysql中执行命令set global read_only = 1; 在centos2\3上设置relay log的清除方式set global relay_log_purge = 0; MHA在切换过程中，从苦的恢复过程依赖于relay log的相关信息，设置relay log 为off状态，采用手动清除的方式，创建脚本文件，并将脚本文件添加到crontab中定期执行，脚本文件purge_relay_log.sh123456789101112#!/bin/bashuser=rootpassword=mysql_root_passwdport=3306log_dir=&apos;/var/log/masterha/log&apos;work_dir=`home/user/mysql/`purge=&apos;/usr/local/bin/purge_relay_logs&apos;if [ ! -d $&#123;log_dir&#125; ]then mkdir $&#123;log_dir&#125; -pfi$&#123;purge&#125; --user=$&#123;user&#125; --password=$&#123;passwd&#125; --disable_relay_log_purge --port=$&#123;port&#125; --workdir=$&#123;work_dir&#125; &gt;&gt; $&#123;log_dir&#125;/purge_relay_logs.log 2&gt;&amp;1 配置 创建配置文件目录mkdir -p /etc/masterha 创建配置文件cd /etc/masterha &amp;&amp; touch app1.cnf 12345678910111213141516171819202122232425262728293031323334353637383940414243444546[server default]# 设置mha-manager工作目录manager_workdir=/var/log/masterha/app1# 设置mha-manager日志目录manager_log=/var/log/masterha/app1/app1.log# 设置master保存二进制日志事件的位置,便于mha找到master_binlog_dir=/var/lib/mysql# 设置自动failover的切换脚本，需要提前安装master_ip_failover_script=/usr/local/bin/master_ip_failover# 设置手动切换的脚本master_ip_online_change_script=/usr/local/bin/master_ip_online_change# 设置Mysql用户密码user=rootpassword=mysql_root_passwd# 监控主库，发送ping包的间隔，默认3s，尝试3次没有回应自动railoverping_interval=1# 设置远程mysql发生切换时二进制日志事件保存位置remote_workdir=/tmp# 设置数据库主从复制的用户密码repl_user=slave_dbrepl_password=slave_password# 设置发生切换后发送的报警脚本report_script=/usr/local/bin/send_reportsecondary_check_script=/usr/local/bin/master_secondary_check -s centos2 -s centos3# 设置故障发生后关闭故障主机的脚本，作用是防止发生脑裂# shutdown-script=&quot;&quot;# 设置ssh登陆的用户名,使用普通用户在检查复制时会报错ssh_user=root[server1]# Master服务器hostname=centos1# port=3306[server2]# 备用Master服务器hostname=centos2# port=3306# 设置为候选Master，如果设置了该参数，Master发生故障后，将该服务器提升为Master，无论该服务器是否是最新的Slavecandidate_master=1# 默认情况下，如果一个slave落后master 100M的relay logs,MHA将不会选择该slave作为新master;通过设置该参数值为0，MHA在出发切换mster主机时，会忽略复制延时，该参数对设置了candidate_master=1的主机非常有用check_repl_delay=0[server3]hostname=centos3# port=3306 在mha中引入keepalived，修改vip漂移脚本文件master_ip_failover,在该脚本中添加Master宕机后MHA对keepalived的处理vi /usr/local/bin/master_ip_failover添加以下内容 4123my $vip = &apos;192.168.0.110&apos;; # vip地址my $ssh_start_vip =&quot;systemctl start keepalived&quot;;my $ssh_stop_vip =&quot;systemctl stop keepalived&quot;; 修改脚本文件master_ip_online,添加以下内容123456my $vip = &apos;192.168.0.110/32&apos;; # vip地址my $key = &apos;1&apos;;my $ssh_start_vip = &quot;systemctl start keepalived&quot;;my $ssh_stop_vip = &quot;systemctl stop keepalived&quot;;my $orig_master_ssh_port = 22;my $new_master_ssh_port = 22; 修改脚本文件send_report,添加以下内容 12345my $smtp=&apos;smtp.xxx.com&apos;;my $mail_from=&apos;send@xx.com&apos;;my $mail_user=&apos;send_user@xx.com&apos;;my $mail_pass=&apos;Password&apos;;my$mail_to=[&apos;mail1@xx.com&apos;,&apos;mail2@xx.com&apos;]; 检查MHA-Manager到所有结点的ssh连接状态,使用命令masterha_check_ssh --conf=/etc/masterha/app1.cnf 执行命令后会有一个报错Can&#39;t locate MHA/SSHCheck.pm in @INC (@INC contains: /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at /usr/bin/masterha_check_ssh line 25.,解决方法是在所有的服务器中做一个软连接ln -s /usr/lib/perl5/vendor_perl/MHA /usr/lib64/perl5/vendor_perl/ 检查 检查masterha的ssh无密码验证masterha_check_ssh --conf=/etc/masterha/app1.cnf 检查masterha的复制环境masterha_check_repl，在app1.cnf配置文件中，如果ssh_user使用的是普通用户，在服务器中，普通用户无权访问mysql的二进制日志bin_log,relay_log,mha工作目录，会提示以下错误[error][/usr/lib64/perl5/vendor_perl/MHA/MasterMonitor.pm, ln386] Error happend on checking configurations. SSH Configuration Check Failed! at /usr/lib64/perl5/vendor_perl/MHA/MasterMonitor.pm line 341. 检查Mysql主从复制masterha_check_repl --conf=/etc/masterha/app1.cnf,出现ok字样，表明检查通过 在管理服务器中后台启动MHA监控nohup masterha_manager --conf=/etc/masterha/app1.cnf --remove_dead_master_conf --ignore_last_failover &lt; /dev/null &gt; /var/log/masterha/app1/app1.log 2&gt;&amp;1 &amp; --remove_dead_master_conf 该参数代表当发生主从切换后，老的主库的ip将会从配置文件中移除 --manager_log 日志存放位置 --ignore_last_failover 在缺省情况下，如果mha检测到连续发生宕机，且2次宕机的事件不足8小时，则不会进行failover。该参数代表忽略上次mha触发切换产生的文件 查看mha的运行状态masterha_check_status,可以看到mha运行中，Master服务器为centos 关闭mha监控masterha_stop --conf=/etc/masterha/app1.cnf 测试 使用sysbench对Mysql数据库进行插入测试，执行以下命令 12345 sysbench \--mysql-user=root --mysql-password=mysql_root_passwd --db-driver=mysql \--mysql-socket=/var/lib/mysql/mysql.sock --mysql-db=test1120 --range-size=100 \--table-size=2000 --tables=20 --threads=2 --time=60 \--rand-type=uniform /usr/share/doc/sysbench/oltp_insert.lua prepare 停掉Backup服务器中的slave sql线程，模拟主从延时 停掉Master服务器中的Mysqld，模拟Master宕机,Master切换到centos2，vip漂移到centos2,slave指向centos2,centos2中的slave指向被清空，mha manager进程会停掉，在配置文件中，关于server1的信息被删除 参考 https://www.cnblogs.com/gomysql/p/3675429.html https://github.com/yoshinorim/mha4mysql-manager/wiki/Installation https://rpmfind.net/linux/RPM/index.html http://www.voidcn.com/article/p-fsekcbpa-mt.html]]></content>
      <categories>
        <category>Mysql</category>
        <category>高可用</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>keepalived</tag>
        <tag>集群</tag>
        <tag>MHA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos中安装keepalive和基础使用]]></title>
    <url>%2F2018%2F11%2F20%2Fcentos-keepalived.html</url>
    <content type="text"><![CDATA[Centos中安装keepalive和基础使用环境 CentOS7 keepalived 安装 在master和备用master上安装keepalive软件,执行命令yum install keepalived -y 修改keepalive的配置 编辑配置文件 vi /etc/keepalived/keepalived.conf,主被服务器的配置相同，不配置vrrp_script`,使用mha实现vip的自动漂移 123456789101112131415161718192021222324252627282930313233343536global_defs &#123;# 定义接收邮件的邮箱notification_email &#123; test@localhost &#125;# 定义发送邮件的邮箱notification_email_from test1@localhost# 定义smtp服务器smtp_server smtp.localhostsmtp_connect_timeout 10&#125;# 定义mysql_a实例vrrp_instance mysql_a &#123;# 定义服务器状态，在Master服务器中也可以将状态设置为MASTER，不设置的目的是期望在Master宕机后再恢复时，不主动将Master状态抢过来，避免Mysql服务的波动state BACKUP# 定义使用的网络接口interface eth0 # 定义虚拟路由id，一组lvs的虚拟路由标志须相同 virtual_router_id 51 # 服务启动优先级，值越大，优先级越大，但是不能大于MASTER值 priority 150 # 服务器之间的存活检查时间 advert_int 1 # keepalived工作模式为非抢占 nopreemptauthentication &#123; # 认证类型 auth_type PASS # 认证密码，一组lvs的认证密码须相同 auth_pass mysql_pass &#125;virtual_ipaddress &#123; # 配置虚拟ip地址 192.168.0.110 &#125;&#125; 启动keepalived systemctl start keepalived 查看Master服务器中的vip ip addr 注意 vrrp_instance实例有2种工作模式，master-backup,backup-backup。区别：mastterbackup模式下，一旦master宕机，vip会漂移到backup服务器上，当master修复后，keepalived启动后，master会从backup把vip抢占回来，即便设置了非抢占模式;backup-backup模式下，当master宕机后，vip会漂移到backup服务器上，当master修复后，并启动keepalived，master并不会抢占vip，即便master的优先级高于bakcup，也不会发生抢占vip。为了减少vip的漂移次数，通常是把修复好的主库当作新的备用库。 测试 将Master Server中的keepalived服务停掉systemctl stop keepalived 查看ip地址ip addr ,vip已经消失 查看Backup Server中的ip地址，vip已经漂移 参考 http://www.voidcn.com/article/p-fsekcbpa-mt.html]]></content>
      <categories>
        <category>CentOS</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>keepalived</tag>
        <tag>集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS安装HeartBeat服务]]></title>
    <url>%2F2018%2F11%2F16%2Fcentos-heartbeat.html</url>
    <content type="text"><![CDATA[CentOS安装HeartBeat服务环境 主机：CentOS9 依赖 Cluster Glue Resource Agents 安装安装Cluster Glue 下载软件源码 下载地址：http://www.linux-ha.org/wiki/Downloads 安装编译所需依赖 yum install glib2-devel libtool-ltdl-devl net-snmp-devel bzip2-devel ncurses-devel openssl-devel libtool libxml2 libxml2-devel gettext bison flex zlib-devel mailx which libxslt docbook-dtds docbook-style-xsl PyXML shadow-utils opensp autoconf automake bzip2 e2fsprogs-devel libxslt-devel libtool-ltdl-devel make asciidoc libuuid-devel 配置编译软件 ./autogen.sh ./configure --prefix=/usr/local/heartbeat make -j 20 安装 make install 安装Resource Agent 下载软件源码，并上传到服务器中 解压软件源码压缩包 配置编译软件 ./autogent.sh export CFLAGS=&quot;${CFLAGS} -I/usr/local/heartbeat/include -L/usr/local/heartbeat/lib&quot; 注意-I与-L与后接字段间无空格，如出现错误需要unset变量CFLAGS ./configure --prefix=/usr/local/heartbeat ln -s /usr/localheartbeat/lib/* /lib/ ln -s /usr/local/heart/lib/* /lib64/ make -j20 安装 make install 安装HeartBeat 下载软件源码，并上传到服务器中 解压软件源码 配置编译软件 ./bootstrap ./configure --prefix=/usr/local/heartbeat make -j20 编译会出现一个HA_HBCONF_DIR&quot; redefined [-Werror]的错误提示，说明在glue.conf.h中，宏HA_HBCONF_DIR被定义多次，编辑文件glue_conf.h,将代码define HA_HBCONF_DIR &quot;/usr/local/heartbeat/etc/ha.d/&quot;删除掉，重新编译。文件位置/usr/local/heartbeat/include/heartbeat 安装 make install cd doc cp -a ha.cf authkeys haresources /usr/local/heartbeat/etc/ha.d/ ln -svf /usr/local/heartbeat/lib64/heartbeat/plugins/RAExec/* /usr/local/heartbeat/lib/heartbeat/plugins/RAExec/ ln -svf /usr/local/heartbeat/lib64/heartbeat/plugins/* /usr/local/heartbeat/lib/heartbeat/plugins/ groupadd haclient useradd -r -s /bin/nologin -g haclient hacluster 配置 编辑HeartBeat的配置文件 cd /usr/local/heartbeat/etc/ha.d &amp;&amp; cp ha.cf ha.cf.old &amp;&amp; vi ha.cf 修改以下内容 1234567891011121314151617181920212223242526272829303132# 保存调试信息文件debugfile /var/log/ha-debug # 日志文件logfile /var/log/ha-log# 使用系统日志logfacility local0# 心跳的时间间隔，单位秒keeplive 2# 超出该时间未收到对方节点的心跳，则判定对方死亡deadtime 30# 超出改时间未收到对方节点的心跳，则发出警告记录日志warntime 10# 在某系统上，系统启动或重启后需要经过一段时间网络才能正常工作，该参数就是为解决这个问题，取值至少时deadtime的2倍initdead 120# 设置广播通信使用的端口，默认694udpport 694# 传播心跳的广播网卡bcast eth0# 对方服务器心跳检测ipucast eth0 192.168.0.82# 设置为on表示一旦主节点恢复运行，则自动获取资源并取代从节点auto_failback off# 配置主从节点node centos-kvmnode centos1106-kvm# 如果ping不通该地址，就认为当前断网ping 192.168.0.1# 指定与HeartBeat一同启动和关闭的进程，该进程自动监视，遇到故障则重新启动。# 最常用的经常是ipfail，该进程用于检测和处理网络故障需要配合ping语句指定的pingrespawn heartbeat /usr/local/heartbeat/libexec/heartbeat/ipfail# 指定用户和组apiauth ipfail gid=haclient uid=hacluster 在其他节点中配置HeartBeat时，只需要修改广播网卡和对方心跳检测的ip 编辑认证文件,并修改权限为600 auth 2 2 sha1 HI 编辑资源配置文件haresources centos1 Ipaddr::192.168.0.80/24/eth0:0 mysqld # centos1作为主结点，192.168.0.80作为vip，mysql是主机启动后执行的脚本，脚本所在目录与资源配置文件相同 脚本mysqld内容/etc/init.d/mysqld 参考 https://blog.csdn.net/wzy0623/article/details/81188814#%E4%BA%8C%E3%80%81%E5%AE%89%E8%A3%85Heartbeat]]></content>
      <categories>
        <category>CentOS</category>
        <category>HA</category>
      </categories>
      <tags>
        <tag>HeartBeat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Debian安装配置Bridge网络]]></title>
    <url>%2F2018%2F11%2F15%2Fdebian-bridge.html</url>
    <content type="text"><![CDATA[Debian 安装配置Bridge网络安装 安装桥接网络所需要的软件 apt install bridge-utils 配置文件/etc/network/interfaces 仅保留网络lo的配置，其他的全部删除 进入附加网络配置文件夹 cd /etc/network/interfaces.d 添加桥接网络的配置文件touch br0 &amp;&amp; vi br0 1234bridge_ports enp0s31f6 # enp0s31f6通过网桥连接网络bridge_stp off # 不使用生成树协议bridge_waitport 0 # 在端口可用前无延迟bridge_fd 0 # 无转发延迟 如果本机有多个网卡，需要将多个网卡逻辑到网卡中，还可以使用命令brctl addif br0 eth1 eth2 重启网络管理器 systemctl restart network-manager systemctl status network-manager 查看本机的网络，原设置的ip地址已经消失，只剩下网桥的ip地址 参考 https://www.cyberciti.biz/faq/how-to-configuring-bridging-in-debian-linux/ https://wiki.debian.org/BridgeNetworkConnections]]></content>
      <categories>
        <category>Debian</category>
        <category>网络</category>
      </categories>
      <tags>
        <tag>Debian</tag>
        <tag>桥接</tag>
        <tag>Bridge</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS安装shutter]]></title>
    <url>%2F2018%2F11%2F12%2Fcentos-shutter.html</url>
    <content type="text"><![CDATA[使用Nux-dextop软件源安装软件简介 Nux-dextop是类Rhel的的第三方软件源，同样对CentOS也有很好的支持。Shutter是Linux上强大的截图工具，在系统默认的repo中，未包含该软件。 在使用Nux-dextop之前需要安装repel-release repo源，可以通过执行命令yum install repel-release完成安装。 安装 yum -y install epel-release &amp;&amp; rpm -Uvh http://li.nux.ro/download/nux/dextop/el7/x86_64/nux-dextop-release-0-5.el7.nux.noarch.rpm /# 安装reel-release和nux-dextop yum makecache yum install shutter --enablerepo=nux-dextop /# 安装shutter 参考 https://www.2daygeek.com/install-enable-nux-dextop-repository-on-centos-rhel-scientific-linux/]]></content>
      <categories>
        <category>CentOS</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>shutter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS安装KVM虚拟机]]></title>
    <url>%2F2018%2F11%2F11%2Fcentos-kvm.html</url>
    <content type="text"><![CDATA[CentOS 安装KVM虚拟机环境 CentOS7 KVM Qemu-kvm 安装 安装kvm yum install qemu-kvm libvirt libvirt-python libguestfs-tools virt-install virt-viewer systemctl enable libvirtd systemctl start libvirtd 检查kvm的安装 lsmod | grep -i kvm 配置桥接网络 brctl show # 查看本机桥接网络 virsh net-list --all # 查看kvm网络 cd /etc/sysconfig/network-scripts &amp;&amp; cp ifcfg-enp0s31f6 ifcfg-enp0s31f6.old &amp;&amp; vi ifcfg-enp0s31f6 # 添加BRIDGE=br0 touch ifcfg-br0 &amp;&amp; vi ifcfg-br0 12345678910DEVICE=&quot;br0&quot;BOOTPROTO=&quot;static&quot;IPADDR=&quot;192.168.0.100&quot;PREFIX=&quot;24&quot;GATEWAY=&quot;192.168.0.1&quot;DNS1=&quot;192.168.0.1&quot;IPV6INIT=&quot;no&quot;ONBOOT=&quot;yes&quot;TYPE=&quot;Bridge&quot;DELAY=&quot;0&quot; systemctl restart NetworkManager ifdown enp0s31f6 &amp;&amp; ifup enp0s31f6 导入虚拟机 123456 virt-install \ --name centos1 --os-variant auto --kvm --memory 2048 --vcpus 2 --graphics vnc --netwok bridge=br0,model=virtio --disk /home/user/kvm/centos1.qcow2 --import# 或者使用xml文件重新定义一个虚拟机 virsh dedine centos.xml# 使用virt-viewer连接到虚拟机 virt-viewer centos1 错误 在创建完成虚拟机后，会出现Permission denied 的错误提示，常规的解决方法有以下几个方法： 关闭selinux vi /etc/selinux /# 将selinux设置为permissive或disabled 如果在启用SELinux下使用kvm，则需要将存放VM Disk的路径添加到SELinux中，具体执行以下命令：12semanage fcontext -a -t virt_image_t &quot;/paht/disk(/.*)?&quot;restorecon /paht/disk 将用户添加到kvm用户组中 usermod -a -G libvirt username usermod -a -G kvm username usermod -a -G qemu username usermod -a -G wheel username 修改libvirt配置文件中定义的用户和用户组 vi /etc/libvirt/qemu.conf /# 添加user=”username” group=”kvm” 修改设备文件kvm的用户和用户组 cd /dev/ &amp;&amp; chown root:kvm kvm /# 用户组要与配置文件中定义的用户组对应 参考 https://www.cyberciti.biz/faq/how-to-install-kvm-on-centos-7-rhel-7-headless-server/ SELINUX AND VIRTUALIZATIONs]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>KVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HAProxy基础学习笔记]]></title>
    <url>%2F2018%2F11%2F05%2Fhaproxy-doc.html</url>
    <content type="text"><![CDATA[HAProxy代理软件基础学习笔记基础知识 HAProxy是使用C语言编写的，提供负载均衡，以及基于TCP和HTTP的应用程序代理。它支持Client-HAProxy，HAProxy-Server的全程SSL链接； HAProxy采用单线程事件驱动型非阻塞引擎，支持较大的并发数链接，具有较高的性能和稳定性；多线程或多进程受内存、系统调度、锁的限制，很少能处理数千并发链接。事件驱动型因为有更好的资源和事件管理的用户空间实现所有任务，所以可以处理较大并发数链接。 1 HAProxy适用于负载大的Web站点，支持数以万计的并发连接，它的运行模式可以简单安全的整合到当前的架构中，使Web服务器不被暴露到网络中； HAProxy还提供基于Web的统计信息页面，用于展现健康状态和数据流量； HAProxy常见的架构形式如下：2 核心功能 负载均衡 提供2种负载均衡模式：TCP伪四层和HTTP七层 健康检查 支持TCP和HTTP两种健康检查模式 会话保持 对尉氏县会话共享的应用集群，可以通过Insert\Reweite\Prefix Cookie多种Hash方式实现会话保持 SSL 可以解析HTTPS协议，并能够将请求解密为HTTP后后端传输 监控与统计 提供基于Web的统计信息页面 安装 下载HAProxy压缩包，版本号是1.8.14 下载地址：https://www.haproxy.org 将下载的压缩包上传到服务器中,解压 3 123456tar -xzvf haproxy-1.8.14.tar.gzcd haproxy-1.8.14uname -r # 获取系统内核版本信息make TARGET=linux310 ARCH=x86_64 PREFIX=/usr/local/haproxy -j 20# TRAGET 指定操作系统内核版本 ；ARCH 指定操作系统位数； PREFIX指定安装路径;使用参数-j 可以指定用于编译的线程数目make install 完成安装 准备配置文件 12345cd /usr/local/haproxymkdir confcd conftouch haproxy.cfg # haproxy的配置文件user add -r -s /bin/nologin haproxy 准备启动脚本 12345678910cd ~/haproxy-1.8.14/examplecp haproxy.init /etc/init.d/haproxychmod +x haproxyvi haproxy# 分别修改HAProxy配置文件目录和sbin目录HAP_HOME=/usr/local/haproxyBIN=$&#123;HAP_HOME&#125;/sbinCFG=$&#123;HAP_HOME&#125;/conf/$&#123;BASENAME&#125;.cfg# 如果出现“第26行，期待一元表达式”的提示信息，修改NETWORK字段[ $&#123;NETWORKING&#125;x = &quot;no&quot;x ] &amp;&amp; exit 0 配置 配置文件分为2大段：全局配置(Global),代理配置(Proxies) 全局配置 全局配置建议不做修改，包含的字段有： 进程与安全配置相关的参数； 性能配置参数； Debug配置参数； 用户列表参数； peers参数； Mailers参数 代理配置 代理配置又分为3部分，分别是defaults,frontend,backend,listen。 defaults 默认参数的配置，在该部分内配置的参数，会被自动引用到之后的frontend，backend，listen，某些参数属于公用的配置，既可以在defaults中配置，也可以在frontend，backend，listen中配置，两者都配置的以后者为准，defaults中的配置会被覆盖； frontend 负责配置接受用户请求的虚拟前段节点，类似于nginx配置文件中的server{}字段； backend 负责配置集群后端服务器集群的配置，用来定义一组真实服务器，用来处理用户发出的请求，类似于nginx配置中的upstream{}字段； listen 用来配置前段和haproxy后端 配置文件5 6 配置日志文件 123456789cd /etc/ &amp;&amp; cp rsyslog.conf rsyslog.conf.oldvi rsyslog.conf# Provides UDP syslog reception$ModLoad imudp$UDPServerRun 514 ------&gt;启动udp，启动端口后将作为服务器工作# Provides TCP syslog reception$ModLoad imtcp$InputTCPServerRun 514 ------&gt;启动tcp监听端口local2.* /var/log/haproxy.log haproxy配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263global log 127.0.0.1 local0 # 全局日志文件配置条目，最多可定义2个 log 127.0.0.1 local2 notice chroot /var/lib/haproxy # 切换根目录，将haproxy运行在/var/lib/haproxy，增加其安全性，注意该目录的权限和所属用户。 # stats socket /run/haproxy/admin.sock mode 660 level admin # stats timeout 30s user haproxy # 指定运行用户和组 group haproxy daemon # 设定haproxy以后台方式运行 maxconn 40000 #设定前段的最大连接数，不能用户backend，默认为2000 # HAProxy会为每个连接维持2个缓冲，每个缓冲的大小为8kb，再加上其他数据，每个连接大约占17kb RAM，这就意味着1GB的RAM可以维持40000-50000的并发连接 # maxsslconn # 设定每个进程所能接受的ssl最大并发连接数defaults log global # 继承全局日志 mode http # 设置haproxy默认的运行模式，默认为http，支持tcp、http，tcp常用于ssl\ssh\smtp等服务 option httplog option dontlognull # 不记录上级负载均衡发送的用于检测状态的心跳包 option http-server-close # 客户端与服务器在完成一次请求后，hap会主动关闭该tcp连接，有助于提供性能 option forwardfor except 127.0.0.0/8 # 由于hap工作在反向代理方向，后端集群中的服务器可能无法获取发送请求的真实ip，使用forwardfor可以在报文中分装新的字段记录请求段ip，使用except排除本地的ip地址 option redispatch # 是否允许在session失败后重新分配 retries 3 # 连接后端服务器失败重试次数，超出该数，hap会将对应的后端服务器设置为不可用状态 timeout http-request 10s # timeout connect 10s # 成功连接到一台服务器的最长等待时间，默认为毫秒，可以换用其他单位 timeout queue 10s # 等待最大时长 timeout client 10s # 连接客户端发送数据的最长等待时间，默认为毫秒 timeout server 10s # 服务器端回应客户端数据放的最长等待时间 timeout check 10s # 设置对后端服务器的检测超时时间 maxconn 3000 # 每个server最大连接数frontend main bind :80,:443 # 同时监听2个端口，之间不能有空格，监听端口要重启服务 # bind 192.168.0.93:8080 mode http # sats uri /haproxy?stats # dfault_backend http_back default_bakcend webserverbackend webserver balance roundrobin # 后端集群服务器组内的调度算法，roundrobin轮循，依次访问每个后端服务器 server webserver 192.168.0.91:80 check # webserver为服务器在haproxy中的内部名称，主要出现在日志和警告中 server webserver 192.168.0.92:80 check # ip地址可以使用主机名替代 server backserver 192.168.0.90:80 check backup 设定当前服务器为备用服务器，check表示对当前server做健康状态检查，默认是tcp检测 # disable标记为不可用 #redir &lt;prefix&gt; 将发往该服务器的请求重定向到指定的url #cookie &lt;value&gt;为当前server指定其cookie，用于实现基于cookie的会话黏性 # server options: weight 支持配置权重 # weight默认为1，最大为256,0表示不参与负载均衡，不被调度 # 动态算法：支持权重的运行时调整，支持慢启动，每个后端中最多支持4095个server # static-rr 静态算法，不支持权重运行时调整及慢启动，后端主机数量无上限 server webserver 192.168.0.93:80 check weight 3 # 加权轮询，权重为3，未设置的权重默认为1 server webserver 192.168.0.94:80 check listen hap_page mode http bind *:8081 option httplog # 采用http日志格式 stats refresh 30s # 统计页面自动刷新时间 stats uri /hap?stats # 统计页面url地址 stats realm HAProxy Manage Page # 弹出用户名密码对话框的提示文本 stats auth root:haproxy # 设置登录统计页面的用户名密码 stats hide-version # 隐藏统计页面上的HAProxy版本信息 stats admin if TRUE # 如果任何通过就做管理功能，可以管理后端服务器 配置文件部分参数 ACL ACL用于实现基于请求报文的首部、响应保温的内容或其他的环境状态信息来做出转发决策。配置的步骤分为2步，首先定义1个ACL，而后在满足ACL的情况下执行特定的动作。 ACL语法格式： acl &lt;aclname&gt; &lt;criterion&gt; [flags] [operator] &lt;value&gt; aclname区分大小写 criterion测试标准 be_sess_rate用于测试指定的backend上会话常见的速率是否满足指定的条件，常用于在指定backend上的会话速率过高时将用户请求转发至另外的backend，或用于阻止攻击行为 acl being_scanned be_sess_rate gt 50 # 定义一个acl redirect location /error_pages/ednied.html if being_scanned # 指定满足acl条件，执行的操作 fe_sess_rate用户测试指定的frontend或当前frontend的会话创建速率是否满足指定的条件 hdr\&lt;string> 用于测试请求报文中的所有首部或指定首部满足指定的条件。指定首部时，名称不分区大小写，且字符内不能有任何多于的空白字符 method\&lt;string>用于测试http请求报文中使用的方法 path_beg \&lt;string>用于测试请求的url是否以指定的模式开头 acl url_static path_beg -i /static /css /images /js # 测试url中是否以这些字段开头 path_end \&lt;string> 用于测试请求的url是否以指定的模式结尾 acl url_static path_end -i .jpg .png .css .js hdr_beg \&lt;string> 用于测试请求的报文的指定首部的开头部分是否符合指定的模式 hdr_end\&lt;string> flage -i 不区分中模式字符的大小写 -f 从指定的文件中加载模式 – 标识符的强制结束标记 value 整数或整数范围 字符串，支持使用-i忽略大小写，使用\转义字符 正则表达式 ip地址及网络地址 server参数 语法格式server &lt;name&gt; &lt;address&gt;[:port] [param*] name: 服务器内部名称 address: 服务器的ip地址，也可以使用主机名 port: 连接请求发往服务器时的目标端口，未指定时，使用客户端请求时的端口 param服务器设定的参数 backup: 设定为备用服务器 check: 启动对服务器的健康检查 inter \&lt;delay> :设定健康状态检查的时间间隔，单位为毫秒，默认为2000 rise \&lt;count> : 设定健康状态检查中，某离线的server从离线状态转换至正常状态需要成功检查的次数 fall \&lt;count>: 去人server从正常状态转为不可用状态需要检查的次数 错误提示 统计页面 参考 http://blog.51cto.com/11010461/2139872 https://www.haproxy.org http://blog.51cto.com/11010461/2139872 https://cbonte.github.io/haproxy-dconv/1.8/configuration.html https://www.unixmen.com/installing-haproxy-for-load-balancing-on-centos-7/ https://www.cnblogs.com/pangguoping/p/7647091.html http://www.ttlsa.com/linux/haproxy-study-tutorial/]]></content>
      <categories>
        <category>HA</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>HAProxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HeartBeat基础知识学习]]></title>
    <url>%2F2018%2F11%2F04%2Fheartbeat-doc.html</url>
    <content type="text"><![CDATA[HeartBeat 软件基础知识学习笔记 HeartBeat是Linux-HA项目中的一个组件，集成了HA软件中多需要的基本功能：心跳检测、资源监管、检测群集系统服务、集群节点转移。 心跳检测可以通过网络链路和串口进行，而且支持冗余链路。几点之间相互通过发送报文来告诉对方自己当前的状态，如果在指定的时间内没有收到对方发送的报文，那么就认定对方失效，这时启动资源接管模块来接管运行在对方节点上的资源或服务。 HeartBeat主要由以下部分组成： HeartBeat节点间通信检测模块 检测主次节点的运行状态，以决定节点是否失效 Ha-Logd 集群事件日志服务 用于记录集群中所有模块和服务的运行信息 CCM 集群成员一致性管理模块 用于管理集群节点，同时管理成员之间的关系和节点间资源的分配 CRM 集群资源管理模块 处理节点和资源之间的依赖关系，管理节点对资源的使用，一般由CRM守护进程crmd、Cluster Policy Engine、Cluster Transition Engine组成 LRM 本地资源管理模块 负责本地资源的启动、停止、监控，一般由LRM守护进程lrmd和节点监控进程Stonith Daemon组成，lrmd负责节点间的通讯，Stonith Daemon通常时一个Fence设备，用于监控节点状态，当节点出现问题时处于正常状态的节点会通过Fence设备将其关机或重启以释放IP、磁盘等资源，保证资源被一个节点拥有，防止资源争用 Stonith Daemon 使出现问题的节点从集群环境中脱离 Cluster Policy Engine 集群策略引擎 具体实施节点资源间的管理和依赖 Cluster Transition Engine 集群转移引擎 当节点出现故障时，负责协调另一个节点上的进程进行合理的资源接管 HeartBeat为了监视它控制的资源或应用程序是否正常运行，需要通过第三方插件来扩展功能。HeartBeat自带了ipfail,Stonith,Ldirector插件。 ipfail: 用于检测网络故障并做出合理反应，使用ping节点或ping接电阻来检测网络是否出现故障； Stonith: 在失效的节点恢复后，合理接管集群服务资源，放置数据冲突；节点失效后，会从集群中删除该节点，保证共享存储环境中的数据完整性。如果不使用Stonith，那么失效的节点不被删除，就导致服务在多于1个节点中运行，会造成数据冲突； Ldirector: 监控集群服务节点运行状态，Ldirector如果检测到集群中节点出现故障，会屏蔽此节点的对外连接功能，同时将后续请求转移到正常的节点中，继续提供服务；]]></content>
      <categories>
        <category>HA</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>HeartBeat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql数据库半同步复制的配置]]></title>
    <url>%2F2018%2F11%2F03%2Fundefined.html</url>
    <content type="text"><![CDATA[Mysql半同步主从复制环境 宿主：CentOS7 Mysql：使用5.7.23，基于Docker搭建的实验环境 其他：在上一篇中已经成功搭建了异步同步主从复制的环境，在该基础上搭建半同步主从复制 使用半同步复制，在主机宕机的情况下，可以保证至少有一台从服务器中的数据与主服务器中的数据保持一致。 配置主服务器 进入Mysql数据库，安装插件rpl_remi_sync_master 查看插件是否安装show plugins; mysql&gt; install plugin rpl_semi_sync_master soname &#39;semisynv_mster.so&#39;; 启用插件 set global rpl_semi_sync_master_enabled = 1; # 注意= 与字符和数字之间有空格，否则会报错 安装完成后，查看插件的状态。 配置从服务器 进入数据库，安装并启用插件rpl_semi_sync_slave,相关的操作可以参考配置主服务器的1&amp;2 mysql&gt; install plugin rpl_semi_sync_slave soname &#39;semisync_slave.so&#39;; # 需要注意，在Master中使用的是master 模块，在从服务器中使用的是slave模块 查看semi插件的状态 查看是否进行半同步 在主服务器中查看半同步复制客户端的数量show global status like &#39;%semi%&#39;;]]></content>
      <categories>
        <category>Mysql</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>半同步</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql-master2slave]]></title>
    <url>%2F2018%2F11%2F02%2Fundefined.html</url>
    <content type="text"><![CDATA[Mysql 数据库配置主从复制环境 为了实验Mysql数据库的主从复制，我们使用Docker搭建相关的环境； 使用Docker-Compose分别创建3个容器，容器名称分别是mysql-master mysql-slave0 mysql-slave1 mysql-slave2 ; 将Mysql数据库的附加配置文件和数据库文件映射到宿主机中； 异步复制主服务器文件配置 找到主服务器的配置文件${PWD}/mysql/master/my.cnf 对配置文件进行如下修改： 123456789101112[mysqld]pid-file=/var/run/mysqld/mysqld.pidsocket=/var/run/mysqld/mysqld.sockdatadir=/var/lib/mysqllog-error=/var/log/mysql/mysql-error.logsymbolic-links=0 # 不使用到表的链接符号lower_case_table_names=1 #表名在硬盘中以小写保存，名称比较对大小写不敏感server-id=80 #服务器id，在局域网内，该id是唯一的，一般设置为ip的最后一位log-bin=master-bin #开启二进制文件，名称可以根据需要自定义，默认保存在数据目录下，会自动添加一个数字扩展名用于日志老化，不支持自定义扩展名log-bin-index=master-bin.index #二进制日志文件对应的日志索引文件，该文件包含所有的二进制日志，文件名与二进制日志文件名相同，扩展名为.indexmax_binlog_size=1M # 若当前的日志大小达到1M，则自动创建新的二进制日志。但是，对于大的事物，二进制日志会超过该设定值，将所有事务仅写入一个日志文件expire_logs_days = 10 #日志保留时间 主服务器数据库配置 进入容器mysql-master:docker exec -it mysql-master bash 进入mysql数据库内容进行相关的操作： 123456mysql -uroot -pmysql&gt; grant replication slave ,replication client on *.* to &apos;slave_db&apos;@&apos;192.168.10.%&apos; identified by &apos;passwd&apos;; flush privileges; flush table with read lock; #锁库，不让数据再进行写操作，这个命令在结束终端会话时自动解锁 show master status; 从服务器文件配置 修改从服务器的配置文件的配置文件： 12345678910[mysqld]pid-file=/var/run/mysqld/mysqld.pidsocket=/var/run/mysqld/mysqld.sockdatadir=/var/lib/mysqllog-error=/var/log/mysql/mysql-error.logsymbolic-links=0lower_case_table_names=1server-id=100 # 其他从服务器依次修改log-bin=slave0-bin.log # 修改为其他从服务器名称sync_binlog=3 #控制binlog写入频率，每执行多少次事务写入一次，这个参数性能消耗很大，但是可以减少Mysql崩溃造成的损失 从服务器数据库配置 进入数据库进行相关操作,其他从服务器参考以下内容修改： 1234mysql&gt; change master to master_host=`192.168.10.2`,master_port=3306,master_user=`slave_db`,master_password=`20170110`,master_log_file=`master-bin.000001`,master_log_pos=634; start slave; show slave status; 参考 https://juejin.im/post/5a2e4bd66fb9a044fa19cfb7 https://juejin.im/post/5afed922f265da0ba76ffeab]]></content>
      <categories>
        <category>Mysql</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>Mysql</tag>
        <tag>配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql数据库主从复制学习笔记]]></title>
    <url>%2F2018%2F11%2F02%2Fmysql-master2slave.html</url>
    <content type="text"><![CDATA[Mysql数据库主从复制学习笔记概述 主从复制是指一台服务器充当主数据库服务器，另一台或多台服务器充当从数据库服务器，主服务器中的数据自动复制到从服务器中。 Mysql主从复制的基础是主服务器对数据库修改记录二进制日志，从服务器通过主服务器的二进制日志自动执行更新。 Mysql使用主从复制，可以作为一种热备份，还可以用来做读写分离，均衡数据库负载。 Mysql主从复制解决了数据分布、负载均衡、备份、高可用性和容错性的问题。 Mysql主从服务器最多不超过9台，推荐不超过5台。 主从复制的类型基于语句复制 主服务器上执行的语句从服务器再执行一遍，Mysql默认使用基于语句的复制。 存在问题：在时间上可能不能完全同步造成偏差，执行语句的用户也可能不是同一个用户。 基于行复制 把主服务器中修改的内容直接复制到从服务器中，不关心到底修改的内容是由哪条语句引发的。 存在问题：如果修改了数据表中每行的内容，那么需要复制数据表中所有的内容到从服务器中，造成比较大的开销。 混合类型的复制 Mysql默认使用基于语句的复制，当基于语句的复制会引发问题是，就使用基于行的复制，Mysql会自动进行选择。 在主从复制架构中，读操作可以在所有的服务器中执行，而写操作只能在主服务器中执行。主从复制架构虽然对读操作提供了扩展，但是，如果写操作比较多，单主模型的复制中主服务器势必会成为性能瓶颈。 主从复制工作原理 主服务器Master将改变记录到二进制日志Binary log中； 从服务器Slave将主服务Master的二进制日志事件复制到它的中继日志Relay log中； 从服务器Slave重做中继日志中的事件，将改变反应到自己的数据库中； 一主多从的复制模式中，主服务器既要负责数据的写入，又要负责为从服务器提供二进制日志，那么主服务器将面临更多的负担。 在一主多从的复制模式中，可以将主服务器中的二进制日志复制到某一个从服务器中，该从服务器再开启二进制日志事件，并将自己的二进制日志发送给其他从服务器或者该从无服务器不开启二进制日志事件，只负责把二进制日志转发给其他从服务器，这样的架构性能更好，而且数据之间的延时也更好。 主从复制的过程 Mysql主从复制有：同步复制、异步复制、半同步复制。3 异步复制：事物首先从主节点上提交，然后复制给从节点，并在从节点上应用。这就代表着在同一个时间点主从服务器上的数据可能不一致，异步复制的好处是比同步复制要快，如果对数据的一致性要求很高，最好采用同步复制。 半同步复制：半同步复制在多个Slave节点中选取一个节点进行半同步复制。也就是说，当主服务器提交一个事物时，在这个这个半同步复制的Slave端返回一个同步完成的Ack包之后，主服务器才会向用户返回事务提交成功，其他的Slave则采用传统的异步方式进行同步。4 半同步复制基于异步复制模式的，在配置半同步模式之前需要先配置异步复制。 半同步复制可以保证在主节点发生故障的时候，至少有一个从服务器与主服务器的数据一样，这样在进行切换时，可以更加快速的将该Slave设置成Master来使用。 二进制日志的模式5 二进制日志Binlog有3种模式：row、statement、mixed； row模式 日志中会记录成每一行数据被修改的格式,然后在slave端再对相同的数据进行修改，只记录要修改的数据，只有value，不会有sql多表关联的情况； row模式的日志清楚的记录下每行数据修改的细节，比较容易理解，不会出现某些情况下的存储过程、trigger的调用和触发无法被正确复制的问题。但是在该模式下，所有执行过的语句都被记录到日志中，日志将根据每行的修改来记录，这样就会产生大量的日志内容； statement模式 每一条会修改数据的sql语句都会记录到二进制日志文件中，在Slave复制的时候sql进程会解析成和Master端相同的sql语句再执行。在这种模式下，解决了row模式的缺点，不需要记录每行数据的变化，减少了日志量，节省了I/O、存储资源，提高了性能。但是，在这种模式下，为了让Slave端执行相同的sql语句得到与Master端相同的结果，二进制日志文件必须要记录执行语句的上下文信息，这就对mysql的复制造成了不小的吊炸你，自然复制的时候涉及到越复杂的内容，bug就越容易出现； mixed模式 mixed模式是row和statement模式的混合模式，在mixed模式下，mysql会根据执行的每一条具体的sql语句来区分对待二进制日志文件的记录形式，它会在row和statement模式之间自动选择。启用该日志模式，需要在配置文件中添加binlog_format=mixed Tips Mysql如果要使用主从复制，主服务器需要开启Binlog二进制日志事件功能，从服务器需要开启Relay-log日志中继功能； 二进制日志只记录主服务器内容更改的语句，对查询语句不做记录； 主服务器需要建立从服务器账号； 从服务器要配置master.info ; 主从复制时，主服务器有一个I/O线程，从服务器有一个I/O，一个SQL线程； 参考 https://segmentfault.com/a/1190000008942618 https://blog.csdn.net/hguisu/article/details/7325124 http://blog.51cto.com/junwang/1424711 https://www.jianshu.com/p/d877cbe9f0f0 https://blog.csdn.net/keda8997110/article/details/50895171]]></content>
      <categories>
        <category>Mysql</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>Mysql</tag>
        <tag>主从复制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx配置文件location标签的学习笔记]]></title>
    <url>%2F2018%2F11%2F01%2Fnginx-location.html</url>
    <content type="text"><![CDATA[Nginx 配置文件中location标签的学习语法规则12location [/|=|~|~*|^~] url &#123;&#125;location @name &#123;&#125; 修饰符 / 通配符，如果没有找到任何匹配规则，则执行该项； = 表示精确匹配。只有请求的url路径与后面的字符完全匹配时，才会执行； ～ 表示该规则是使用正则表达式，区分大小写 ； ~* 表示该规则时使用正则表达式，不区分大小写 ; ^~ 表示如果该符号后面的字符是最佳匹配，采用该规则，不再进行后续的查找,该项不是正则表达式； URL匹配过程 location有两种表达形式：前缀字符\正则规则 匹配过程 首先检查使用前缀 字符定义的location ,选择最长匹配项并记录下来； 如果找到了精确匹配的location ，也就是使用了= 的location，则执行该项，结束查找； 如果没有精确匹配，则开始寻找使用正则表达定义的location,如果找到匹配项，则执行，结束查找； 如果没有找到匹配的正则location,则使用前面记录的最长匹配前缀字符location ; Tips 使用正则表达的location 按照出现的先后顺序查找，优先执行先匹配到的正则location ; 使用精确匹配= 可以提高查找速度； 实例说明12345678910location = / &#123;A&#125; # 使用精确匹配，请求/，则执行A，不再往下查找location / &#123;B&#125;# 请求/index.html,首先查找匹配的前缀字符，找到最长匹配是B，继续查找正则表达的location，没有找到正则表达的，则执行最长匹配Blocation /user/ &#123;C&#125;# 请求/user/index.html，首先查找前缀字符，找到最长匹配是C，继续查找正则表达，没有找到正则表达的location，则执行最长匹配Clocation ^~ /images/ &#123;D&#125;# 请求/images/1.jpg ，首先查找前缀字符，找到最长匹配D，因为该location使用了字符^~，所以最佳匹配为D，不再继续查找location ~* \.(gif|jpg|png)$ &#123;E&#125; # \是转义字符，$表示以某字符结尾# 请求/user/2.gif,首先查找前缀字符，找到最长匹配C，继续查找正则表达，找到匹配E，则执行E location @的用法 前面我们看到了location 有2种表达的方式，location @用来内部重定向，不能用来处理正常的请求； 用法：123456location / &#123; try_files $url $url/ @inside # 当尝试访问url url/时，找不到对应的文件，则重定向到@inside&#125;location @inside &#123; ........&#125; location扩展 临时跳转 临时需要将原有的url跳转到新的url，可以使用精确匹配，并将其放置在其他location之前 location /admin {return 302 http://newurl/;} 访问控制 有一些目录为了安全，我们想限制访问，仅允许某些ip地址访问，如需使用此功能，nginx需要安装ngx_http_stus_status_module模块12345678location /admin &#123; stub_status; # 开启ngx_http_stub_status_module模块； allow 127.0.0.1; allow ip1; allow ip2; deny all; &#125; 列出目录 1234567location ^~ /filesys &#123; autoindex on; # 开启目录索引 autoindex_exact_size off; # 默认为on，显示文件的确切大小，单位为byte； # 改为off后，显示文件的大概大小，单位为kb、MB、GB autoindex_localtime on; # 默认为off，显示的文件时间为GMT时间； # 改为on后，显示的文件时间为文件服务器的时间&#125; 参考 https://segmentfault.com/a/1190000013267839 https://segmentfault.com/a/1190000013980557]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>Nginx</tag>
        <tag>配置文件</tag>
        <tag>location</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx负载均衡策略]]></title>
    <url>%2F2018%2F11%2F01%2Fnginx-ha.html</url>
    <content type="text"><![CDATA[Nginx负载均衡策略概要 Nginx工作在OSI的第七层，可以这对http应用做一些分流策略； Nginx反向代理服务的核心主要是转发Http请求，扮演了浏览器后端和后端服务器中转的角色； Nginx官方测试支持5万并发连接，在实际生产环境中可以到2-3万并发数连接，1万个非活跃http keep-alive连接占用约2.5M内存。3万并发连接下，10个Nginx进程，消耗内存约150M； 负载均衡的目的是为了解决单个节点压力过大，导致Web服务响应慢的问题； 内置负载策略策略 轮循(round-robin)默认策略 根据请求次数，将每个请求均匀分配到每台服务器,如果后端服务器宕机，自动剔除。 权重(Weight) 把请求更多的分配到高配置的后端服务器上，默认每个服务器的权重都是1。 ip_hash 同一客户端的Web请求被分发到同一个后端服务器进行处理，使用该策略可以有效的避免用户Session失效的问题。该策略可以连续产生1045个互异的value，经过20次hash仍然找不到可用的机器时，算法会退化成轮循。 最少连接(last_conn) Web请求会被转发到连接数最少的服务器上。 参数说明 weight 启用权重策略，总数按照10进行计算，如果分配为3，则表示所有连接中的30%分配给该服务器,默认值为1； max_fail/fail_time 某台服务器允许请求失败的次数，超过最大数后，在fail_timeout时间内，新的请求不会分配给这台机器，如果设置为0，反向代理服务器则会将这台服务器设置为永久无效状态。fail_time默认为10秒； backup 将某台服务器设定为备用机，当列表中的其他服务器都不可用时，启用备用机 down 将某台服务器设定为不可用状态 max_conns 限制分配给某台服务器的最大连接数，超过这个数量，反向代理服务器将不会分配新的连接，默认为0，表示不限制； 代码123456789101112131415http &#123; upstream server_group_name &#123; # ip_hash; # 启用ip_hash策略 # last_conn; #启用最少连接策略 server ip or domain:port weight=2 max_fails=3 fail_timeout=15 max_conns=1000; # 使用weight设置权重为20% server ip or domain:port backup; # 设置为备用机，当其他服务器全部宕机时，启用备用服务器 server ip or domain:port down; # 设置服务器为不可用状态 &#125; server &#123; listen 80; location / &#123; proxy_pass http://server_group_name; &#125; &#125;&#125; 扩展策略策略 扩展策略默认不被编译进nginx内核，如果启用该策略，需要自行编译安装 fair 根据后台服务器的响应时间判断负载情况，从中选出负载最轻的后端服务。但是在实际请款中，网络环境往往不那么简单，所以慎用。 在编译安装后，如果需要启用该策略，需要在upstream标签中添加fair;,启用该策略后，加权轮循将失效。 url_hash 按照请求url的hash结果来分配请求，试每个url定向到同一个后端服务器，在1.7.2之后的nginx版本中，该模块应集成到内核中，不需要单独安装。 启用该策略，需要在upstream标签中添加hash $request_url; 问题 使用Nginx的反向代理，让同一个用户的请求一定转发到同一台服务器上，这种均衡策略会消耗更多的服务器资源，也增加了代理服务器的负担； 使用其他策略作为负载均衡时，会出现用户Session丢失的情况，为避免出现这种情况，可以将用户的Session存放到缓存服务器中，比较常用的方案时redis/memchache； 反向代理服务器也可以开启缓存服务，但是开启该项服务会增加代理服务器的负担，影响整体的负载均衡效率； 使用Nginx反向代理布置负载均衡，操作相对金丹，但是会有“单点故障”的问题，如果后台某台服务器宕机，会带来很多的麻烦，后期如果后台服务器继续增加，反向代理服务器会成为负载均衡方案的瓶颈。 参考 https://juejin.im/post/5821c24e570c350060bef4c3 https://www.jianshu.com/p/ac8956f79206 https://segmentfault.com/a/1190000014483200 https://www.kancloud.cn/digest/understandingnginx/202607]]></content>
      <categories>
        <category>Nginx</category>
        <category>负载均衡</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>Nginx</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix-Agent使用主动模式与Server连接]]></title>
    <url>%2F2018%2F10%2F30%2Fzabbix-agent-model.html</url>
    <content type="text"><![CDATA[Zabbix-Agent主动模式Agent主动与被动模式 被动模式 zabbix-agent会监听10050端口，等待server段的监控信息收集请求 当被监控端数量增加后，web操作会出现卡顿和502、图层断裂、数据丢失的现象 主动模式 agent会主动收集信息并通过10050端口将信息传送到server段的10051端口 打开主动模式(Agent段操作) cd /etc/zabbix &amp;&amp; vi zabbix_agentd.conf 1234567Server=x.x.x.x # 如果使用纯主动模式，则需要将该行注释掉StartAgents=0 # 数值范围为0-100,0表示关闭被动模式SeverActive=x.x.x.x #主动模式的Zabbix-Server ip地址Hostname=hostname #hostname名称需要与Zabbix-Web中添加的主机名称对应，否则会出错RefreshActiveChecks=120 # 被监控端到服务器获取监控项的周期，默认为120s即可BufferSize=200 # 被监控端存储监控信息的空间大小Timeout=10 # 超时时间 systemctl restart zabbix-agent #重启agent 设置主动监控模式的监控模板 完整复制原有的模板 将复制的模板中的监听项目的模式修改为Agent Active，复制的模板会链接到其他模板，可以复制连接到的模板，将监控选项修改为Active模式，并重新链接 添加监控主机 查看是否更新监控数据]]></content>
      <categories>
        <category>Zabbix</category>
      </categories>
      <tags>
        <tag>Zabbix</tag>
        <tag>Zabbix-Agent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Docker-compose搭建Zabbix开源监控系统]]></title>
    <url>%2F2018%2F10%2F30%2Fzabbix-docker.html</url>
    <content type="text"><![CDATA[使用Docker-Compose搭建Zabbix系统编辑docker-compose文件 新建一个文件夹zabbix,并创建一个名为zabbix.yml的文件 mkdir zabbix &amp;&amp; cd zabbix &amp;&amp; touch zabbix.yml 根据docker-compose的规范编辑yml文件 详细设置 zabbix.tar.gz - 注意事项 1. 在yml文件中不能使用`Tab`进行分隔； 2. 在符号`:`后接空格； 3. 在对`zabbix-server` `zabbix-web`的环境变量进行设置时，变量的内容不需要使用符号`&quot;&quot;` ，否则进行数据库连接时出错； 4. 首次登录wen界面时，如果出现数据库配置错误的信息，可以进入`mysql-server`的docker容器内，查看`zabbix`数据库中`users`表中的数据是否为空，如果为空，需要在yml文件中将本地包含zabbix数据库初始化的文件夹映射到mysql容器中，然后进入容器内，使用`source`命令初始化zabbix数据库的表结构； - ![database error.png][2] - `docker exec -it mysql-server bash` - `mysql -uroot -p` - `use zabbix;` - `source /home/create.sql;` #开始会出现错误提示，部分表和字段已经存在，可忽略，之后会进行其他表和字段的创建，等待完成 5. 注意设置正确的端口映射，`zabbix-server`使用`10051`端口，`zabbix-web`使用`80`和`443`端口，`mysql-server`使用`3306`端口，但是可以不映射到本机，仅限在容器间使用。 使用docker-compose创建容器 docker-compose -f ./zabbix.yml up -d # -d表示后台运行 等待命令完成，当出现done字符时，表示容器已经启动 登录web界面 http://servername Zabbix默认的用户名和密码是:Admin/zabbix]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>容器</tag>
        <tag>Zabbix</tag>
        <tag>Docker-compose</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Docker-compose构建Nginx-PHP-Mysql环境]]></title>
    <url>%2F2018%2F10%2F30%2Flnmp-docker.html</url>
    <content type="text"><![CDATA[使用Docker-Compose搭建nginx+php+mysql基础应用PHP 为了能连接mysql数据库，php还需要安装相关的插件 首先需要建立docker-php目录 mkdir docker4php 创建Dockerfile vi Dockerfile 添加以下内容1 1234FROM php:7.1-fpm-alpine Run apt-get update \&amp;&amp; apt install iputils-ping \&amp;&amp; docker-php-ext-install mysqli &amp;&amp; docker-php-ext-enable mysqli Mysql- 从Docker Hub拉取最新版的mysql镜像 &lt;sup&gt;2&lt;/sup&gt; - `sudo pull mysql` Docker-Compose- 下载对应系统版本的docker-compose，上传到服务器，并添加执行权限；除此之外，还可以使用脚本安装&lt;sup&gt;3&lt;/sup&gt; 使用docker-compose创建项目4 建立项目结构 123456mkdir npm4composecd npm4composemkdir conf.d php html &amp;&amp; touch docker-compose.ymlcd conf.d &amp;&amp; touch nginx.confcd html &amp;&amp; touch index.php &amp;&amp; echo "&lt;?php phpinfo(); ?&gt;" &gt;index.phpcp ~/docker4php/Dockerfile ./php/ 目录结构如下 12345678npm4compose/|—— conf.d #nginx配置文件目录 |—— nginx.conf #自定义nginx配置文件|—— docker-compose.yml # compose文件|—— html #网站根目录 |—— index.php|—— php #php目录 |—— Dockerfile 编辑docker-compose.yml文件5 6 123456789101112131415161718192021222324252627282930313233343536version: '3'services: nginx: image: nginx:lastest ports: #端口映射 - "80:80" depends_on: #依赖关系，需要先运行php - "php" volumes: - "$&#123;PWD&#125;/conf.d:/etc/nginx/conf.d" #将宿主机上nginx配置文件映射到容器中 - “$&#123;PWD&#125;/html:/usr/share/nginx/html” #映射网站根目录 networks: - d_net container_name: "compose-nginx" #容器名称 php: build: ./php #指定build Dockerfile生成镜像 image: php:7.1-fpm-alpine ports: - "9000:9000" volumes: - "$PWD/html:/var/www/html" networks: - d_net: container_name: "compose-php" mysql: image: mysql:8.0 ports: - "3306:3306" environment: - MYSQL_ROOT_PASSWORD=&#123;your_passwd&#125; networks: - d_net container_name: "compose-mysql"networks: #配置docker 网络 app_net: driver: bridge 配置本地nginx.conf文件7 12345678910111213141516171819server&#123; listen 80; server_name localhost; location /&#123; root /var/www/html; index index.php index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /var/www/html; &#125; location ~ \.php$ &#123; include fastcgi_params; fastcgi_pass php:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /var/www/html/$fastcgi_script_name; &#125; &#125; 运行docker-compose，docker-compose up -d 参考 docker-php Tags docker-mysql docker-compose docker-compose for nginx php mysql compose yml file compose yml file for CN nginx config document docker q&amp;a]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>容器</tag>
        <tag>Docker-compose</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS安装Docker容器及基础使用]]></title>
    <url>%2F2018%2F10%2F30%2Fcentos-docker-doc.html</url>
    <content type="text"><![CDATA[CentOS 安装Docker使用repository安装 设置repository 安装docker需要yum-utils yum-config-manager device-persistent-data lvm2 工具的帮助，所以首先要安装所需要的工具 sudo yum install -y yum-utils device-mapper-persistent-data lvm2 启用docker stable安装源 sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 如果需要启用edge和test安装源，可以分别启用如下安装源,,，默认为关闭 sudo yum-config-manager --enable docker-ce-edge or replace with docker-ce-test 安装docker sudo yum install docker-ce 如果需要安装某个特殊版本的docker，可以使用以下命令列出系统支持的docker版本 sudo yum list docker-ce --showduplicates | sort -r sudo yum install docker-ce-&lt;version string&gt; # 安装制定的版本 等待安装完成，完成后启用docker服务 sudo systemctl start docker sudo systemctl status docker -l # 查看docker启动信息 添加国内的docker仓库镜像源 cd /etc/docker/ # 如果没有该文件，可以单独创建 sudo cp daemon.json daemon.json.old sudo vi daemon.json 将以下信息加入到该文件张，注意格式是否正确，否则影响docker的启动 123&#123; "registry-mirrors": ["https://registry.docker-cn.com"]&#125; 使用rpm包安装docker 下载系统对应rpm格式的docker包 下载地址：https://download.docker.com/linux/centos/7/x86_64/stable/Packages/ 将下载的安装包上传到服务器中 scp ./docker-version-string.rpm user@centos:~/docker 登录到服务器，执行安装程序 sudo yum install ~/docker/docker-version-string.rpm 安装程序完成后，会在系统中创建一个docker用户组，但是该用户组中无用户，需要将系统中的用户添加到组中 sudo usermod -a -G docker user 按照同样的步骤分别修改国内仓库镜像和重启docker服务 参考 docker官方安装指导 https://docs.docker.com/install/linux/docker-ce/centos/#uninstall-docker-ce 原文地址：http://baby-time.cn/index.php/note/87.html]]></content>
      <categories>
        <category>CentOS</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>Docker</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql数据库修改root用户密码]]></title>
    <url>%2F2018%2F10%2F30%2Fchange-mysql-root-passwd.html</url>
    <content type="text"><![CDATA[修改Mysql root用户密码 在CentOS中使用yum安装的Mysql，root用户的密码默认为空，首先使用root用户登录mysql mysql -u root 进入mysql数据库后，使用mysql数据库 mysql>use mysql; 更新表中的user字段 123mysql&gt; update set password=password(&apos;newpasswd&apos;) where user=&apos;root&apos;;mysql&gt; flus privileges;mysql&gt; exit; 重启mysql服务 sudo systemctl restart mysql]]></content>
      <categories>
        <category>Mysql</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KVM虚拟机常用命令]]></title>
    <url>%2F2018%2F10%2F30%2Fkvm-cmd.html</url>
    <content type="text"><![CDATA[KVM 维护常用命令 kvm虚拟机的配置文件位置：/etc/libvirt/qemu 查看虚拟机的配置情况sudo virsh dominfo vm_name 修改虚拟机的相关配置 sudo virsh edit virt_host_name 备份虚拟机的配置文件 sudo virsh dumpxml virt_host_name &gt; backup _path/virt_host_name_backup.xml 查看正在运行的虚拟机 sudo virsh list [-all] 启动虚拟机 sudo virsh start virt_host 关闭、重启虚拟机 如果使用virsh关闭或重启虚拟机，需要在虚拟机中安装acpi scpid-sysvinit 2个软件包,并启动相关的服务 sudo virsh shutdown|reboot virt_host 强制关机与挂机、恢复 sudo virsh destroy|suspend|resume virt_host 移除虚拟机，该方法只删除虚拟机的配置文件，磁盘文件保留 sudo virsh undefine virt_host sudo virsh define virt_host_new.xml #导入虚拟机 彻底删除虚拟机 sudo virsh destroy virt_host #强制关闭 sudo virsh undefine virt_host #解除标记虚拟机 删除虚拟机的磁盘文件 开机启动虚拟机 sudo virsh autostart virt_host sudo virsh autostart --disable virt_host #取消开机启动 克隆虚拟机 sudo virt-clone -o virt_host -n new_host -f /disk path/new.qcow2 虚拟机快照 创建虚拟机快照，要求虚拟机的磁盘格式为qcow2，如果不是，需要使用qemu-img 进行转换 sudo qemu-img info virt_host #查看虚拟机磁盘格式 sudo qemu-img convert -f raw disk.raw -o qcow2 convert_new.qcow2 sudo qemu-img create -f qcow2 /disk_path/name.qcow2 size #新建一个虚拟机镜像磁盘 sudo virsh attach-disk virt_host_name /disk_path/name.qcow2 vdb --cache=none --subdriver=qcow2 #在线追加虚拟机镜像磁盘 创建快照 sudo virsh snapshot-create virt_host #创建的快照名称为默认格式 sudo virsh snapshot-create-as --domain kvm_host --name &quot;snapshot_name&quot; 查看快照 sudo snapshot-list virt_host 恢复快照 sudo snapshot-revert virt_host snapshot_name 删除快照 sudo virsh snapshot-delete virt_host snapshot_name 添加网卡 live添加,重启不失效sudo virsh attach-interface --domain vm1 --type bridge --source br1 --model virtio --config --live 重启后生效sudo virsh attach-interface --domain vm1 --type bridge --source br1 --model virtio --config 删除网卡sudo virsh detach-interface --domain vm1 --type bridge]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
        <tag>虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Debian9安装KVM虚拟化工具]]></title>
    <url>%2F2018%2F10%2F30%2Fdebian-kvm.html</url>
    <content type="text"><![CDATA[Debian9 安装KVM系统信息 sudo lsb_release -a 1234Distributor ID: DebianDescription: Debian GNU/Linux 9.5 (stretch)Release: 9.5Codename: stretch 查看CPU是否支持虚拟化 cat /proc/cpuinfo vmx //Inter svm //AMD 安装QEMU、KVM sudo apt install qemu-kvm libvirt-clients libvirt-daemon-system bridge-utils libguestfs-tools virtinst libosinfo-bin 将当前用户添加到libvirt libvirt-qume组中 sudo usermod -a -G libvirt user sudo usermod -a -G libvirt-qume 创建桥接网络 123456789101112cd /etc/network/interfaces.dsudo touch br0 &amp;&amp; sudo vi bro//[insert]auto br0iface br0 inet staticaddress 192.168.0.101netmask 255.255.255.0gateway 192.168.0.1bridge_ports enp0s31f6bridge_stp offbridge_waitport 0bridge_fd 0 重启网络 sudo systemctl restart network-manager sudo systemctl restart networking 查看kvm网络 sudo virsh net-list --all 查看本机桥接网络 sudo brctl show 在kvm中找不到br0网卡，但是在kvm虚拟机中可以连通到网络 查看网络配置 ip addr 如果查看不到创建的桥接网络，需要reboot 添加kvm桥接网卡2 新建一个xml文件sudo touch /var/kvm/bridge,xml 将网卡定义写进xml文件中sudo vi /var/kvm/bridge.xml 12345&lt;network&gt; &lt;name&gt;br1&lt;/name&gt; &lt;forward mode=&quot;bridge&quot; /&gt; &lt;bridge name=&quot;br1&quot; /&gt;&lt;/network&gt; 在kvm中定义网卡sudo virsh net-define --file /var/kvm/bridge.xml 设置自启动并启用该网卡sudo virsh net-autostart br1 &amp;&amp; sudo virsh net-start br1 创建虚拟机 12345678910111213cd ~ &amp;&amp; mkdir kvmiso #用于存放iso镜像mkdir kvmimg #用户存放安装后的img镜像sudo virt-install \--virt-type kvm \--name Debian-kvm \--memory 1024 \ #单位为M--vcpus 1 \--os-variant debian9 \--hvm \ #请求全虚拟化--cdrom /home/user/kvmiso/debian9.iso \--network bridge=br0,model=virtio \ \#直接桥接到宿主机的br0网卡上--graphics vnc \--disk path=/home/user/kvmimg/debian-kvm.qcow2,size=30,bus=virtio,format=qcow2 \ #常用格式有raw\qcow2\vmdk 特别注意事项：在创建镜像磁盘时，如果虚拟机是Linux，磁盘bus可以使用virtio，如果是Windows则会出现找不到磁盘的问题，可以将bus修改为ide 创建完成，开启虚拟机 sudo virt-viewer centos1 如果需要使用vnc，需要在宿主中安装vnc viewer,通过virsh dumpxml centos1 | grep vnc 命令查看vnc的端口号，然后使用vnc viewer连接到图形虚拟机 参考 https://wiki.debian.org/KVM https://www.cyberciti.biz/faq/install-kvm-server-debian-linux-9-headless-server/]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>Debian</tag>
        <tag>KVM</tag>
        <tag>虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS源码安装Msql数据库]]></title>
    <url>%2F2018%2F10%2F30%2Fcentos-mysql-sources.html</url>
    <content type="text"><![CDATA[CentOS通过源码安装Mysql 下载mysql源码和boost文件，并上传到服务器中 下载地址：http://ftp.jaist.ac.jp/pub/mysql/Downloads/MySQL-8.0/ ncurses下载地址：https://ftp.gnu.org/pub/gnu/ncurses/ 创建myql所需要的用户和用户组 sudo groupadd mysql sudo useradd -r -g mysql -s /bin/false mysql 创建mysql数据库存放位置 cd /var/ &amp;&amp; sudo mkdir mysqldb sudo chown -R mysql:mysql mysqdb 解压源码安装包，并进入文件目录,制定boost路径，在该文件夹中存放的是boost压缩包，文件名为boost_1_67_0.tar.gz 12345678sudo yum install numactl-devel ncurses-develmkdir bldsudo cmake ../ -DCMAKE_INSTALL_PREFIX=/usr/local/mysql \-DSYSCONFDIR=/etc/mysql/ \-DMYSQL_DATADIR=/var/mysqldb \-DWITH_BOOST=boost/sudo make -j 20 #使用多线程编译sudo make install 将mysql添加到环境变量中 sudo export PATH=${PATH}:/usr/local/mysql/bin 安装mysql服务，并开机启动 12345sudo chown -R mysql:mysql /usr/local/mysqlsudo cp ~/mysql/support-files/mysql.server /etc/init.d/mysqldsudo chmod +x mysqldsudo chkconfig --add mysqlsudo chkconfgi mysql on 创建mysql配置文件 12345678910111213141516171819202122[mysqld]datadir=/var/mysqldbsocket=/var/run/mysql.sockuser=mysql symbolic-links=0 [mysqld_safe]log-error=/var/log/mysql_error.logpid-file=/var/run/mysql.pidkey_buffer_size = 8144Mtable_cache_size = 1024Mread_buffer_size = 128Msort_buffer_size = 32Mquery_cache_size = 100Mthread_cache_size = 16thread_concurrency = 32max_heap_table_size = 400Mtmp_table_size = 400Mmax_connections = 500# The end# 安装mysql数据库文件 1234cd /usr/local/mysql/binsudo ./mysqld --initializa -user=mysql --datadir=/var/mysqldbsudo ./mysql_ssl_rsa_setupsudo ./mysqld_safe --user=mysql &amp; 参考 cmake参数说明:https://dev.mysql.com/doc/refman/5.7/en/source-configuration-options.html#cmake-option-reference http://shihlei.iteye.com/blog/2296886 https://dev.mysql.com/doc/refman/5.6/en/installing-source-distribution.html mysql官方安装指导：https://dev.mysql.com/doc/refman/8.0/en/binary-installation.html]]></content>
      <categories>
        <category>CentOS</category>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>Mysql</tag>
        <tag>源码安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iOS平刷工具-Semi-Restore]]></title>
    <url>%2F2018%2F10%2F30%2Fios-semi-restore.html</url>
    <content type="text"><![CDATA[IOS 平刷工具Semi-Restore 工具 Semi-Restore 下载地址：https://semi-restore.com 支持的操作系统有Windows OSX Linux ,系统要求必须为64位 支持的IOS版本 IOS 5.0-9.1 Tips 该平刷工具用于越狱后的设备，即可以保留设备原有的系统版本，同时删除设备的所有数据，恢复到出厂设置； 未越狱的设备，可以直接使用设备自带的擦除全部数据来重置设备； 越狱后的设备，不可以使用擦除全部数据来重置设备，使用该项操作后，重启会出现白苹果的现象。 平刷步骤 本操作以Windows 7为例，首先需要在系统中安装.NET Freamwork 4.0+和Visual C++ Redistributable for Visual Studio 2015 和Itunes12+ .NET下载地址 https://www.microsoft.com/en-us/download/details.aspx?id=53344 Visual运行库下载地址 https://www.microsoft.com/en-us/download/confirmation.aspx?id=48145 将下载的Semi-Restore 工具解压到文件夹中； 将设备通过数据线与电脑连接，连接成功后需要关闭ITunes，同时需要关闭设备的锁屏密码； 打开解压后的文件夹，找到文件SemiRestore9，右键以管理员身份运行，打开平刷工具； 系统中所有必须工具全部安装正确，并且设备已经连接到电脑，打开平刷工具后，会出现设备的相关信息； 点击右侧SemiRestore按钮开始平刷，在平刷过程中，设备会出现多次重启的现象，在平刷过程中切勿操作设备 ； 平刷完成后，会出现SemiRestore Complete 的提示，至此，平刷完成； 打开设备，进行相关的初始设置。]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux-Shell学习笔记-输入输出|管线|排序]]></title>
    <url>%2F2018%2F10%2F30%2Flinux-shell-stip.html</url>
    <content type="text"><![CDATA[数据流重导向 标准输入 &lt;或&lt;&lt; &lt;&lt; 终止输入 标准输出 &gt;或&gt;&gt; 2者的区别是&gt;若文件不存在，则创建新的文件，若文件存在，则覆盖文件；&gt;&gt;不覆盖新的文件，在原来的文件上累积数据。 标准错误输出 2&gt; 或2&gt;&gt; 特殊用法 标准输出和标准错误输出，全部存入同一个文件command filename &gt; file 2&gt;&amp;1 命令执行判断 不考虑命令相关性，连续执行 cmd1;cmd2 指令之间彼此有相关性，前一个指令是否成功执行与后一个指令是否要执行有关 cmd1 &amp;&amp; cmd2 or cmd1 || cmd2 &amp;&amp; 若cmd1执行完毕并且正确执行，则cmd2开始执行； || 若cmd1执行正确，则cmd2不执行； 管线命令 管线命令| 仅能处理经由前面一个指令传来的正确信息，对于标准错误信息无法进行处理。 关联命令后面接的第一个数据必须是命令，并且这个命令必须能够接受标准输入。 常用的管线命令 撷取命令cut grep 所谓撷取命令，就是将一段数据经过分析或分析关键字，取出我们需要的一行信息。撷取信息是针对一行一行来分析的 cut cut -d &#39;分割字符&#39; -f num1,num2 //根据分割字符，将一行数据进行分割，并取出第num段数据 cut -c 字符区间 num1-num2 cut 命令分析有多个空格的数据时，会比较吃力。 grep grep是分析一行数据，如果该行中有我们需要的，则把该行信息输出；cut 是将该行中的一部分数据输出。 grep [-acinv] [--color=auto] &#39;搜寻字符串&#39; filename -a 将binary文件以text文件的方式搜寻 -c 计算找到搜寻字符串的次数 -i 忽略大小写 -n 输出行号 --v 反选，输出没有搜寻字符串的那行信息 --color=auto 将找到的关键字部分加上颜色显示 排序命令 sort sort [-fbMnrtuk] [file or stdin] 12345678-f //忽略大小写-b //忽略最前面的空白字符-M //以月份的名称排序-n //以纯数字进行排序- r //反向排序-u //相同的数据中仅出现一行代表-t //分割符号，默认喂[tab]- k //以第num个区间进行排序 uniq 重复数据仅显示一个，一般配合sort使用 uniq [-ic] -i 忽略大小写 -c 计数 wc wc [lwm] -l 仅列出行 -w 仅列出多少个英文单字 -m 多少字符 三列数据分别代表行数、字数、字符数]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>Linux</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Samba共享文件服务器-用户验证登录]]></title>
    <url>%2F2018%2F10%2F30%2Fsamba-account.html</url>
    <content type="text"><![CDATA[Samba设定需要密码访问资源 设定一个系统用户,并加入到系统原有账户kim的群组中 sudo useradd -G kim -d /home/smb_kim -s /bin/bash smb_kim //新建用户smb_kim，使用/bin/bash,home目录为/home/smb_home，群组为kim sudo passwd smb_kim //修改密码 设置配置文件smb.conf 1234567891011121314151617[global] workgroup = kimhome netbios name = kimserver server string = This is a file share server! log file= /var/log/samba/log.%m-%I@%T # 变量m代表客户端NetBiso name，变量I代表客户端ip地址，变量T代表当前系统时间 max log size = 50 load printers = no security = user passdb backend = tdbsam # 使用tdb数据库格式[share] comment = smb_kim\&apos;s share browseable = no # 除了使用者外，其他人不可浏览 writeable = yes create mode = 0664 # 建立文件的权限为664 directory mode = 0775 # 建立目录的权限为775 write list = @kim # kim 群组下的用户都可以使用 检查配置文件testparm 将系统中的用户添加到samba中 sudo pdbedit -a -u smb_kim //需要单独输入samba的密码 pdbedit -a|-r|-x -u user 分别代表增加|修改|删除用户 sudo pdbedit -L [-vw] //显示当前数据库中的账号信息 -v 搭配-L使用，列出更多信息 -w 搭配-L使用，使用旧版的smbpasswd格式显示数据]]></content>
      <categories>
        <category>Samba</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>Samba</tag>
        <tag>验证登录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Samba文件共享服务器基础知识学习]]></title>
    <url>%2F2018%2F10%2F30%2Fsamba-doc.html</url>
    <content type="text"><![CDATA[Samba服务器基础Samba服务 nmbd 用来管理工作组、NetBIOS name解析。利用UDP协议开启137\138端口解析名称。 smbd 用来管理samba主机分享的目录、档案和打印机等。利用可到的TCP协议传输数据，开放的端口是139\445 联机模式 peer-peer对等模式 局域网中的所有pc均可以在自己的计算机上面管理自己的账号和密码，同时每部计算机都具有独立执行各项软件的能力，只是由网络将各个pc链接在一起。 domain model主控模式 所有账号密码全都防止在一部主控计算机PDC中，在局域网中，在任何一台计算机中输入账号和密码，都可以根据身份使用不同的计算机资源。 软件 samba 提供samba服务器所需要的各项服务程序、文档、与samba相关的logrotate配置文件、开机默认选项档案。 samba-client samba客户端程序 samba-common 服务器与客户端都会使用到的数据，包括samba的主要配置文件、语法检验指令。 /etc/samba/smb.con samba的主要配置文件，主要设定项目是服务器相关设定global /etc/samba/Imhosts 功能类似/etc/hosts,设定NetBios name，目前samba预设会使用本机的名称作为NetBIOS name，可不用设定。 /etc/sysconfig/samba 提供启动smbd、nmbd时，需要加入的相关服务参数。 常用脚本服务器 /usr/sbin/{smbd,bmbd} 服务器功能，权限管理smbd，NetBIos name查询nmbd /usr/bin/{tdbdump,tdbtool} 服务器功能，tdbdump可以查看数据库的内容，tdbtool则可以进入数据库操作接口直接手动修改账号密码，但是需要安装软件tdb-tools /usr/bin/smbstatus 服务器功能，可以列出目前samba的联机状况 /usr/bin/{smbpasswd,edbedit} 服务器功能，管理samba用户账号和密码，后期建议使用pdbedit管理用户数据 /usr/bin/testparm 检验配置文件smb.conf语法是否正确，在编辑过配置文件时，务必使用testparm检查一次 客户端 /sbin/mount.cifs 使用mount.cifs将远程主机分享的档案文件与目录挂载到主机上。 /usr/bin/smbclient 用来查看其他计算机所分享的目录与装置，也可用在本机上查询samba设定是否成功。 /usr/bin/nmblookup 查询NetBIOS name /usr/bin/smbtree 查询工作组与计算机名称的树状目录分布图 update:2018-09-29 Samba设定流程 服务器设定，在文件smb.conf中设定好工作组、NetBios Name、密码使用状态(无密码分享|本机密码)。 规划准备分享的目录参数。 建立所需要的文件系统。 建立可用samba账号，建立所需的Linux实体账号，再以pdbedit建立使用samba的密码 启动服务smbd，nmbd。 。 smb.conf文件注意事项 符号# ;都是批注符号； 在该文件中不区分大小写； global项目 [global]中设定的是服务器的整体参数，包括工作组、主机的NetBios Name、字符编码的显示、登录文件的设定、是否使用密码以及使用密码验证的机制等； 主要参数 workgroup = 工作组名称 //主机群要相同 netbios name = 主机的NetBios name //每部主机均不相同 server string = 主机的简易说明 log file = 日志文件 //文件名可能需要使用变量处理 max log size = 最大日志文件kb 安全参数 security = share|user|domain //三选一 share 分享的数据不需要密码，大家均可以使用 user 使用samba服务器本身的密码数据库，密码数据库与底下的passdb backend有关 domain 使用外部服务器的密码，如果设定为该项，还需要配合password server = ip一同使用 encrypt passwords = Yes //代表密码需要加密 passdb backend = 数据库格式 //为了加快速度，目前密码文件已经使用数据库。默认的数据格式喂tdbsam ,预设的数据库文件存放在/var/lib/samba/private/passwd.tdb 资源分享参数 [分享名称] comment //目录说明 path //分享资源的实际目录 brosweable //是否让所有的用户看到这个目录 writeable //是否可以写入 read ony //是否为只读 如果read ony 与writeable同时出现，以最后出现的设定为主要设定 writelist = 用户|@群组 //指定能够进入到此资源的特定使用者，如果使用@群组则加入该群组的使用者均可取得使用权限 内置变量 %S 取代目前的设定项目值 检查 testparm使用该命令检查设置的smb.conf 是否存在错误 -v 参数可以查阅完整的参数设定，联通默认值也会显示 smbclient -L [//主机或ip] [-U 使用者账号] -L 仅查阅后面接的主机所提供分享的目录资源 -U 尝试使用账号来取得该主机的可用资源]]></content>
      <categories>
        <category>Samba</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>Samba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Shell学习笔记]]></title>
    <url>%2F2018%2F10%2F30%2Flinux-shell-doc.html</url>
    <content type="text"><![CDATA[Linux Shell学习笔记基本信息 系统：Debian 9.5 发行信息：Debian GNU/Linux 9.5/Stretch Shell基础update:2018年09月22日 只要能够操作应用程序的借口都能成为壳程序Shell。 文件/etc/shells中存放着用户可以使用的shell，/bin/bash是Linux默认的的shell ex:Debian9.5中可以使用的shell 文件/etc/passwd中存放着登录时取得的shell,每行的最后一个数据，就是该用户登陆后取得的默认shell ex:用户kim登录取得的shell为/bin/bash 命令历史记录 用户通过bash操作的记录都被记录到用户主目录下的.bash_history中，该文件中记录的是前一次登录以前所执行过的命令。本次登录所执行的命令都被暂存到内存中，当登出系统后，该用户的操作记录才会被记录到该文件中。 使用命令history可查看当前登录用户执行过的命令 命令别名设置 alias ll=&#39;ls -la&#39; //使用命令ll替代命令ls -la 如果下次登录该别名时效，还可以通过在文件～/.bashrc78行左右添加一条记录alias ll=&#39;ls-la&#39; 查询指令类型 使用命令type 可以查询shell指令是file or alias or builtin 可用的参数有- p -a -t //参数p仅在指令为外部指令时，显示完整文件名 指令的快速编辑 ctrl+u 从光标处向前删除指令 ctrl+k 从光标处向后删除指令 ctrl+a 从光标处移动到整个指令的最前面 ctrl+e 从光标处移动到整个指令的末尾 update:2018-09-25 Shell中的变量 输出变量内容使用echo,ex:echo $PATH or echo ${PATH} 变量规则 变量与变量内容以=连接，=两侧不能直接接空白字符 变量名称只能是英文字母和数字，但是不能以数字开头 变量内容若有空白字符，可以使用双引号&quot;或单引号&#39;将变量内容结合起来，二者的区别是：双引号内的特殊字符仍然保持原本的特性，单引号内的特殊字符仅为一般字符 可以使用转义字符\将特殊字符转换为一般字符 若指令中需要使用额外指令提供的内容，可以使用$(comm) or “`\comm `\”(数字1左侧的反引号) 若变量需要在其他子程序中执行，则需要以export使变量变成环境变量 通常大写字符为系统默认变量，自行设置变量可以用小写字符 取消变量使用unset 变量名 环境变量的功能 使用env查看环境变量与常见环境变量说明- 使用set查看所有变量(包含环境变量与自定变量) update:2018-09-26 变量键盘读取 read [-pt] 变量 p 后面可以连接提示符 t 后面可以连接等待时间 变量类型定义 declar/typeset [-aixr] 变量 -a 将变量定义为阵列类型[array] -i 将变量类型定义为整数数字[integer] -x 用法与export一样，将变量定义为环境变量 -r 将变量定义为只读不可更改内容[readonly] 终端可用资源 ulimit [-SHacdfltu] 变量内容的删除、取代与替换 变量内容的删除需要用到特殊字符# new=${old#*_} //删除变量old内容中的_及前面所有的内容，删除从最左侧开始]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>Linux</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vi编辑器常用命令]]></title>
    <url>%2F2018%2F10%2F30%2Fvi-cmd.html</url>
    <content type="text"><![CDATA[Vi编辑器易忘命令备忘移动 行下移动n行：nj，还可以使用n&lt;Enter&gt;//n代表数字,Enter代表按键 移动n个字符： n&lt;space&gt; //n代表数字，输入数字后按空格键 数字0移动到这一列最前面的字符，$移动到这一列最后面一个字符 G移动到文件最后一行 gg移动到文件首行，效果相当于1G 查找 /word 向光标之下寻找字符 ?word 向光标之上寻找字符 删除 x向后删除一个字符，X向前删除一个字符 nx向后连续删除n个字符 d$ 删除光标到本行最后一个字符 d0 删除光标到本行第一个字符 ndd 删除光标后的n行 //n代表数字 d1G 删除光标所在行到第一行的所有数据 dG 删除光标所在行到最后一行的所有数据 复制粘贴 yy复制光标所在行 nyy 复制光标后的n行数据 //n代表数字 p粘贴复制数据到光标下一行，P粘贴复制数据到光标当前行 重复与复原 u复原上一个动作 &lt;ctrl&gt;+r重做上一个动作 . 重复前一个动作 保存 ZZ 若文件没有变动则不储存离开，有变动则储存离开 //不需要进入命令模式 :w filename 将编辑的数据存储成另一个文件 //类似另存新文件 读入 :r filename 在编辑的数据中，读入另一个文件的数据在光标所在行后 vi file1 file2 同时打开多个文件 :n 将vi编辑器切换到下一个文件 //n表示字母n :N 将vi编辑器切换到上一个文件 :files 列出vi编辑器打开的文件列表 其他 ：！ command 暂时离开vi编辑器，执行另一个命令command :set nu 显示行号，:set nonu 取消显示行号 ：sp [filename] //分割vi编辑器，加filename则打开另一个文件，不同窗口间移动的命令ctrl+w然后再按j或k，离开需要再按q]]></content>
      <categories>
        <category>Linux</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>备忘</tag>
        <tag>Vi编辑器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下打包与压缩命令的使用]]></title>
    <url>%2F2018%2F10%2F30%2Ftar-format.html</url>
    <content type="text"><![CDATA[Linux 下打包压缩命令备忘常用打包压缩解压命令 *.tar.gz //使用gzip压缩，用tar打包 打包压缩命令：tar -czvf new.tar.gz files 解压命令： tar -xzvf new.tar.gz *.tar.bz2 //使用bzip2压缩，用tar打包 打包压缩命令： tar -cjvf new.tar.bz2 files 解压命令： tar -xjvf new.tar.bz2 *.tar.xz //使用xz压缩，用tar打包 打包压缩命令： tar -cJvf new.tar.xz files //参数中注意是大写的J 解压命令： tar -xJvf new.tar.xz [-C dirname] //可以添加参数-C dirname指定解压缩的目录单独解压压缩包中的某个文件 首先查看压缩包中的文件，并筛选出该文件，使用命令：tar -tjvf new.tar.gz | grep &#39;filename&#39; 使用解压命令，单独解压该文件，使用命令：tar -xjvf new.tar.gz dir/filename注意事项 参数c创建压缩包，参数x解压压缩包，参数t查看压缩包内部文件名； 使用gzip压缩打包时使用参数z,使用bzip时使用参数j,使用xz时使用参数J; 参数p可以用来保留备份数据的原本权限与属性，使用时与参数c同时使用； 在打包压缩命令前添加time可以显示程序运行的时间； 压缩率xz&gt;bz2&gt;gz，压缩率越高，需要的时间越多]]></content>
      <categories>
        <category>Linux</category>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>打包压缩</tag>
        <tag>备忘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS通过源码安装PHP]]></title>
    <url>%2F2018%2F10%2F30%2Fcentos-php-sources.html</url>
    <content type="text"><![CDATA[CentOS通过源码安装PHP 安装必要的库文件和工具 sudo yum install autoconf libtool re2c bison libxml2-devel bzip2-devel libcurl-devel libpng-devel libicu-devel gcc-c++ libmcrypt-devel libwebp-devel libjpeg-devel openssl-devel -y 下载PHP并上传到服务器中，解压到制定文件夹中 php下载地址 https://secure.php.net/downloads.php 上传到服务器中 scp ./php.tar.gz user@centos:~/php 登录到服务器中，进行相关的操作 ssh -i ./ssh_key user@centos cd ~/php &amp;&amp; tar-xzvf php.tar.gz 创建configure 文件 sudo ./buildconf --force 使用configure 1234567891011121314151617181920212223242526sudo ./configure \--prefix=/usr/local/php \--enable-fpm \--disable-short-tags \--with-openssl \--with-pcre-regex \--with-pcre-jit \--with-zlib \--enable-bcmath \--with-bz2 \--enable-calendar \--with-curl \--enable-exif \--with-gd \--enable-intl \--enable-mbstring \--with-mysqli \--enable-pcntl \--with-pdo-mysql \--enable-soap \--enable-sockets \--with-xmlrpc \--enable-zip \--with-webp-dir \--with-jpeg-dir \--with-png-dir 编译php文件并安装 sudo make clean &amp;&amp; sudo make &amp;&amp; sudo make install 配置php-fpm 1234567891011121314151617181920212223cd /usr/local/php/etc/sudo cp ./php-fpm.conf.default ./php-fpm.confsudo vi php-fpm.conf//[insert]include =etc/fpm.d/*.confpid = /var/run/php-fpm.piderror_log = /var/log/php-fpm.logcd php-fpm.dsudo cp www.conf.default www.confvi www.conf//[insert][www]user = nginxgroup = nginxlisten = 127.0.0.1:9000catch_workers_output = yesslowlog = /var/log/php-fpm.slow.logrequest_slowlog_timeout = 30sphp_flag[display_errors] = offphp_admin_value[error_log] = /var/log/php-fpm.error.logphp_admin_flag[log_errors] = onphp_admin_value[memory_limit] = 64M;php_admin_value[open_basedir] = /var/www 配置php.ini文件 123456789101112131415161718192021222324sudo cp ~/php/php.ini-develop /usr/local/php/lib/php.inicd /usr/local/php/libsudo vi php.ini//[insert]short_open_tag = On;open_basedir = /var/wwwdisable_functions = exec,passthru,shell_exec,system,proc_open,popenexpose_php = Offmax_execution_time = 30memory_limit = 64Mdate.timezone = Europe/Warsawerror_reporting = E_ALL &amp; ~E_DEPRECATED &amp; ~E_STRICTdisplay_errors = Offdisplay_startup_errors = Offlog_errors = Onpost_max_size = 5Mupload_max_filesize = 4Mopcache.enable=1opcache.memory_consumption=64opcache.interned_strings_buffer=16opcache.max_accelerated_files=7000opcache.validate_timestamps=0 ;set this to 1 on production serveropcache.fast_shutdown=1 创建php-fpm启动脚本 123456789cd /etc/init.dsudo cp ~/php/sapi/fpm/init.php-fpm ./php-fpm &amp;&amp; sudo chmod +x php-fpmsudo vi php-fpm//[insert]prefix=/usr/local/phpexec_prefix=$&#123;prefix&#125;php_fpm_BIN=$&#123;exec_prefix&#125;/sbin/php-fpmphp_fpm_CONF=$&#123;prefix&#125;/etc/php-fpm.d/*.confphp_fpm_PID=/var/run/php-fpm.pid 创建php-fpm服务 sudo chkconfig -add php-fpm sudo chkconfig php-fpm on 测试php-fpm并启动 sudo /etc/init.d/php-fpm configtest sudo systemctl start php-fpm 配置nginx支持php 12345678910cd /etc/nginx &amp;&amp; sudo cp nginx.conf nginx.conf.oldsudo vi nginx.conf//[insert]location ~ \.php$ &#123; root /var/www; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125; 重启nginx服务 sudo service nginx restart 参考 https://www.webhostingneeds.com/install_php-fpm_from_source_on_centos https://blacksaildivision.com/php-install-from-source https://www.webhostingneeds.com/nginx_php-fpm_php_site_configuration]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下快速创建WindowsXP模式]]></title>
    <url>%2F2018%2F10%2F30%2Flinux-windowsxp.html</url>
    <content type="text"><![CDATA[在Virtualbox快速安装Windows XP Windows XP是微软经典操作系统，目前虽然微软已经停止支持，但是其市场占有量依然很高。有时候我们需要在XP系统中进行测试，那么有什么简单的方法可以搭建Windows XP环境呢？ 方法一：Virtuabox+Windows XP iso 使用Virtuabox创建一个Windows XP的虚拟机，该方法耗费的时间比较长，安装起来无异于安装一台新电脑。 方法二：Windows 7+ XP mode 微软已经很贴心的帮我们想好了，在Windows 7中有一种XP模式，启用该模式后可以在系统中搭建出XP环境。 方法三：Linux+Virtualbox+XP Mode 在Linux系统下，我们也可以很简单的使用XP模式。首先通过地址https://www.microsoft.com/zh-CN/download/details.aspx?id=8002 下载XP Mode； 下载的文件后缀格式为exe,在linux系统中是无法直接运行的，我们使用7zip软件解压2次； 7za x xpmode.exe //第一次解压，解压后，在文件夹source中有一个xpm文件 cd ./source &amp;&amp; 7za x xpm //第二次解压，对xpm文件解压，解压后会出现一个VirtualXPVHD文件 打开Virtualbox,新建一个虚拟机，创建虚拟磁盘时，选择已经存在的磁盘，选中文件VirtualXPVHD ; 创建完成后，打开XP虚拟机，然后安装Virtualbox扩展功能，重启后进入系统，此时XP模式已经全部完成，可以随意在系统中测试。 注意事项：在第一次启动xp虚拟机后，鼠标不能使用，此时需要使用Tab+↑+↓安装替代鼠标完成扩展功能的安装， 完成安装后不要重启系统否则会出现黑屏，选择强制关机，再次进入系统后各项功能正常使用。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>WindowsXP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Github和Hexo搭建个人网站]]></title>
    <url>%2F2018%2F10%2F30%2Fgithub-hexo-website.html</url>
    <content type="text"><![CDATA[使用Github和Hexo搭建个人网站 概述 Hexo使用Markdown解析文章，能够在几秒内利用主题生成静态网页； Github是世界上最大的代码存放网站和开源社区，目前已经被微软收购； 利用Hexo生成静态网页，将生成的静态网页发布到Github上，已完成个人网站所具有的功能。 系统信息 系统：Debian9.5 NVM：V11.0.0 NPM：6.4.1 Nodejs：11.0.0 登录Github 登录Github，新建一个repository，格式应为：xxxx.github.io 其中xxx代表你的用户名 安装Nodejs和Git 首先需要在本机上安装Node.js和Git 安装nvm aria2c https://raw.github.com/creationix/nvm/v0.33.11/install.sh &amp;&amp; chmod +x install.sh &amp;&amp; ./install.sh 安装Nodejs nvm install stable 安装hexo npm install hexo-cli -g 安装Git apt install git-core 创建一个文件夹，文件夹名称与第1步中新建的repository相同 mkdir kim1024.github.io &amp;&amp; cd kim1024.github.io 切换到刚创建的文件夹中，创建.git文件 git config --global user.name &quot;kim1024&quot; # replace kim1024 with your git username git config --global user.email &quot;email_address&quot; 如果需要使用SSH秘钥，则需要创建秘钥对 ssh-keygen -t rsa -C &quot;email_address -f ./&quot; 在目录下找到刚创建的秘钥对，其中一个文件后缀格式为.pub,这就会刚刚创建的加密公钥； cat gir_rsa.pub # 会看到一串字符，以ssh-rsa开头，以邮箱地址结尾，复制全部的字符串； 打开github.com,登录自己的账号，点击右上角的头像，找到Setting ，然后打开右侧的SSH and GPG Keys，点击New SSH Key，然后命名，将复制的字符串粘贴到下方的空白处，提交即可； 添加成功后，在本机上进行ssh测试连接ssh -i ./gir_rsa git@github.com,当出现如下提示时，表示ssh连接成功； 还可以通过ssh-add 命令将生成的密钥添加到ssh-agent中，在添加之前，需要修改key的权限 安装Hexo 安装hexo,进入创建的kim1024.github.io文件夹中，依次执行以下命令： hexo init ./ # hexo初始化 初始化完成后，在文件夹中会创建多个文件夹和文件，找到文件_config.yml进行编辑 vi _config.yml # 根据自己的需要修改其中的项目 1234deploy: type: git repo: git@github.com:kim1024/kim1024.github.io.git # replace this url with your own ssh url branch: master # 必须使用master，使用其他分支会出现问题 编辑完成后保存 hexo g hexo d # 发布，使用ssh认证，避免了每次发布都需要输入用户名和密码的麻烦]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Github</tag>
        <tag>个人网站</tag>
      </tags>
  </entry>
</search>
